========Trial 0 params: {'n_layers': 2, 'K': 3, 'Dropout': 0.3, 'Hidden0': 32, 'Hidden1': 256, 'optimizer': 'Adam', 'lr': 0.014841171742650953, 'batch': 32}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |    2304    |
|    sptH.0.gcn.bias     |     32     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.sAtt.Vs     |    729     |
|     sptH.2.sAtt.bs     |    729     |
|     sptH.2.sAtt.W1     |     32     |
|     sptH.2.sAtt.W2     |     32     |
|     sptH.2.sAtt.W3     |     1      |
|   sptH.2.gcn.weight    |   24576    |
|    sptH.2.gcn.bias     |    256     |
| sptH.2.timeConv.weight |     3      |
|  sptH.2.timeConv.bias  |     1      |
|     sptH.4.weight      |    3072    |
|      sptH.4.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    1152    |
|    sptD.0.gcn.bias     |     32     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.sAtt.Vs     |    729     |
|     sptD.2.sAtt.bs     |    729     |
|     sptD.2.sAtt.W1     |     32     |
|     sptD.2.sAtt.W2     |     32     |
|     sptD.2.sAtt.W3     |     1      |
|   sptD.2.gcn.weight    |   24576    |
|    sptD.2.gcn.bias     |    256     |
| sptD.2.timeConv.weight |     3      |
|  sptD.2.timeConv.bias  |     1      |
|     sptD.4.weight      |    3072    |
|      sptD.4.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |    2304    |
|    sptW.0.gcn.bias     |     32     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.sAtt.Vs     |    729     |
|     sptW.2.sAtt.bs     |    729     |
|     sptW.2.sAtt.W1     |     32     |
|     sptW.2.sAtt.W2     |     32     |
|     sptW.2.sAtt.W3     |     1      |
|   sptW.2.gcn.weight    |   24576    |
|    sptW.2.gcn.bias     |    256     |
| sptW.2.timeConv.weight |     3      |
|  sptW.2.timeConv.bias  |     1      |
|     sptW.4.weight      |    3072    |
|      sptW.4.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 98730
 Trial: 0 Epoch: 0, train_loss: 108917.75390625, validation_loss = 9993.1728515625
 Trial: 0 Epoch: 1, train_loss: 52392.638671875, validation_loss = 10494.4296875
 Trial: 0 Epoch: 2, train_loss: 30282.583984375, validation_loss = 8046.35791015625
 Trial: 0 Epoch: 3, train_loss: 26862.5791015625, validation_loss = 19266.15234375
 Trial: 0 Epoch: 4, train_loss: 24217.4306640625, validation_loss = 7358.97607421875
 Trial: 0 Epoch: 5, train_loss: 17054.94921875, validation_loss = 8305.0869140625
 Trial: 0 Epoch: 6, train_loss: 17753.576171875, validation_loss = 6995.6572265625
 Trial: 0 Epoch: 7, train_loss: 13198.8173828125, validation_loss = 5593.138671875
 Trial: 0 Epoch: 8, train_loss: 15027.681640625, validation_loss = 6171.49462890625
 Trial: 0 Epoch: 9, train_loss: 14308.33740234375, validation_loss = 5153.1416015625
 Trial: 0 Epoch: 10, train_loss: 13115.0673828125, validation_loss = 6533.51708984375
 Trial: 0 Epoch: 11, train_loss: 13174.61083984375, validation_loss = 5290.189453125
 Trial: 0 Epoch: 12, train_loss: 11948.8125, validation_loss = 5492.55126953125
 Trial: 0 Epoch: 13, train_loss: 12883.38916015625, validation_loss = 6346.82470703125
 Trial: 0 Epoch: 14, train_loss: 11550.33349609375, validation_loss = 4627.9462890625
 Trial: 0 Epoch: 15, train_loss: 10715.8955078125, validation_loss = 4749.7236328125
 Trial: 0 Epoch: 16, train_loss: 12264.67529296875, validation_loss = 5322.7900390625
 Trial: 0 Epoch: 17, train_loss: 12521.97412109375, validation_loss = 4634.1025390625
 Trial: 0 Epoch: 18, train_loss: 9577.96240234375, validation_loss = 6759.041015625
 Trial: 0 Epoch: 19, train_loss: 10800.3837890625, validation_loss = 5101.21435546875
 Trial: 0 Epoch: 20, train_loss: 10835.60498046875, validation_loss = 7299.6796875
 Trial: 0 Epoch: 21, train_loss: 10434.8828125, validation_loss = 5313.85546875
 Trial: 0 Epoch: 22, train_loss: 9782.4951171875, validation_loss = 4962.29736328125
 Trial: 0 Epoch: 23, train_loss: 9886.89501953125, validation_loss = 7381.07373046875
 Trial: 0 Epoch: 24, train_loss: 11827.171875, validation_loss = 4871.12939453125
 Trial: 0 Epoch: 25, train_loss: 9874.900634765625, validation_loss = 5678.9482421875
 Trial: 0 Epoch: 26, train_loss: 10029.33740234375, validation_loss = 4481.36962890625
 Trial: 0 Epoch: 27, train_loss: 8355.80322265625, validation_loss = 4519.76513671875
 Trial: 0 Epoch: 28, train_loss: 9145.10595703125, validation_loss = 5134.9658203125
 Trial: 0 Epoch: 29, train_loss: 9930.12744140625, validation_loss = 4724.4384765625
 Trial: 0 Epoch: 30, train_loss: 10040.17138671875, validation_loss = 5723.03857421875
 Trial: 0 Epoch: 31, train_loss: 10731.94677734375, validation_loss = 4350.30322265625
 Trial: 0 Epoch: 32, train_loss: 10280.685546875, validation_loss = 3821.423095703125
 Trial: 0 Epoch: 33, train_loss: 9929.71240234375, validation_loss = 5181.97265625
 Trial: 0 Epoch: 34, train_loss: 9072.12548828125, validation_loss = 4120.59716796875
 Trial: 0 Epoch: 35, train_loss: 9312.07177734375, validation_loss = 5235.47607421875
 Trial: 0 Epoch: 36, train_loss: 9879.1337890625, validation_loss = 3927.4140625
 Trial: 0 Epoch: 37, train_loss: 9562.9697265625, validation_loss = 3843.6005859375
 Trial: 0 Epoch: 38, train_loss: 9757.29345703125, validation_loss = 4917.04638671875
 Trial: 0 Epoch: 39, train_loss: 9336.9736328125, validation_loss = 3936.169921875
 Trial: 0 Epoch: 40, train_loss: 8946.04443359375, validation_loss = 4780.83544921875
 Trial: 0 Epoch: 41, train_loss: 9411.25634765625, validation_loss = 4390.2021484375
 Trial: 0 Epoch: 42, train_loss: 9697.5224609375, validation_loss = 4364.90869140625
 Trial: 0 Epoch: 43, train_loss: 9145.06298828125, validation_loss = 4388.14990234375
 Trial: 0 Epoch: 44, train_loss: 8464.296875, validation_loss = 4380.44140625
 Trial: 0 Epoch: 45, train_loss: 8991.08544921875, validation_loss = 5303.87451171875
 Trial: 0 Epoch: 46, train_loss: 8747.762939453125, validation_loss = 3924.5634765625
 Trial: 0 Epoch: 47, train_loss: 8618.49951171875, validation_loss = 3829.236572265625
 Trial: 0 Epoch: 48, train_loss: 8635.6435546875, validation_loss = 5249.306640625
 Trial: 0 Epoch: 49, train_loss: 8614.8251953125, validation_loss = 3885.573486328125
 Trial: 0 Epoch: 50, train_loss: 9183.269287109375, validation_loss = 4658.58935546875
 Trial: 0 Epoch: 51, train_loss: 9108.37646484375, validation_loss = 4150.10498046875
 Trial: 0 Epoch: 52, train_loss: 9747.017578125, validation_loss = 3650.626220703125
 Trial: 0 Epoch: 53, train_loss: 11074.9990234375, validation_loss = 3595.248291015625
 Trial: 0 Epoch: 54, train_loss: 9352.22802734375, validation_loss = 3977.08544921875
 Trial: 0 Epoch: 55, train_loss: 8686.5498046875, validation_loss = 7706.8125
 Trial: 0 Epoch: 56, train_loss: 10415.58447265625, validation_loss = 4234.5908203125
 Trial: 0 Epoch: 57, train_loss: 10102.1298828125, validation_loss = 4156.681640625
 Trial: 0 Epoch: 58, train_loss: 8518.4697265625, validation_loss = 4350.97314453125
 Trial: 0 Epoch: 59, train_loss: 8272.9541015625, validation_loss = 4249.69921875
 Trial: 0 Epoch: 60, train_loss: 8891.32275390625, validation_loss = 5098.1220703125
 Trial: 0 Epoch: 61, train_loss: 8330.74072265625, validation_loss = 4182.37060546875
 Trial: 0 Epoch: 62, train_loss: 7632.884765625, validation_loss = 3915.976318359375
 Trial: 0 Epoch: 63, train_loss: 7657.361328125, validation_loss = 4064.9560546875
 Trial: 0 Epoch: 64, train_loss: 7646.40234375, validation_loss = 4118.37451171875
 Trial: 0 Epoch: 65, train_loss: 7606.235107421875, validation_loss = 4395.404296875
 Trial: 0 Epoch: 66, train_loss: 7444.19775390625, validation_loss = 3384.31591796875
 Trial: 0 Epoch: 67, train_loss: 8188.632568359375, validation_loss = 3944.921875
 Trial: 0 Epoch: 68, train_loss: 7728.974853515625, validation_loss = 4014.084716796875
 Trial: 0 Epoch: 69, train_loss: 7669.136962890625, validation_loss = 4119.4130859375
 Trial: 0 Epoch: 70, train_loss: 8155.311767578125, validation_loss = 3643.3974609375
 Trial: 0 Epoch: 71, train_loss: 7214.97705078125, validation_loss = 3740.6962890625
 Trial: 0 Epoch: 72, train_loss: 7654.68359375, validation_loss = 3531.56591796875
 Trial: 0 Epoch: 73, train_loss: 6855.163330078125, validation_loss = 3641.891845703125
 Trial: 0 Epoch: 74, train_loss: 7432.5615234375, validation_loss = 4097.73193359375
 Trial: 0 Epoch: 75, train_loss: 6840.9326171875, validation_loss = 3883.583251953125
 Trial: 0 Epoch: 76, train_loss: 7400.23583984375, validation_loss = 3892.306884765625
 Trial: 0 Epoch: 77, train_loss: 6717.074462890625, validation_loss = 3763.12939453125
 Trial: 0 Epoch: 78, train_loss: 7581.3974609375, validation_loss = 3817.261474609375
 Trial: 0 Epoch: 79, train_loss: 6728.39794921875, validation_loss = 3883.123779296875
 Trial: 0 Epoch: 80, train_loss: 6857.14013671875, validation_loss = 3739.112548828125
 Trial: 0 Epoch: 81, train_loss: 7227.65087890625, validation_loss = 3761.574951171875
 Trial: 0 Epoch: 82, train_loss: 6745.0673828125, validation_loss = 3070.0009765625
 Trial: 0 Epoch: 83, train_loss: 7828.3525390625, validation_loss = 4540.30322265625
 Trial: 0 Epoch: 84, train_loss: 7457.635009765625, validation_loss = 3600.850830078125
 Trial: 0 Epoch: 85, train_loss: 8265.32861328125, validation_loss = 4618.97216796875
 Trial: 0 Epoch: 86, train_loss: 7459.08642578125, validation_loss = 3521.652587890625
 Trial: 0 Epoch: 87, train_loss: 8257.67626953125, validation_loss = 3427.80029296875
 Trial: 0 Epoch: 88, train_loss: 7757.0888671875, validation_loss = 3297.226318359375
 Trial: 0 Epoch: 89, train_loss: 8412.46142578125, validation_loss = 3824.8203125
 Trial: 0 Epoch: 90, train_loss: 7903.3447265625, validation_loss = 4136.70068359375
 Trial: 0 Epoch: 91, train_loss: 7020.848876953125, validation_loss = 3381.46435546875
 Trial: 0 Epoch: 92, train_loss: 7215.430419921875, validation_loss = 4056.59814453125
 Trial: 0 Epoch: 93, train_loss: 6986.851806640625, validation_loss = 3622.303466796875
 Trial: 0 Epoch: 94, train_loss: 7479.100341796875, validation_loss = 4593.15771484375
 Trial: 0 Epoch: 95, train_loss: 7398.026123046875, validation_loss = 3662.13330078125
 Trial: 0 Epoch: 96, train_loss: 7016.845703125, validation_loss = 3153.697265625
 Trial: 0 Epoch: 97, train_loss: 7172.8681640625, validation_loss = 3686.219482421875
 Trial: 0 Epoch: 98, train_loss: 6488.5556640625, validation_loss = 3757.710693359375
 Trial: 0 Epoch: 99, train_loss: 7099.88427734375, validation_loss = 3620.0419921875
========Trial 1 params: {'n_layers': 2, 'K': 4, 'Dropout': 0.4, 'Hidden0': 8, 'Hidden1': 64, 'optimizer': 'Adam', 'lr': 0.006334845354090511, 'batch': 32}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |    768     |
|    sptH.0.gcn.bias     |     8      |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.sAtt.Vs     |    729     |
|     sptH.2.sAtt.bs     |    729     |
|     sptH.2.sAtt.W1     |     8      |
|     sptH.2.sAtt.W2     |     8      |
|     sptH.2.sAtt.W3     |     1      |
|   sptH.2.gcn.weight    |    2048    |
|    sptH.2.gcn.bias     |     64     |
| sptH.2.timeConv.weight |     3      |
|  sptH.2.timeConv.bias  |     1      |
|     sptH.4.weight      |    768     |
|      sptH.4.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    384     |
|    sptD.0.gcn.bias     |     8      |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.sAtt.Vs     |    729     |
|     sptD.2.sAtt.bs     |    729     |
|     sptD.2.sAtt.W1     |     8      |
|     sptD.2.sAtt.W2     |     8      |
|     sptD.2.sAtt.W3     |     1      |
|   sptD.2.gcn.weight    |    2048    |
|    sptD.2.gcn.bias     |     64     |
| sptD.2.timeConv.weight |     3      |
|  sptD.2.timeConv.bias  |     1      |
|     sptD.4.weight      |    768     |
|      sptD.4.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |    768     |
|    sptW.0.gcn.bias     |     8      |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.sAtt.Vs     |    729     |
|     sptW.2.sAtt.bs     |    729     |
|     sptW.2.sAtt.W1     |     8      |
|     sptW.2.sAtt.W2     |     8      |
|     sptW.2.sAtt.W3     |     1      |
|   sptW.2.gcn.weight    |    2048    |
|    sptW.2.gcn.bias     |     64     |
| sptW.2.timeConv.weight |     3      |
|  sptW.2.timeConv.bias  |     1      |
|     sptW.4.weight      |    768     |
|      sptW.4.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 19602
 Trial: 1 Epoch: 0, train_loss: 115100.43359375, validation_loss = 24595.39453125
 Trial: 1 Epoch: 1, train_loss: 65289.0234375, validation_loss = 25010.94921875
 Trial: 1 Epoch: 2, train_loss: 56723.400390625, validation_loss = 22102.875
 Trial: 1 Epoch: 3, train_loss: 40132.5732421875, validation_loss = 11993.091796875
 Trial: 1 Epoch: 4, train_loss: 31676.763671875, validation_loss = 11864.275390625
 Trial: 1 Epoch: 5, train_loss: 33311.0517578125, validation_loss = 13194.39453125
 Trial: 1 Epoch: 6, train_loss: 34421.076171875, validation_loss = 11259.04296875
 Trial: 1 Epoch: 7, train_loss: 30713.2744140625, validation_loss = 13474.81640625
 Trial: 1 Epoch: 8, train_loss: 25313.3408203125, validation_loss = 11384.50390625
 Trial: 1 Epoch: 9, train_loss: 24503.337890625, validation_loss = 10767.3681640625
 Trial: 1 Epoch: 10, train_loss: 20843.6484375, validation_loss = 8186.99560546875
 Trial: 1 Epoch: 11, train_loss: 20485.478515625, validation_loss = 8628.4072265625
 Trial: 1 Epoch: 12, train_loss: 19234.62548828125, validation_loss = 8723.3974609375
 Trial: 1 Epoch: 13, train_loss: 19912.759765625, validation_loss = 9167.2177734375
 Trial: 1 Epoch: 14, train_loss: 17435.568359375, validation_loss = 9226.0615234375
 Trial: 1 Epoch: 15, train_loss: 16519.61279296875, validation_loss = 7109.18310546875
 Trial: 1 Epoch: 16, train_loss: 16518.48291015625, validation_loss = 6805.6748046875
 Trial: 1 Epoch: 17, train_loss: 16925.013671875, validation_loss = 6695.5517578125
 Trial: 1 Epoch: 18, train_loss: 14901.85693359375, validation_loss = 7458.044921875
 Trial: 1 Epoch: 19, train_loss: 15579.39892578125, validation_loss = 6845.6728515625
 Trial: 1 Epoch: 20, train_loss: 14768.80419921875, validation_loss = 6586.9677734375
 Trial: 1 Epoch: 21, train_loss: 14595.8701171875, validation_loss = 5816.232421875
 Trial: 1 Epoch: 22, train_loss: 14739.5546875, validation_loss = 6636.83740234375
 Trial: 1 Epoch: 23, train_loss: 13799.23779296875, validation_loss = 5418.11279296875
 Trial: 1 Epoch: 24, train_loss: 14230.68896484375, validation_loss = 6050.5986328125
 Trial: 1 Epoch: 25, train_loss: 13341.8916015625, validation_loss = 6760.6650390625
 Trial: 1 Epoch: 26, train_loss: 13111.84326171875, validation_loss = 6301.0576171875
 Trial: 1 Epoch: 27, train_loss: 13006.015625, validation_loss = 5285.64404296875
 Trial: 1 Epoch: 28, train_loss: 14322.69140625, validation_loss = 7853.10888671875
 Trial: 1 Epoch: 29, train_loss: 13627.1806640625, validation_loss = 6559.9501953125
 Trial: 1 Epoch: 30, train_loss: 12844.80859375, validation_loss = 6086.443359375
 Trial: 1 Epoch: 31, train_loss: 13470.3935546875, validation_loss = 6438.17919921875
 Trial: 1 Epoch: 32, train_loss: 13466.60400390625, validation_loss = 6135.44921875
 Trial: 1 Epoch: 33, train_loss: 13737.18359375, validation_loss = 6430.75634765625
 Trial: 1 Epoch: 34, train_loss: 11783.7568359375, validation_loss = 5584.37841796875
 Trial: 1 Epoch: 35, train_loss: 12714.24462890625, validation_loss = 5942.80810546875
 Trial: 1 Epoch: 36, train_loss: 12109.5791015625, validation_loss = 6169.4833984375
 Trial: 1 Epoch: 37, train_loss: 12171.326171875, validation_loss = 5621.14453125
 Trial: 1 Epoch: 38, train_loss: 11246.7919921875, validation_loss = 5674.2900390625
 Trial: 1 Epoch: 39, train_loss: 11281.0078125, validation_loss = 6662.529296875
 Trial: 1 Epoch: 40, train_loss: 11862.283203125, validation_loss = 6242.0087890625
 Trial: 1 Epoch: 41, train_loss: 12166.15576171875, validation_loss = 5883.251953125
 Trial: 1 Epoch: 42, train_loss: 11458.54150390625, validation_loss = 5540.5
 Trial: 1 Epoch: 43, train_loss: 11987.421875, validation_loss = 5288.2724609375
 Trial: 1 Epoch: 44, train_loss: 11609.361328125, validation_loss = 6611.64404296875
 Trial: 1 Epoch: 45, train_loss: 11301.212890625, validation_loss = 5048.81982421875
 Trial: 1 Epoch: 46, train_loss: 11329.431640625, validation_loss = 4913.5361328125
 Trial: 1 Epoch: 47, train_loss: 10589.2275390625, validation_loss = 5467.76123046875
 Trial: 1 Epoch: 48, train_loss: 11781.8671875, validation_loss = 4716.18408203125
 Trial: 1 Epoch: 49, train_loss: 11495.3212890625, validation_loss = 4640.619140625
 Trial: 1 Epoch: 50, train_loss: 11338.77001953125, validation_loss = 6177.3740234375
 Trial: 1 Epoch: 51, train_loss: 11127.78515625, validation_loss = 5531.9267578125
 Trial: 1 Epoch: 52, train_loss: 12226.4658203125, validation_loss = 5278.26953125
 Trial: 1 Epoch: 53, train_loss: 10147.00244140625, validation_loss = 5548.15625
 Trial: 1 Epoch: 54, train_loss: 11818.79736328125, validation_loss = 4948.18212890625
 Trial: 1 Epoch: 55, train_loss: 13410.5751953125, validation_loss = 5555.79296875
 Trial: 1 Epoch: 56, train_loss: 11364.8798828125, validation_loss = 6741.248046875
 Trial: 1 Epoch: 57, train_loss: 11412.9736328125, validation_loss = 5756.26708984375
 Trial: 1 Epoch: 58, train_loss: 11128.81396484375, validation_loss = 5352.13623046875
 Trial: 1 Epoch: 59, train_loss: 10664.3173828125, validation_loss = 5702.34033203125
 Trial: 1 Epoch: 60, train_loss: 10330.1962890625, validation_loss = 4219.14697265625
 Trial: 1 Epoch: 61, train_loss: 11065.05712890625, validation_loss = 5850.21044921875
 Trial: 1 Epoch: 62, train_loss: 10395.9169921875, validation_loss = 4493.77880859375
 Trial: 1 Epoch: 63, train_loss: 10316.623046875, validation_loss = 4660.2734375
 Trial: 1 Epoch: 64, train_loss: 9911.3447265625, validation_loss = 6216.99755859375
 Trial: 1 Epoch: 65, train_loss: 10127.08984375, validation_loss = 5073.8447265625
 Trial: 1 Epoch: 66, train_loss: 9821.80224609375, validation_loss = 4950.14453125
 Trial: 1 Epoch: 67, train_loss: 10310.40771484375, validation_loss = 4981.9638671875
 Trial: 1 Epoch: 68, train_loss: 10399.5556640625, validation_loss = 4698.98974609375
 Trial: 1 Epoch: 69, train_loss: 10087.646484375, validation_loss = 5520.9638671875
 Trial: 1 Epoch: 70, train_loss: 10773.96923828125, validation_loss = 5089.8388671875
 Trial: 1 Epoch: 71, train_loss: 9246.173828125, validation_loss = 4870.53271484375
 Trial: 1 Epoch: 72, train_loss: 9222.052734375, validation_loss = 5512.20361328125
 Trial: 1 Epoch: 73, train_loss: 9189.234130859375, validation_loss = 4684.806640625
 Trial: 1 Epoch: 74, train_loss: 10467.40625, validation_loss = 5299.22607421875
 Trial: 1 Epoch: 75, train_loss: 9403.02783203125, validation_loss = 5132.8671875
 Trial: 1 Epoch: 76, train_loss: 8928.1298828125, validation_loss = 4706.73779296875
 Trial: 1 Epoch: 77, train_loss: 9925.36669921875, validation_loss = 5370.62548828125
 Trial: 1 Epoch: 78, train_loss: 9820.5595703125, validation_loss = 4696.49658203125
 Trial: 1 Epoch: 79, train_loss: 9432.380859375, validation_loss = 4753.4853515625
 Trial: 1 Epoch: 80, train_loss: 9512.837890625, validation_loss = 4522.50390625
 Trial: 1 Epoch: 81, train_loss: 9619.4736328125, validation_loss = 4131.4599609375
 Trial: 1 Epoch: 82, train_loss: 9588.52099609375, validation_loss = 4588.134765625
 Trial: 1 Epoch: 83, train_loss: 9397.333740234375, validation_loss = 4825.48193359375
 Trial: 1 Epoch: 84, train_loss: 8935.1943359375, validation_loss = 4722.8662109375
 Trial: 1 Epoch: 85, train_loss: 8778.523681640625, validation_loss = 3953.275146484375
 Trial: 1 Epoch: 86, train_loss: 9317.7900390625, validation_loss = 4344.73974609375
 Trial: 1 Epoch: 87, train_loss: 9042.947265625, validation_loss = 4710.611328125
 Trial: 1 Epoch: 88, train_loss: 9396.1259765625, validation_loss = 4601.76220703125
 Trial: 1 Epoch: 89, train_loss: 9361.75146484375, validation_loss = 5153.54052734375
 Trial: 1 Epoch: 90, train_loss: 9441.4755859375, validation_loss = 4626.025390625
 Trial: 1 Epoch: 91, train_loss: 9227.958740234375, validation_loss = 5339.328125
 Trial: 1 Epoch: 92, train_loss: 9056.2822265625, validation_loss = 4201.84130859375
 Trial: 1 Epoch: 93, train_loss: 8709.41064453125, validation_loss = 4054.203125
 Trial: 1 Epoch: 94, train_loss: 8997.83203125, validation_loss = 4888.0244140625
 Trial: 1 Epoch: 95, train_loss: 10312.30859375, validation_loss = 4895.12646484375
 Trial: 1 Epoch: 96, train_loss: 9212.44189453125, validation_loss = 4421.330078125
 Trial: 1 Epoch: 97, train_loss: 9048.412109375, validation_loss = 4062.227294921875
 Trial: 1 Epoch: 98, train_loss: 9680.76220703125, validation_loss = 4982.91552734375
 Trial: 1 Epoch: 99, train_loss: 9951.816650390625, validation_loss = 4756.44580078125
========Trial 2 params: {'n_layers': 2, 'K': 4, 'Dropout': 0.1, 'Hidden0': 32, 'Hidden1': 16, 'optimizer': 'Adam', 'lr': 0.001169685342754271, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |    3072    |
|    sptH.0.gcn.bias     |     32     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.sAtt.Vs     |    729     |
|     sptH.2.sAtt.bs     |    729     |
|     sptH.2.sAtt.W1     |     32     |
|     sptH.2.sAtt.W2     |     32     |
|     sptH.2.sAtt.W3     |     1      |
|   sptH.2.gcn.weight    |    2048    |
|    sptH.2.gcn.bias     |     16     |
| sptH.2.timeConv.weight |     3      |
|  sptH.2.timeConv.bias  |     1      |
|     sptH.4.weight      |    192     |
|      sptH.4.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    1536    |
|    sptD.0.gcn.bias     |     32     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.sAtt.Vs     |    729     |
|     sptD.2.sAtt.bs     |    729     |
|     sptD.2.sAtt.W1     |     32     |
|     sptD.2.sAtt.W2     |     32     |
|     sptD.2.sAtt.W3     |     1      |
|   sptD.2.gcn.weight    |    2048    |
|    sptD.2.gcn.bias     |     16     |
| sptD.2.timeConv.weight |     3      |
|  sptD.2.timeConv.bias  |     1      |
|     sptD.4.weight      |    192     |
|      sptD.4.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |    3072    |
|    sptW.0.gcn.bias     |     32     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.sAtt.Vs     |    729     |
|     sptW.2.sAtt.bs     |    729     |
|     sptW.2.sAtt.W1     |     32     |
|     sptW.2.sAtt.W2     |     32     |
|     sptW.2.sAtt.W3     |     1      |
|   sptW.2.gcn.weight    |    2048    |
|    sptW.2.gcn.bias     |     16     |
| sptW.2.timeConv.weight |     3      |
|  sptW.2.timeConv.bias  |     1      |
|     sptW.4.weight      |    192     |
|      sptW.4.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 23706
 Trial: 2 Epoch: 0, train_loss: 289746.89453125, validation_loss = 23067.533203125
 Trial: 2 Epoch: 1, train_loss: 187659.6259765625, validation_loss = 18784.7294921875
 Trial: 2 Epoch: 2, train_loss: 139001.56103515625, validation_loss = 12138.240234375
 Trial: 2 Epoch: 3, train_loss: 111261.15283203125, validation_loss = 11962.26025390625
 Trial: 2 Epoch: 4, train_loss: 97907.3115234375, validation_loss = 10637.84375
 Trial: 2 Epoch: 5, train_loss: 86371.06127929688, validation_loss = 9809.915283203125
 Trial: 2 Epoch: 6, train_loss: 67826.64770507812, validation_loss = 7850.186279296875
 Trial: 2 Epoch: 7, train_loss: 56817.062744140625, validation_loss = 7519.39501953125
 Trial: 2 Epoch: 8, train_loss: 52707.56201171875, validation_loss = 7458.548583984375
 Trial: 2 Epoch: 9, train_loss: 49540.04931640625, validation_loss = 6787.2158203125
 Trial: 2 Epoch: 10, train_loss: 47809.392333984375, validation_loss = 6864.548828125
 Trial: 2 Epoch: 11, train_loss: 47443.38427734375, validation_loss = 6579.700927734375
 Trial: 2 Epoch: 12, train_loss: 47306.34716796875, validation_loss = 6057.51513671875
 Trial: 2 Epoch: 13, train_loss: 44455.331298828125, validation_loss = 6656.23291015625
 Trial: 2 Epoch: 14, train_loss: 42337.77685546875, validation_loss = 6126.8349609375
 Trial: 2 Epoch: 15, train_loss: 43160.681396484375, validation_loss = 6398.78759765625
 Trial: 2 Epoch: 16, train_loss: 40967.08679199219, validation_loss = 5781.3837890625
 Trial: 2 Epoch: 17, train_loss: 40180.473388671875, validation_loss = 6415.724609375
 Trial: 2 Epoch: 18, train_loss: 40018.328857421875, validation_loss = 5349.0899658203125
 Trial: 2 Epoch: 19, train_loss: 39312.418701171875, validation_loss = 5629.055419921875
 Trial: 2 Epoch: 20, train_loss: 39603.02978515625, validation_loss = 6610.65185546875
 Trial: 2 Epoch: 21, train_loss: 39385.92138671875, validation_loss = 5227.2406005859375
 Trial: 2 Epoch: 22, train_loss: 37497.19860839844, validation_loss = 5050.6334228515625
 Trial: 2 Epoch: 23, train_loss: 35299.621826171875, validation_loss = 4996.5076904296875
 Trial: 2 Epoch: 24, train_loss: 35897.37121582031, validation_loss = 5372.522705078125
 Trial: 2 Epoch: 25, train_loss: 34559.372314453125, validation_loss = 5479.83203125
 Trial: 2 Epoch: 26, train_loss: 33850.15856933594, validation_loss = 4635.283203125
 Trial: 2 Epoch: 27, train_loss: 33187.02795410156, validation_loss = 4985.5999755859375
 Trial: 2 Epoch: 28, train_loss: 32937.1923828125, validation_loss = 4631.2752685546875
 Trial: 2 Epoch: 29, train_loss: 32914.03759765625, validation_loss = 4547.4422607421875
 Trial: 2 Epoch: 30, train_loss: 31638.609008789062, validation_loss = 4323.0736083984375
 Trial: 2 Epoch: 31, train_loss: 31738.447631835938, validation_loss = 4780.6497802734375
 Trial: 2 Epoch: 32, train_loss: 30987.522094726562, validation_loss = 4869.4967041015625
 Trial: 2 Epoch: 33, train_loss: 30430.814208984375, validation_loss = 4628.52392578125
 Trial: 2 Epoch: 34, train_loss: 29628.848266601562, validation_loss = 4376.0194091796875
 Trial: 2 Epoch: 35, train_loss: 29211.294799804688, validation_loss = 4340.28515625
 Trial: 2 Epoch: 36, train_loss: 31066.078125, validation_loss = 4112.3634033203125
 Trial: 2 Epoch: 37, train_loss: 29689.32958984375, validation_loss = 4227.6234130859375
 Trial: 2 Epoch: 38, train_loss: 27926.85205078125, validation_loss = 4650.417236328125
 Trial: 2 Epoch: 39, train_loss: 29390.449462890625, validation_loss = 4505.4739990234375
 Trial: 2 Epoch: 40, train_loss: 29922.001342773438, validation_loss = 4294.7701416015625
 Trial: 2 Epoch: 41, train_loss: 28757.666137695312, validation_loss = 4099.8133544921875
 Trial: 2 Epoch: 42, train_loss: 29806.527587890625, validation_loss = 3969.3819580078125
 Trial: 2 Epoch: 43, train_loss: 29104.227416992188, validation_loss = 3973.411376953125
 Trial: 2 Epoch: 44, train_loss: 29465.736694335938, validation_loss = 4300.213134765625
 Trial: 2 Epoch: 45, train_loss: 27565.03466796875, validation_loss = 4945.4918212890625
 Trial: 2 Epoch: 46, train_loss: 29594.926879882812, validation_loss = 4007.2510986328125
 Trial: 2 Epoch: 47, train_loss: 27677.904663085938, validation_loss = 4700.683837890625
 Trial: 2 Epoch: 48, train_loss: 28753.652954101562, validation_loss = 3996.9813232421875
 Trial: 2 Epoch: 49, train_loss: 28583.935302734375, validation_loss = 3898.157470703125
 Trial: 2 Epoch: 50, train_loss: 26625.274291992188, validation_loss = 3870.1173095703125
 Trial: 2 Epoch: 51, train_loss: 27467.586303710938, validation_loss = 4312.440673828125
 Trial: 2 Epoch: 52, train_loss: 25244.84619140625, validation_loss = 4167.34130859375
 Trial: 2 Epoch: 53, train_loss: 24847.815673828125, validation_loss = 3759.8271484375
 Trial: 2 Epoch: 54, train_loss: 27146.132202148438, validation_loss = 3845.3592529296875
 Trial: 2 Epoch: 55, train_loss: 25595.225952148438, validation_loss = 4037.260986328125
 Trial: 2 Epoch: 56, train_loss: 26483.905395507812, validation_loss = 4322.0390625
 Trial: 2 Epoch: 57, train_loss: 25151.613159179688, validation_loss = 3792.27001953125
 Trial: 2 Epoch: 58, train_loss: 25349.221069335938, validation_loss = 3992.3494873046875
 Trial: 2 Epoch: 59, train_loss: 24191.058166503906, validation_loss = 3749.84228515625
 Trial: 2 Epoch: 60, train_loss: 24862.896728515625, validation_loss = 3493.6414794921875
 Trial: 2 Epoch: 61, train_loss: 26231.270629882812, validation_loss = 4029.5372314453125
 Trial: 2 Epoch: 62, train_loss: 26627.345336914062, validation_loss = 3885.8599853515625
 Trial: 2 Epoch: 63, train_loss: 24557.20135498047, validation_loss = 3803.033447265625
 Trial: 2 Epoch: 64, train_loss: 25366.130615234375, validation_loss = 3914.6165771484375
 Trial: 2 Epoch: 65, train_loss: 27014.19580078125, validation_loss = 3491.93994140625
 Trial: 2 Epoch: 66, train_loss: 27192.573974609375, validation_loss = 4097.091552734375
 Trial: 2 Epoch: 67, train_loss: 24335.963256835938, validation_loss = 4007.8009033203125
 Trial: 2 Epoch: 68, train_loss: 24046.274169921875, validation_loss = 3989.4189453125
 Trial: 2 Epoch: 69, train_loss: 23697.25146484375, validation_loss = 3654.553955078125
 Trial: 2 Epoch: 70, train_loss: 24504.928344726562, validation_loss = 3518.1455078125
 Trial: 2 Epoch: 71, train_loss: 22760.25408935547, validation_loss = 4032.8614501953125
 Trial: 2 Epoch: 72, train_loss: 24196.15283203125, validation_loss = 3968.252685546875
 Trial: 2 Epoch: 73, train_loss: 24470.02734375, validation_loss = 4047.0416259765625
 Trial: 2 Epoch: 74, train_loss: 23998.478759765625, validation_loss = 3987.9957275390625
 Trial: 2 Epoch: 75, train_loss: 25211.800537109375, validation_loss = 3710.568115234375
 Trial: 2 Epoch: 76, train_loss: 24170.112915039062, validation_loss = 3651.36572265625
 Trial: 2 Epoch: 77, train_loss: 23675.268676757812, validation_loss = 3700.8912353515625
 Trial: 2 Epoch: 78, train_loss: 23173.999267578125, validation_loss = 3747.7989501953125
 Trial: 2 Epoch: 79, train_loss: 24083.63671875, validation_loss = 3474.9005126953125
 Trial: 2 Epoch: 80, train_loss: 22805.423950195312, validation_loss = 3612.1148681640625
 Trial: 2 Epoch: 81, train_loss: 23546.593505859375, validation_loss = 3756.0433349609375
 Trial: 2 Epoch: 82, train_loss: 22917.664794921875, validation_loss = 3936.0931396484375
 Trial: 2 Epoch: 83, train_loss: 22289.11700439453, validation_loss = 3623.2052001953125
 Trial: 2 Epoch: 84, train_loss: 23089.102905273438, validation_loss = 3735.0648193359375
 Trial: 2 Epoch: 85, train_loss: 22765.196655273438, validation_loss = 3716.2130126953125
 Trial: 2 Epoch: 86, train_loss: 23071.955688476562, validation_loss = 3638.7947998046875
 Trial: 2 Epoch: 87, train_loss: 22840.203002929688, validation_loss = 3571.3046875
 Trial: 2 Epoch: 88, train_loss: 22681.686645507812, validation_loss = 3521.7041015625
 Trial: 2 Epoch: 89, train_loss: 21902.73272705078, validation_loss = 3995.3978271484375
 Trial: 2 Epoch: 90, train_loss: 23672.723388671875, validation_loss = 3845.0982666015625
 Trial: 2 Epoch: 91, train_loss: 23317.75848388672, validation_loss = 3912.97802734375
 Trial: 2 Epoch: 92, train_loss: 22683.395385742188, validation_loss = 3625.7091064453125
 Trial: 2 Epoch: 93, train_loss: 22063.169921875, validation_loss = 3531.3790283203125
 Trial: 2 Epoch: 94, train_loss: 21758.820251464844, validation_loss = 3567.767578125
 Trial: 2 Epoch: 95, train_loss: 21944.235900878906, validation_loss = 3499.220458984375
 Trial: 2 Epoch: 96, train_loss: 21573.902709960938, validation_loss = 3536.8314208984375
 Trial: 2 Epoch: 97, train_loss: 21735.427124023438, validation_loss = 3569.1666259765625
 Trial: 2 Epoch: 98, train_loss: 21006.64453125, validation_loss = 3950.5369873046875
 Trial: 2 Epoch: 99, train_loss: 21320.99072265625, validation_loss = 3747.70068359375
========Trial 3 params: {'n_layers': 3, 'K': 4, 'Dropout': 0.2, 'Hidden0': 256, 'Hidden1': 16, 'Hidden2': 16, 'optimizer': 'Adam', 'lr': 0.007548978247872489, 'batch': 8}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |   24576    |
|    sptH.0.gcn.bias     |    256     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.sAtt.Vs     |    729     |
|     sptH.2.sAtt.bs     |    729     |
|     sptH.2.sAtt.W1     |    256     |
|     sptH.2.sAtt.W2     |    256     |
|     sptH.2.sAtt.W3     |     1      |
|   sptH.2.gcn.weight    |   16384    |
|    sptH.2.gcn.bias     |     16     |
| sptH.2.timeConv.weight |     3      |
|  sptH.2.timeConv.bias  |     1      |
|     sptH.4.sAtt.Vs     |    729     |
|     sptH.4.sAtt.bs     |    729     |
|     sptH.4.sAtt.W1     |     16     |
|     sptH.4.sAtt.W2     |     16     |
|     sptH.4.sAtt.W3     |     1      |
|   sptH.4.gcn.weight    |    1024    |
|    sptH.4.gcn.bias     |     16     |
| sptH.4.timeConv.weight |     3      |
|  sptH.4.timeConv.bias  |     1      |
|     sptH.6.weight      |    192     |
|      sptH.6.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |   12288    |
|    sptD.0.gcn.bias     |    256     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.sAtt.Vs     |    729     |
|     sptD.2.sAtt.bs     |    729     |
|     sptD.2.sAtt.W1     |    256     |
|     sptD.2.sAtt.W2     |    256     |
|     sptD.2.sAtt.W3     |     1      |
|   sptD.2.gcn.weight    |   16384    |
|    sptD.2.gcn.bias     |     16     |
| sptD.2.timeConv.weight |     3      |
|  sptD.2.timeConv.bias  |     1      |
|     sptD.4.sAtt.Vs     |    729     |
|     sptD.4.sAtt.bs     |    729     |
|     sptD.4.sAtt.W1     |     16     |
|     sptD.4.sAtt.W2     |     16     |
|     sptD.4.sAtt.W3     |     1      |
|   sptD.4.gcn.weight    |    1024    |
|    sptD.4.gcn.bias     |     16     |
| sptD.4.timeConv.weight |     3      |
|  sptD.4.timeConv.bias  |     1      |
|     sptD.6.weight      |    192     |
|      sptD.6.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |   24576    |
|    sptW.0.gcn.bias     |    256     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.sAtt.Vs     |    729     |
|     sptW.2.sAtt.bs     |    729     |
|     sptW.2.sAtt.W1     |    256     |
|     sptW.2.sAtt.W2     |    256     |
|     sptW.2.sAtt.W3     |     1      |
|   sptW.2.gcn.weight    |   16384    |
|    sptW.2.gcn.bias     |     16     |
| sptW.2.timeConv.weight |     3      |
|  sptW.2.timeConv.bias  |     1      |
|     sptW.4.sAtt.Vs     |    729     |
|     sptW.4.sAtt.bs     |    729     |
|     sptW.4.sAtt.W1     |     16     |
|     sptW.4.sAtt.W2     |     16     |
|     sptW.4.sAtt.W3     |     1      |
|   sptW.4.gcn.weight    |    1024    |
|    sptW.4.gcn.bias     |     16     |
| sptW.4.timeConv.weight |     3      |
|  sptW.4.timeConv.bias  |     1      |
|     sptW.6.weight      |    192     |
|      sptW.6.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 130095
 Trial: 3 Epoch: 0, train_loss: 346122.546875, validation_loss = 41423.3046875
 Trial: 3 Epoch: 1, train_loss: 225860.353515625, validation_loss = 19997.587890625
 Trial: 3 Epoch: 2, train_loss: 156328.501953125, validation_loss = 19441.90234375
 Trial: 3 Epoch: 3, train_loss: 117263.3564453125, validation_loss = 13722.7060546875
 Trial: 3 Epoch: 4, train_loss: 96093.8544921875, validation_loss = 14097.6357421875
 Trial: 3 Epoch: 5, train_loss: 82540.00537109375, validation_loss = 13329.845703125
 Trial: 3 Epoch: 6, train_loss: 73450.64404296875, validation_loss = 12318.1337890625
 Trial: 3 Epoch: 7, train_loss: 69957.322265625, validation_loss = 12114.0634765625
 Trial: 3 Epoch: 8, train_loss: 68666.6240234375, validation_loss = 7263.07080078125
 Trial: 3 Epoch: 9, train_loss: 61444.85205078125, validation_loss = 10235.955078125
 Trial: 3 Epoch: 10, train_loss: 52633.83203125, validation_loss = 7540.93017578125
 Trial: 3 Epoch: 11, train_loss: 50884.68017578125, validation_loss = 8427.7294921875
 Trial: 3 Epoch: 12, train_loss: 51649.6767578125, validation_loss = 9251.7431640625
 Trial: 3 Epoch: 13, train_loss: 51806.7236328125, validation_loss = 7093.9404296875
 Trial: 3 Epoch: 14, train_loss: 46784.162109375, validation_loss = 9027.9794921875
 Trial: 3 Epoch: 15, train_loss: 47428.27734375, validation_loss = 6423.64697265625
 Trial: 3 Epoch: 16, train_loss: 49790.20068359375, validation_loss = 8820.35546875
 Trial: 3 Epoch: 17, train_loss: 43372.28515625, validation_loss = 6892.03076171875
 Trial: 3 Epoch: 18, train_loss: 42138.484375, validation_loss = 8089.59326171875
 Trial: 3 Epoch: 19, train_loss: 41394.4072265625, validation_loss = 6758.8720703125
 Trial: 3 Epoch: 20, train_loss: 44389.4833984375, validation_loss = 7837.46923828125
 Trial: 3 Epoch: 21, train_loss: 41817.32080078125, validation_loss = 7279.369140625
 Trial: 3 Epoch: 22, train_loss: 41753.80224609375, validation_loss = 7479.70458984375
 Trial: 3 Epoch: 23, train_loss: 40255.61669921875, validation_loss = 6803.07666015625
 Trial: 3 Epoch: 24, train_loss: 41759.0087890625, validation_loss = 6604.1435546875
 Trial: 3 Epoch: 25, train_loss: 42534.46826171875, validation_loss = 7362.65087890625
 Trial: 3 Epoch: 26, train_loss: 40199.6025390625, validation_loss = 6871.1279296875
 Trial: 3 Epoch: 27, train_loss: 39204.2197265625, validation_loss = 7344.72216796875
 Trial: 3 Epoch: 28, train_loss: 38712.62158203125, validation_loss = 6883.78173828125
 Trial: 3 Epoch: 29, train_loss: 39886.7138671875, validation_loss = 6253.697265625
 Trial: 3 Epoch: 30, train_loss: 40087.2587890625, validation_loss = 7204.259765625
 Trial: 3 Epoch: 31, train_loss: 36658.9794921875, validation_loss = 7684.03564453125
 Trial: 3 Epoch: 32, train_loss: 38318.7578125, validation_loss = 7235.8486328125
 Trial: 3 Epoch: 33, train_loss: 39686.91796875, validation_loss = 6314.39892578125
 Trial: 3 Epoch: 34, train_loss: 38891.63427734375, validation_loss = 7785.5498046875
 Trial: 3 Epoch: 35, train_loss: 38408.69140625, validation_loss = 6276.18408203125
 Trial: 3 Epoch: 36, train_loss: 40086.70458984375, validation_loss = 6983.30859375
 Trial: 3 Epoch: 37, train_loss: 39729.2705078125, validation_loss = 7564.251953125
 Trial: 3 Epoch: 38, train_loss: 37328.931640625, validation_loss = 7441.583984375
 Trial: 3 Epoch: 39, train_loss: 37717.1640625, validation_loss = 6616.291015625
 Trial: 3 Epoch: 40, train_loss: 38330.140625, validation_loss = 7485.37744140625
 Trial: 3 Epoch: 41, train_loss: 36254.31494140625, validation_loss = 8037.76318359375
 Trial: 3 Epoch: 42, train_loss: 40670.77734375, validation_loss = 6557.37841796875
 Trial: 3 Epoch: 43, train_loss: 42356.869140625, validation_loss = 6000.1318359375
 Trial: 3 Epoch: 44, train_loss: 42925.46044921875, validation_loss = 8456.9970703125
 Trial: 3 Epoch: 45, train_loss: 38677.513671875, validation_loss = 6171.98486328125
 Trial: 3 Epoch: 46, train_loss: 35409.764892578125, validation_loss = 7168.666015625
 Trial: 3 Epoch: 47, train_loss: 37642.04541015625, validation_loss = 6552.81103515625
 Trial: 3 Epoch: 48, train_loss: 37676.44140625, validation_loss = 6396.47021484375
 Trial: 3 Epoch: 49, train_loss: 35507.95166015625, validation_loss = 7682.35595703125
 Trial: 3 Epoch: 50, train_loss: 38897.14306640625, validation_loss = 6482.31103515625
 Trial: 3 Epoch: 51, train_loss: 37302.385986328125, validation_loss = 7077.13232421875
 Trial: 3 Epoch: 52, train_loss: 37069.69140625, validation_loss = 6219.7734375
 Trial: 3 Epoch: 53, train_loss: 35764.9150390625, validation_loss = 5811.13037109375
 Trial: 3 Epoch: 54, train_loss: 35914.6845703125, validation_loss = 7317.20849609375
 Trial: 3 Epoch: 55, train_loss: 37864.57177734375, validation_loss = 6123.32275390625
 Trial: 3 Epoch: 56, train_loss: 35946.28759765625, validation_loss = 6665.57421875
 Trial: 3 Epoch: 57, train_loss: 36036.3955078125, validation_loss = 6090.8486328125
 Trial: 3 Epoch: 58, train_loss: 36583.93896484375, validation_loss = 6471.8916015625
 Trial: 3 Epoch: 59, train_loss: 35198.583984375, validation_loss = 5996.0966796875
 Trial: 3 Epoch: 60, train_loss: 35853.335205078125, validation_loss = 8477.0283203125
 Trial: 3 Epoch: 61, train_loss: 37601.82568359375, validation_loss = 5235.91650390625
 Trial: 3 Epoch: 62, train_loss: 37831.695068359375, validation_loss = 7194.3486328125
 Trial: 3 Epoch: 63, train_loss: 37333.3818359375, validation_loss = 6262.10546875
 Trial: 3 Epoch: 64, train_loss: 35201.63671875, validation_loss = 7731.37841796875
 Trial: 3 Epoch: 65, train_loss: 35621.22802734375, validation_loss = 7128.94921875
 Trial: 3 Epoch: 66, train_loss: 34782.98681640625, validation_loss = 6201.34423828125
 Trial: 3 Epoch: 67, train_loss: 33790.45068359375, validation_loss = 5604.30908203125
 Trial: 3 Epoch: 68, train_loss: 33792.0751953125, validation_loss = 6742.9033203125
 Trial: 3 Epoch: 69, train_loss: 33646.51025390625, validation_loss = 5896.2802734375
 Trial: 3 Epoch: 70, train_loss: 33861.10302734375, validation_loss = 6598.22607421875
 Trial: 3 Epoch: 71, train_loss: 35612.1337890625, validation_loss = 6286.38623046875
 Trial: 3 Epoch: 72, train_loss: 35430.9443359375, validation_loss = 6456.76416015625
 Trial: 3 Epoch: 73, train_loss: 34573.99365234375, validation_loss = 6195.94482421875
 Trial: 3 Epoch: 74, train_loss: 36927.944580078125, validation_loss = 8646.8759765625
 Trial: 3 Epoch: 75, train_loss: 42478.9755859375, validation_loss = 5326.10986328125
 Trial: 3 Epoch: 76, train_loss: 36771.63525390625, validation_loss = 6121.36474609375
 Trial: 3 Epoch: 77, train_loss: 37299.7099609375, validation_loss = 7133.90576171875
 Trial: 3 Epoch: 78, train_loss: 35646.306884765625, validation_loss = 6363.55615234375
 Trial: 3 Epoch: 79, train_loss: 35483.7802734375, validation_loss = 6503.6494140625
 Trial: 3 Epoch: 80, train_loss: 32938.592041015625, validation_loss = 5910.4814453125
 Trial: 3 Epoch: 81, train_loss: 35035.98193359375, validation_loss = 6569.17529296875
 Trial: 3 Epoch: 82, train_loss: 36270.16455078125, validation_loss = 6598.7939453125
 Trial: 3 Epoch: 83, train_loss: 38534.93994140625, validation_loss = 6244.0263671875
 Trial: 3 Epoch: 84, train_loss: 38442.2294921875, validation_loss = 7646.43505859375
 Trial: 3 Epoch: 85, train_loss: 40182.9697265625, validation_loss = 6043.0791015625
 Trial: 3 Epoch: 86, train_loss: 41357.98828125, validation_loss = 6442.349609375
 Trial: 3 Epoch: 87, train_loss: 40829.8876953125, validation_loss = 7262.43408203125
 Trial: 3 Epoch: 88, train_loss: 39388.734375, validation_loss = 6612.01318359375
 Trial: 3 Epoch: 89, train_loss: 37725.44384765625, validation_loss = 6820.0380859375
 Trial: 3 Epoch: 90, train_loss: 37764.4677734375, validation_loss = 5944.5791015625
 Trial: 3 Epoch: 91, train_loss: 37533.27783203125, validation_loss = 6251.0283203125
 Trial: 3 Epoch: 92, train_loss: 38107.31201171875, validation_loss = 5619.833984375
 Trial: 3 Epoch: 93, train_loss: 39394.06201171875, validation_loss = 6270.77783203125
 Trial: 3 Epoch: 94, train_loss: 37851.61083984375, validation_loss = 6038.26611328125
 Trial: 3 Epoch: 95, train_loss: 37755.115234375, validation_loss = 6387.2568359375
 Trial: 3 Epoch: 96, train_loss: 38201.794921875, validation_loss = 5949.60400390625
 Trial: 3 Epoch: 97, train_loss: 42391.00927734375, validation_loss = 5986.10791015625
 Trial: 3 Epoch: 98, train_loss: 41494.97607421875, validation_loss = 6142.095703125
 Trial: 3 Epoch: 99, train_loss: 38035.7275390625, validation_loss = 5561.0791015625
========Trial 4 params: {'n_layers': 1, 'K': 4, 'Dropout': 0.4, 'Hidden0': 4, 'optimizer': 'Adam', 'lr': 0.0012151157045463229, 'batch': 8}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |    384     |
|    sptH.0.gcn.bias     |     4      |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |     48     |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    192     |
|    sptD.0.gcn.bias     |     4      |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |     48     |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |    384     |
|    sptW.0.gcn.bias     |     4      |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |     48     |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 5697
 Trial: 4 Epoch: 0, train_loss: 398843.58984375, validation_loss = 42746.72265625
 Trial: 4 Epoch: 1, train_loss: 375728.23828125, validation_loss = 40360.5546875
 Trial: 4 Epoch: 2, train_loss: 354058.00390625, validation_loss = 36851.72265625
 Trial: 4 Epoch: 3, train_loss: 330511.984375, validation_loss = 37674.16796875
 Trial: 4 Epoch: 4, train_loss: 316061.9296875, validation_loss = 33867.54296875
 Trial: 4 Epoch: 5, train_loss: 304134.15625, validation_loss = 33268.0625
 Trial: 4 Epoch: 6, train_loss: 296127.3984375, validation_loss = 32714.654296875
 Trial: 4 Epoch: 7, train_loss: 283467.68359375, validation_loss = 32206.51953125
 Trial: 4 Epoch: 8, train_loss: 275662.34765625, validation_loss = 32951.9140625
 Trial: 4 Epoch: 9, train_loss: 269991.1484375, validation_loss = 29872.03125
 Trial: 4 Epoch: 10, train_loss: 266387.27734375, validation_loss = 28069.51953125
 Trial: 4 Epoch: 11, train_loss: 253918.6328125, validation_loss = 30226.939453125
 Trial: 4 Epoch: 12, train_loss: 253226.85546875, validation_loss = 28089.462890625
 Trial: 4 Epoch: 13, train_loss: 243117.041015625, validation_loss = 27753.455078125
 Trial: 4 Epoch: 14, train_loss: 230089.25, validation_loss = 25562.587890625
 Trial: 4 Epoch: 15, train_loss: 227683.396484375, validation_loss = 24739.404296875
 Trial: 4 Epoch: 16, train_loss: 213078.4921875, validation_loss = 23442.765625
 Trial: 4 Epoch: 17, train_loss: 199575.982421875, validation_loss = 23799.107421875
 Trial: 4 Epoch: 18, train_loss: 180941.54296875, validation_loss = 19719.361328125
 Trial: 4 Epoch: 19, train_loss: 169345.134765625, validation_loss = 20322.220703125
 Trial: 4 Epoch: 20, train_loss: 159440.865234375, validation_loss = 19328.974609375
 Trial: 4 Epoch: 21, train_loss: 151144.783203125, validation_loss = 16592.552734375
 Trial: 4 Epoch: 22, train_loss: 148247.5234375, validation_loss = 16298.7744140625
 Trial: 4 Epoch: 23, train_loss: 145494.490234375, validation_loss = 16222.9501953125
 Trial: 4 Epoch: 24, train_loss: 143837.892578125, validation_loss = 16739.775390625
 Trial: 4 Epoch: 25, train_loss: 133007.24609375, validation_loss = 15000.2666015625
 Trial: 4 Epoch: 26, train_loss: 132629.703125, validation_loss = 14867.2939453125
 Trial: 4 Epoch: 27, train_loss: 126423.623046875, validation_loss = 16615.076171875
 Trial: 4 Epoch: 28, train_loss: 126774.685546875, validation_loss = 15246.1298828125
 Trial: 4 Epoch: 29, train_loss: 119274.7265625, validation_loss = 15469.119140625
 Trial: 4 Epoch: 30, train_loss: 119267.861328125, validation_loss = 14250.7138671875
 Trial: 4 Epoch: 31, train_loss: 115079.7041015625, validation_loss = 14308.8427734375
 Trial: 4 Epoch: 32, train_loss: 113967.994140625, validation_loss = 15085.0654296875
 Trial: 4 Epoch: 33, train_loss: 113471.12890625, validation_loss = 13790.7255859375
 Trial: 4 Epoch: 34, train_loss: 113553.50390625, validation_loss = 13922.314453125
 Trial: 4 Epoch: 35, train_loss: 113104.005859375, validation_loss = 13860.3203125
 Trial: 4 Epoch: 36, train_loss: 115014.287109375, validation_loss = 13399.619140625
 Trial: 4 Epoch: 37, train_loss: 110366.1796875, validation_loss = 13224.619140625
 Trial: 4 Epoch: 38, train_loss: 109184.8720703125, validation_loss = 12951.5576171875
 Trial: 4 Epoch: 39, train_loss: 104412.498046875, validation_loss = 12618.2236328125
 Trial: 4 Epoch: 40, train_loss: 109203.8515625, validation_loss = 12415.62890625
 Trial: 4 Epoch: 41, train_loss: 100186.083984375, validation_loss = 12477.2978515625
 Trial: 4 Epoch: 42, train_loss: 100464.7138671875, validation_loss = 13374.0517578125
 Trial: 4 Epoch: 43, train_loss: 99937.3017578125, validation_loss = 12566.7099609375
 Trial: 4 Epoch: 44, train_loss: 106420.140625, validation_loss = 13359.705078125
 Trial: 4 Epoch: 45, train_loss: 105298.7529296875, validation_loss = 11772.5849609375
 Trial: 4 Epoch: 46, train_loss: 101518.10546875, validation_loss = 13433.427734375
 Trial: 4 Epoch: 47, train_loss: 99726.771484375, validation_loss = 12490.3076171875
 Trial: 4 Epoch: 48, train_loss: 97110.818359375, validation_loss = 11763.5302734375
 Trial: 4 Epoch: 49, train_loss: 102594.8623046875, validation_loss = 12270.8525390625
 Trial: 4 Epoch: 50, train_loss: 99547.3671875, validation_loss = 12147.7607421875
 Trial: 4 Epoch: 51, train_loss: 97141.4375, validation_loss = 13585.24609375
 Trial: 4 Epoch: 52, train_loss: 95885.8017578125, validation_loss = 12915.8759765625
 Trial: 4 Epoch: 53, train_loss: 95293.890625, validation_loss = 10714.83984375
 Trial: 4 Epoch: 54, train_loss: 97565.8408203125, validation_loss = 12465.6455078125
 Trial: 4 Epoch: 55, train_loss: 95315.4951171875, validation_loss = 12778.0859375
 Trial: 4 Epoch: 56, train_loss: 92851.591796875, validation_loss = 12053.62109375
 Trial: 4 Epoch: 57, train_loss: 93357.9033203125, validation_loss = 11162.5
 Trial: 4 Epoch: 58, train_loss: 95182.7431640625, validation_loss = 12792.5986328125
 Trial: 4 Epoch: 59, train_loss: 87726.0537109375, validation_loss = 13193.6884765625
 Trial: 4 Epoch: 60, train_loss: 93765.806640625, validation_loss = 10273.53515625
 Trial: 4 Epoch: 61, train_loss: 89308.2421875, validation_loss = 11044.548828125
 Trial: 4 Epoch: 62, train_loss: 90485.603515625, validation_loss = 13968.603515625
 Trial: 4 Epoch: 63, train_loss: 91073.59765625, validation_loss = 10562.4111328125
 Trial: 4 Epoch: 64, train_loss: 88886.2880859375, validation_loss = 12487.7451171875
 Trial: 4 Epoch: 65, train_loss: 93431.7734375, validation_loss = 12747.4169921875
 Trial: 4 Epoch: 66, train_loss: 84754.9345703125, validation_loss = 12424.916015625
 Trial: 4 Epoch: 67, train_loss: 93092.625, validation_loss = 12669.7763671875
 Trial: 4 Epoch: 68, train_loss: 83865.8330078125, validation_loss = 12083.814453125
 Trial: 4 Epoch: 69, train_loss: 87415.80859375, validation_loss = 12447.5703125
 Trial: 4 Epoch: 70, train_loss: 88757.3251953125, validation_loss = 11340.857421875
 Trial: 4 Epoch: 71, train_loss: 89089.0576171875, validation_loss = 10746.01171875
 Trial: 4 Epoch: 72, train_loss: 83888.5537109375, validation_loss = 10934.6982421875
 Trial: 4 Epoch: 73, train_loss: 88601.4033203125, validation_loss = 10447.8984375
 Trial: 4 Epoch: 74, train_loss: 88760.3291015625, validation_loss = 10881.341796875
 Trial: 4 Epoch: 75, train_loss: 84128.77734375, validation_loss = 10910.7177734375
 Trial: 4 Epoch: 76, train_loss: 82671.08984375, validation_loss = 9583.771484375
 Trial: 4 Epoch: 77, train_loss: 83231.517578125, validation_loss = 10434.353515625
 Trial: 4 Epoch: 78, train_loss: 80427.3115234375, validation_loss = 10005.3408203125
 Trial: 4 Epoch: 79, train_loss: 84205.115234375, validation_loss = 9306.0712890625
 Trial: 4 Epoch: 80, train_loss: 77070.69921875, validation_loss = 9773.201171875
 Trial: 4 Epoch: 81, train_loss: 80307.6025390625, validation_loss = 11109.6435546875
 Trial: 4 Epoch: 82, train_loss: 78069.4150390625, validation_loss = 10877.4404296875
 Trial: 4 Epoch: 83, train_loss: 85325.326171875, validation_loss = 9266.4130859375
 Trial: 4 Epoch: 84, train_loss: 81792.4541015625, validation_loss = 10590.7822265625
 Trial: 4 Epoch: 85, train_loss: 77746.19140625, validation_loss = 9487.8271484375
 Trial: 4 Epoch: 86, train_loss: 76968.4140625, validation_loss = 9833.3330078125
 Trial: 4 Epoch: 87, train_loss: 76454.896484375, validation_loss = 10588.6064453125
 Trial: 4 Epoch: 88, train_loss: 80843.259765625, validation_loss = 10346.9482421875
 Trial: 4 Epoch: 89, train_loss: 73757.27734375, validation_loss = 9319.46484375
 Trial: 4 Epoch: 90, train_loss: 79797.4052734375, validation_loss = 10580.9443359375
 Trial: 4 Epoch: 91, train_loss: 79340.330078125, validation_loss = 10567.95703125
 Trial: 4 Epoch: 92, train_loss: 77522.77734375, validation_loss = 9601.927734375
 Trial: 4 Epoch: 93, train_loss: 72362.79296875, validation_loss = 9215.638671875
 Trial: 4 Epoch: 94, train_loss: 75142.962890625, validation_loss = 11134.498046875
 Trial: 4 Epoch: 95, train_loss: 73925.296875, validation_loss = 10020.6845703125
 Trial: 4 Epoch: 96, train_loss: 77258.763671875, validation_loss = 10305.7333984375
 Trial: 4 Epoch: 97, train_loss: 73936.845703125, validation_loss = 10210.7431640625
 Trial: 4 Epoch: 98, train_loss: 75799.9111328125, validation_loss = 8942.763671875
 Trial: 4 Epoch: 99, train_loss: 80180.234375, validation_loss = 7980.98681640625
========Trial 5 params: {'n_layers': 1, 'K': 4, 'Dropout': 0.1, 'Hidden0': 256, 'optimizer': 'Adam', 'lr': 0.0037704021240824413, 'batch': 64}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |   24576    |
|    sptH.0.gcn.bias     |    256     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |    3072    |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |   12288    |
|    sptD.0.gcn.bias     |    256     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |    3072    |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |   24576    |
|    sptW.0.gcn.bias     |    256     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |    3072    |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 76005
 Trial: 5 Epoch: 0, train_loss: 69671.3046875, validation_loss = 38590.71484375
========Trial 6 params: {'n_layers': 1, 'K': 4, 'Dropout': 0.2, 'Hidden0': 32, 'optimizer': 'Adam', 'lr': 0.0013980623311979333, 'batch': 64}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |    3072    |
|    sptH.0.gcn.bias     |     32     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |    384     |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    1536    |
|    sptD.0.gcn.bias     |     32     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |    384     |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |    3072    |
|    sptW.0.gcn.bias     |     32     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |    384     |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 13509
 Trial: 6 Epoch: 0, train_loss: 47017.4609375, validation_loss = 28420.021484375
========Trial 7 params: {'n_layers': 1, 'K': 4, 'Dropout': 0.5, 'Hidden0': 4, 'optimizer': 'Adam', 'lr': 0.03597067568465008, 'batch': 8}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |    384     |
|    sptH.0.gcn.bias     |     4      |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |     48     |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    192     |
|    sptD.0.gcn.bias     |     4      |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |     48     |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |    384     |
|    sptW.0.gcn.bias     |     4      |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |     48     |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 5697
 Trial: 7 Epoch: 0, train_loss: 316309.79296875, validation_loss = 28332.4140625
========Trial 8 params: {'n_layers': 1, 'K': 2, 'Dropout': 0.5, 'Hidden0': 32, 'optimizer': 'Adam', 'lr': 0.0028215836167609625, 'batch': 16}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |    1536    |
|    sptH.0.gcn.bias     |     32     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |    384     |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    768     |
|    sptD.0.gcn.bias     |     32     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |    384     |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |    1536    |
|    sptW.0.gcn.bias     |     32     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |    384     |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 9669
 Trial: 8 Epoch: 0, train_loss: 167043.375, validation_loss = 33803.34765625
========Trial 9 params: {'n_layers': 1, 'K': 2, 'Dropout': 0.1, 'Hidden0': 128, 'optimizer': 'Adam', 'lr': 0.0014035713962863609, 'batch': 8}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |    6144    |
|    sptH.0.gcn.bias     |    128     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |    1536    |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    3072    |
|    sptD.0.gcn.bias     |    128     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |    1536    |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |    6144    |
|    sptW.0.gcn.bias     |    128     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |    1536    |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 24933
 Trial: 9 Epoch: 0, train_loss: 364467.484375, validation_loss = 32529.962890625
========Trial 10 params: {'n_layers': 3, 'K': 3, 'Dropout': 0.3, 'Hidden0': 16, 'Hidden1': 256, 'Hidden2': 256, 'optimizer': 'Adam', 'lr': 0.031215120187314472, 'batch': 32}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |    1152    |
|    sptH.0.gcn.bias     |     16     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.sAtt.Vs     |    729     |
|     sptH.2.sAtt.bs     |    729     |
|     sptH.2.sAtt.W1     |     16     |
|     sptH.2.sAtt.W2     |     16     |
|     sptH.2.sAtt.W3     |     1      |
|   sptH.2.gcn.weight    |   12288    |
|    sptH.2.gcn.bias     |    256     |
| sptH.2.timeConv.weight |     3      |
|  sptH.2.timeConv.bias  |     1      |
|     sptH.4.sAtt.Vs     |    729     |
|     sptH.4.sAtt.bs     |    729     |
|     sptH.4.sAtt.W1     |    256     |
|     sptH.4.sAtt.W2     |    256     |
|     sptH.4.sAtt.W3     |     1      |
|   sptH.4.gcn.weight    |   196608   |
|    sptH.4.gcn.bias     |    256     |
| sptH.4.timeConv.weight |     3      |
|  sptH.4.timeConv.bias  |     1      |
|     sptH.6.weight      |    3072    |
|      sptH.6.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    576     |
|    sptD.0.gcn.bias     |     16     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.sAtt.Vs     |    729     |
|     sptD.2.sAtt.bs     |    729     |
|     sptD.2.sAtt.W1     |     16     |
|     sptD.2.sAtt.W2     |     16     |
|     sptD.2.sAtt.W3     |     1      |
|   sptD.2.gcn.weight    |   12288    |
|    sptD.2.gcn.bias     |    256     |
| sptD.2.timeConv.weight |     3      |
|  sptD.2.timeConv.bias  |     1      |
|     sptD.4.sAtt.Vs     |    729     |
|     sptD.4.sAtt.bs     |    729     |
|     sptD.4.sAtt.W1     |    256     |
|     sptD.4.sAtt.W2     |    256     |
|     sptD.4.sAtt.W3     |     1      |
|   sptD.4.gcn.weight    |   196608   |
|    sptD.4.gcn.bias     |    256     |
| sptD.4.timeConv.weight |     3      |
|  sptD.4.timeConv.bias  |     1      |
|     sptD.6.weight      |    3072    |
|      sptD.6.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |    1152    |
|    sptW.0.gcn.bias     |     16     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.sAtt.Vs     |    729     |
|     sptW.2.sAtt.bs     |    729     |
|     sptW.2.sAtt.W1     |     16     |
|     sptW.2.sAtt.W2     |     16     |
|     sptW.2.sAtt.W3     |     1      |
|   sptW.2.gcn.weight    |   12288    |
|    sptW.2.gcn.bias     |    256     |
| sptW.2.timeConv.weight |     3      |
|  sptW.2.timeConv.bias  |     1      |
|     sptW.4.sAtt.Vs     |    729     |
|     sptW.4.sAtt.bs     |    729     |
|     sptW.4.sAtt.W1     |    256     |
|     sptW.4.sAtt.W2     |    256     |
|     sptW.4.sAtt.W3     |     1      |
|   sptW.4.gcn.weight    |   196608   |
|    sptW.4.gcn.bias     |    256     |
| sptW.4.timeConv.weight |     3      |
|  sptW.4.timeConv.bias  |     1      |
|     sptW.6.weight      |    3072    |
|      sptW.6.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 655359
 Trial: 10 Epoch: 0, train_loss: 357378100.5390625, validation_loss = 115723.40625
========Trial 11 params: {'n_layers': 2, 'K': 3, 'Dropout': 0.3, 'Hidden0': 64, 'Hidden1': 256, 'optimizer': 'Adam', 'lr': 0.019017010532092444, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |    4608    |
|    sptH.0.gcn.bias     |     64     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.sAtt.Vs     |    729     |
|     sptH.2.sAtt.bs     |    729     |
|     sptH.2.sAtt.W1     |     64     |
|     sptH.2.sAtt.W2     |     64     |
|     sptH.2.sAtt.W3     |     1      |
|   sptH.2.gcn.weight    |   49152    |
|    sptH.2.gcn.bias     |    256     |
| sptH.2.timeConv.weight |     3      |
|  sptH.2.timeConv.bias  |     1      |
|     sptH.4.weight      |    3072    |
|      sptH.4.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    2304    |
|    sptD.0.gcn.bias     |     64     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.sAtt.Vs     |    729     |
|     sptD.2.sAtt.bs     |    729     |
|     sptD.2.sAtt.W1     |     64     |
|     sptD.2.sAtt.W2     |     64     |
|     sptD.2.sAtt.W3     |     1      |
|   sptD.2.gcn.weight    |   49152    |
|    sptD.2.gcn.bias     |    256     |
| sptD.2.timeConv.weight |     3      |
|  sptD.2.timeConv.bias  |     1      |
|     sptD.4.weight      |    3072    |
|      sptD.4.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |    4608    |
|    sptW.0.gcn.bias     |     64     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.sAtt.Vs     |    729     |
|     sptW.2.sAtt.bs     |    729     |
|     sptW.2.sAtt.W1     |     64     |
|     sptW.2.sAtt.W2     |     64     |
|     sptW.2.sAtt.W3     |     1      |
|   sptW.2.gcn.weight    |   49152    |
|    sptW.2.gcn.bias     |    256     |
| sptW.2.timeConv.weight |     3      |
|  sptW.2.timeConv.bias  |     1      |
|     sptW.4.weight      |    3072    |
|      sptW.4.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 178506
 Trial: 11 Epoch: 0, train_loss: 135172.10034179688, validation_loss = 6108.5146484375
 Trial: 11 Epoch: 1, train_loss: 62587.281005859375, validation_loss = 6967.42724609375
 Trial: 11 Epoch: 2, train_loss: 76884.96508789062, validation_loss = 8822.572265625
 Trial: 11 Epoch: 3, train_loss: 63399.89208984375, validation_loss = 9312.683837890625
 Trial: 11 Epoch: 4, train_loss: 48261.50744628906, validation_loss = 5883.5595703125
 Trial: 11 Epoch: 5, train_loss: 39854.91650390625, validation_loss = 6400.9306640625
 Trial: 11 Epoch: 6, train_loss: 40380.96228027344, validation_loss = 6851.079833984375
 Trial: 11 Epoch: 7, train_loss: 42506.75598144531, validation_loss = 4879.6552734375
 Trial: 11 Epoch: 8, train_loss: 42053.22216796875, validation_loss = 4995.38720703125
 Trial: 11 Epoch: 9, train_loss: 43976.54064941406, validation_loss = 7249.574462890625
 Trial: 11 Epoch: 10, train_loss: 66899.29467773438, validation_loss = 6206.6494140625
 Trial: 11 Epoch: 11, train_loss: 47774.83447265625, validation_loss = 5472.56640625
 Trial: 11 Epoch: 12, train_loss: 39409.54345703125, validation_loss = 5051.6298828125
 Trial: 11 Epoch: 13, train_loss: 37205.922607421875, validation_loss = 8539.402587890625
 Trial: 11 Epoch: 14, train_loss: 39680.4990234375, validation_loss = 5219.05224609375
 Trial: 11 Epoch: 15, train_loss: 31276.572021484375, validation_loss = 5521.880859375
 Trial: 11 Epoch: 16, train_loss: 34278.90100097656, validation_loss = 5699.158203125
 Trial: 11 Epoch: 17, train_loss: 31463.052734375, validation_loss = 4884.5869140625
 Trial: 11 Epoch: 18, train_loss: 32079.517211914062, validation_loss = 4870.97265625
 Trial: 11 Epoch: 19, train_loss: 39231.923828125, validation_loss = 7951.124755859375
 Trial: 11 Epoch: 20, train_loss: 38251.537841796875, validation_loss = 3783.693359375
 Trial: 11 Epoch: 21, train_loss: 37701.5673828125, validation_loss = 5096.09423828125
 Trial: 11 Epoch: 22, train_loss: 31821.854248046875, validation_loss = 6397.06494140625
 Trial: 11 Epoch: 23, train_loss: 31518.872802734375, validation_loss = 4331.3494873046875
 Trial: 11 Epoch: 24, train_loss: 29415.201416015625, validation_loss = 6160.515380859375
 Trial: 11 Epoch: 25, train_loss: 34946.22692871094, validation_loss = 5635.676025390625
 Trial: 11 Epoch: 26, train_loss: 31974.454711914062, validation_loss = 8151.581298828125
 Trial: 11 Epoch: 27, train_loss: 33679.49621582031, validation_loss = 3714.3995361328125
 Trial: 11 Epoch: 28, train_loss: 32499.90771484375, validation_loss = 4046.588134765625
 Trial: 11 Epoch: 29, train_loss: 27030.106079101562, validation_loss = 5110.628662109375
 Trial: 11 Epoch: 30, train_loss: 28218.871459960938, validation_loss = 4378.0753173828125
 Trial: 11 Epoch: 31, train_loss: 30175.335327148438, validation_loss = 5591.009521484375
 Trial: 11 Epoch: 32, train_loss: 29259.003173828125, validation_loss = 4336.251708984375
 Trial: 11 Epoch: 33, train_loss: 32230.169555664062, validation_loss = 6809.292236328125
 Trial: 11 Epoch: 34, train_loss: 30171.502197265625, validation_loss = 3976.727294921875
 Trial: 11 Epoch: 35, train_loss: 29358.439331054688, validation_loss = 4198.9066162109375
 Trial: 11 Epoch: 36, train_loss: 29550.640991210938, validation_loss = 5216.109130859375
 Trial: 11 Epoch: 37, train_loss: 26374.001342773438, validation_loss = 4107.337158203125
 Trial: 11 Epoch: 38, train_loss: 26961.636962890625, validation_loss = 4187.520751953125
 Trial: 11 Epoch: 39, train_loss: 29232.53564453125, validation_loss = 4146.964599609375
 Trial: 11 Epoch: 40, train_loss: 25181.521606445312, validation_loss = 4931.0128173828125
 Trial: 11 Epoch: 41, train_loss: 28310.081665039062, validation_loss = 4043.56591796875
 Trial: 11 Epoch: 42, train_loss: 31236.036254882812, validation_loss = 5909.23388671875
 Trial: 11 Epoch: 43, train_loss: 27369.8916015625, validation_loss = 4163.82373046875
 Trial: 11 Epoch: 44, train_loss: 26326.871459960938, validation_loss = 4203.6844482421875
 Trial: 11 Epoch: 45, train_loss: 26368.829223632812, validation_loss = 4052.1253662109375
 Trial: 11 Epoch: 46, train_loss: 28373.869018554688, validation_loss = 5929.51318359375
 Trial: 11 Epoch: 47, train_loss: 27451.210815429688, validation_loss = 4296.8746337890625
 Trial: 11 Epoch: 48, train_loss: 24819.390625, validation_loss = 4372.7078857421875
 Trial: 11 Epoch: 49, train_loss: 24516.169189453125, validation_loss = 3623.0888671875
 Trial: 11 Epoch: 50, train_loss: 27024.983459472656, validation_loss = 5221.54833984375
 Trial: 11 Epoch: 51, train_loss: 30454.697387695312, validation_loss = 4247.407470703125
 Trial: 11 Epoch: 52, train_loss: 27900.145385742188, validation_loss = 3465.2542724609375
 Trial: 11 Epoch: 53, train_loss: 28993.404541015625, validation_loss = 6290.833251953125
 Trial: 11 Epoch: 54, train_loss: 28092.044677734375, validation_loss = 3832.4852294921875
 Trial: 11 Epoch: 55, train_loss: 24607.309936523438, validation_loss = 4168.912353515625
 Trial: 11 Epoch: 56, train_loss: 26661.31298828125, validation_loss = 5103.6318359375
 Trial: 11 Epoch: 57, train_loss: 27193.003784179688, validation_loss = 3670.7978515625
 Trial: 11 Epoch: 58, train_loss: 24887.6708984375, validation_loss = 4155.888671875
 Trial: 11 Epoch: 59, train_loss: 24502.822875976562, validation_loss = 6627.00341796875
 Trial: 11 Epoch: 60, train_loss: 27535.334594726562, validation_loss = 4606.80859375
 Trial: 11 Epoch: 61, train_loss: 29835.675659179688, validation_loss = 6715.885009765625
 Trial: 11 Epoch: 62, train_loss: 28609.01953125, validation_loss = 4016.335693359375
 Trial: 11 Epoch: 63, train_loss: 24661.761962890625, validation_loss = 4354.8111572265625
 Trial: 11 Epoch: 64, train_loss: 27159.19189453125, validation_loss = 5431.969482421875
 Trial: 11 Epoch: 65, train_loss: 33383.86437988281, validation_loss = 4800.474853515625
 Trial: 11 Epoch: 66, train_loss: 30619.233154296875, validation_loss = 4285.465087890625
 Trial: 11 Epoch: 67, train_loss: 27834.790771484375, validation_loss = 5437.104248046875
 Trial: 11 Epoch: 68, train_loss: 26913.307983398438, validation_loss = 5217.701904296875
 Trial: 11 Epoch: 69, train_loss: 26333.708374023438, validation_loss = 4011.664794921875
 Trial: 11 Epoch: 70, train_loss: 25723.281982421875, validation_loss = 4583.465576171875
 Trial: 11 Epoch: 71, train_loss: 25289.704956054688, validation_loss = 3729.9410400390625
 Trial: 11 Epoch: 72, train_loss: 25325.033447265625, validation_loss = 3723.2269287109375
 Trial: 11 Epoch: 73, train_loss: 27819.966918945312, validation_loss = 4607.192138671875
 Trial: 11 Epoch: 74, train_loss: 27646.345947265625, validation_loss = 4982.787841796875
 Trial: 11 Epoch: 75, train_loss: 24359.12158203125, validation_loss = 3920.763916015625
 Trial: 11 Epoch: 76, train_loss: 25023.12109375, validation_loss = 4575.56005859375
 Trial: 11 Epoch: 77, train_loss: 29480.120361328125, validation_loss = 4469.50634765625
 Trial: 11 Epoch: 78, train_loss: 33437.61169433594, validation_loss = 5150.277099609375
 Trial: 11 Epoch: 79, train_loss: 26620.951782226562, validation_loss = 3938.826416015625
 Trial: 11 Epoch: 80, train_loss: 25775.372192382812, validation_loss = 3514.8465576171875
 Trial: 11 Epoch: 81, train_loss: 29305.904418945312, validation_loss = 4155.9669189453125
 Trial: 11 Epoch: 82, train_loss: 24072.986694335938, validation_loss = 4227.7200927734375
 Trial: 11 Epoch: 83, train_loss: 24784.140502929688, validation_loss = 4051.922607421875
 Trial: 11 Epoch: 84, train_loss: 26433.5224609375, validation_loss = 4272.709228515625
 Trial: 11 Epoch: 85, train_loss: 25659.557006835938, validation_loss = 3801.08447265625
 Trial: 11 Epoch: 86, train_loss: 22641.335815429688, validation_loss = 4165.8287353515625
 Trial: 11 Epoch: 87, train_loss: 23513.342407226562, validation_loss = 3706.927001953125
 Trial: 11 Epoch: 88, train_loss: 23993.11395263672, validation_loss = 3897.0010986328125
 Trial: 11 Epoch: 89, train_loss: 22650.68701171875, validation_loss = 3801.7060546875
 Trial: 11 Epoch: 90, train_loss: 22595.95977783203, validation_loss = 4340.0308837890625
 Trial: 11 Epoch: 91, train_loss: 22988.214233398438, validation_loss = 4305.9013671875
 Trial: 11 Epoch: 92, train_loss: 27487.113403320312, validation_loss = 3612.2435302734375
 Trial: 11 Epoch: 93, train_loss: 25553.114135742188, validation_loss = 4301.541259765625
 Trial: 11 Epoch: 94, train_loss: 25213.807495117188, validation_loss = 4384.4537353515625
 Trial: 11 Epoch: 95, train_loss: 24837.971801757812, validation_loss = 3714.84033203125
 Trial: 11 Epoch: 96, train_loss: 24566.054443359375, validation_loss = 4045.029052734375
 Trial: 11 Epoch: 97, train_loss: 24245.01708984375, validation_loss = 4380.456787109375
 Trial: 11 Epoch: 98, train_loss: 25887.868286132812, validation_loss = 4525.228759765625
 Trial: 11 Epoch: 99, train_loss: 23713.48779296875, validation_loss = 4416.869140625
========Trial 12 params: {'n_layers': 2, 'K': 3, 'Dropout': 0.1, 'Hidden0': 32, 'Hidden1': 16, 'optimizer': 'Adam', 'lr': 0.07660842195297392, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |    2304    |
|    sptH.0.gcn.bias     |     32     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.sAtt.Vs     |    729     |
|     sptH.2.sAtt.bs     |    729     |
|     sptH.2.sAtt.W1     |     32     |
|     sptH.2.sAtt.W2     |     32     |
|     sptH.2.sAtt.W3     |     1      |
|   sptH.2.gcn.weight    |    1536    |
|    sptH.2.gcn.bias     |     16     |
| sptH.2.timeConv.weight |     3      |
|  sptH.2.timeConv.bias  |     1      |
|     sptH.4.weight      |    192     |
|      sptH.4.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    1152    |
|    sptD.0.gcn.bias     |     32     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.sAtt.Vs     |    729     |
|     sptD.2.sAtt.bs     |    729     |
|     sptD.2.sAtt.W1     |     32     |
|     sptD.2.sAtt.W2     |     32     |
|     sptD.2.sAtt.W3     |     1      |
|   sptD.2.gcn.weight    |    1536    |
|    sptD.2.gcn.bias     |     16     |
| sptD.2.timeConv.weight |     3      |
|  sptD.2.timeConv.bias  |     1      |
|     sptD.4.weight      |    192     |
|      sptD.4.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |    2304    |
|    sptW.0.gcn.bias     |     32     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.sAtt.Vs     |    729     |
|     sptW.2.sAtt.bs     |    729     |
|     sptW.2.sAtt.W1     |     32     |
|     sptW.2.sAtt.W2     |     32     |
|     sptW.2.sAtt.W3     |     1      |
|   sptW.2.gcn.weight    |    1536    |
|    sptW.2.gcn.bias     |     16     |
| sptW.2.timeConv.weight |     3      |
|  sptW.2.timeConv.bias  |     1      |
|     sptW.4.weight      |    192     |
|      sptW.4.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 20250
 Trial: 12 Epoch: 0, train_loss: 1404963.240234375, validation_loss = 16831.78076171875
 Trial: 12 Epoch: 1, train_loss: 122305.359375, validation_loss = 12229.068359375
 Trial: 12 Epoch: 2, train_loss: 95620.78076171875, validation_loss = 10935.880859375
 Trial: 12 Epoch: 3, train_loss: 92723.46240234375, validation_loss = 13393.42431640625
 Trial: 12 Epoch: 4, train_loss: 81040.712890625, validation_loss = 10097.58251953125
 Trial: 12 Epoch: 5, train_loss: 81099.49145507812, validation_loss = 13433.380859375
 Trial: 12 Epoch: 6, train_loss: 90869.841796875, validation_loss = 10676.8330078125
========Trial 13 params: {'n_layers': 2, 'K': 3, 'Dropout': 0.3, 'Hidden0': 32, 'Hidden1': 128, 'optimizer': 'Adam', 'lr': 0.015109163907323291, 'batch': 32}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |    2304    |
|    sptH.0.gcn.bias     |     32     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.sAtt.Vs     |    729     |
|     sptH.2.sAtt.bs     |    729     |
|     sptH.2.sAtt.W1     |     32     |
|     sptH.2.sAtt.W2     |     32     |
|     sptH.2.sAtt.W3     |     1      |
|   sptH.2.gcn.weight    |   12288    |
|    sptH.2.gcn.bias     |    128     |
| sptH.2.timeConv.weight |     3      |
|  sptH.2.timeConv.bias  |     1      |
|     sptH.4.weight      |    1536    |
|      sptH.4.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    1152    |
|    sptD.0.gcn.bias     |     32     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.sAtt.Vs     |    729     |
|     sptD.2.sAtt.bs     |    729     |
|     sptD.2.sAtt.W1     |     32     |
|     sptD.2.sAtt.W2     |     32     |
|     sptD.2.sAtt.W3     |     1      |
|   sptD.2.gcn.weight    |   12288    |
|    sptD.2.gcn.bias     |    128     |
| sptD.2.timeConv.weight |     3      |
|  sptD.2.timeConv.bias  |     1      |
|     sptD.4.weight      |    1536    |
|      sptD.4.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |    2304    |
|    sptW.0.gcn.bias     |     32     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.sAtt.Vs     |    729     |
|     sptW.2.sAtt.bs     |    729     |
|     sptW.2.sAtt.W1     |     32     |
|     sptW.2.sAtt.W2     |     32     |
|     sptW.2.sAtt.W3     |     1      |
|   sptW.2.gcn.weight    |   12288    |
|    sptW.2.gcn.bias     |    128     |
| sptW.2.timeConv.weight |     3      |
|  sptW.2.timeConv.bias  |     1      |
|     sptW.4.weight      |    1536    |
|      sptW.4.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 56874
 Trial: 13 Epoch: 0, train_loss: 115673.75390625, validation_loss = 16727.2421875
 Trial: 13 Epoch: 1, train_loss: 49802.228515625, validation_loss = 20727.041015625
 Trial: 13 Epoch: 2, train_loss: 40095.935546875, validation_loss = 8974.419921875
 Trial: 13 Epoch: 3, train_loss: 25731.58984375, validation_loss = 10470.779296875
 Trial: 13 Epoch: 4, train_loss: 21989.5185546875, validation_loss = 6863.67431640625
 Trial: 13 Epoch: 5, train_loss: 17764.4873046875, validation_loss = 10004.3671875
 Trial: 13 Epoch: 6, train_loss: 16996.37158203125, validation_loss = 6581.65087890625
 Trial: 13 Epoch: 7, train_loss: 15713.3134765625, validation_loss = 7579.8828125
 Trial: 13 Epoch: 8, train_loss: 15271.2490234375, validation_loss = 5244.55078125
 Trial: 13 Epoch: 9, train_loss: 14142.4931640625, validation_loss = 5891.44921875
 Trial: 13 Epoch: 10, train_loss: 13594.0, validation_loss = 5261.5361328125
 Trial: 13 Epoch: 11, train_loss: 11902.693359375, validation_loss = 6362.7685546875
 Trial: 13 Epoch: 12, train_loss: 12574.92724609375, validation_loss = 4797.751953125
 Trial: 13 Epoch: 13, train_loss: 10780.1328125, validation_loss = 4324.31689453125
 Trial: 13 Epoch: 14, train_loss: 12296.91162109375, validation_loss = 6623.232421875
 Trial: 13 Epoch: 15, train_loss: 12293.49267578125, validation_loss = 4562.79052734375
 Trial: 13 Epoch: 16, train_loss: 11401.751953125, validation_loss = 5283.6650390625
 Trial: 13 Epoch: 17, train_loss: 10878.5302734375, validation_loss = 4449.244140625
 Trial: 13 Epoch: 18, train_loss: 10847.78955078125, validation_loss = 4110.19970703125
 Trial: 13 Epoch: 19, train_loss: 10752.56494140625, validation_loss = 5444.1220703125
 Trial: 13 Epoch: 20, train_loss: 10483.03369140625, validation_loss = 4746.68115234375
 Trial: 13 Epoch: 21, train_loss: 10711.8203125, validation_loss = 4467.31396484375
 Trial: 13 Epoch: 22, train_loss: 9511.70703125, validation_loss = 3807.5322265625
 Trial: 13 Epoch: 23, train_loss: 9574.22119140625, validation_loss = 4151.91064453125
 Trial: 13 Epoch: 24, train_loss: 9349.49658203125, validation_loss = 4356.99365234375
 Trial: 13 Epoch: 25, train_loss: 10062.181640625, validation_loss = 4235.447265625
 Trial: 13 Epoch: 26, train_loss: 9680.1787109375, validation_loss = 3984.3955078125
 Trial: 13 Epoch: 27, train_loss: 9049.8603515625, validation_loss = 4742.59033203125
 Trial: 13 Epoch: 28, train_loss: 8612.26171875, validation_loss = 3849.070556640625
 Trial: 13 Epoch: 29, train_loss: 8921.3115234375, validation_loss = 3946.3369140625
 Trial: 13 Epoch: 30, train_loss: 8621.834716796875, validation_loss = 4288.68212890625
 Trial: 13 Epoch: 31, train_loss: 8573.40380859375, validation_loss = 4175.91845703125
 Trial: 13 Epoch: 32, train_loss: 9277.36865234375, validation_loss = 4442.83349609375
 Trial: 13 Epoch: 33, train_loss: 8654.417724609375, validation_loss = 4085.646728515625
 Trial: 13 Epoch: 34, train_loss: 9939.9970703125, validation_loss = 4192.1171875
 Trial: 13 Epoch: 35, train_loss: 8774.21923828125, validation_loss = 4089.76708984375
 Trial: 13 Epoch: 36, train_loss: 8982.58984375, validation_loss = 4700.27978515625
 Trial: 13 Epoch: 37, train_loss: 8884.962890625, validation_loss = 3886.00146484375
 Trial: 13 Epoch: 38, train_loss: 9027.986328125, validation_loss = 4269.56787109375
 Trial: 13 Epoch: 39, train_loss: 9603.626708984375, validation_loss = 3979.4345703125
 Trial: 13 Epoch: 40, train_loss: 8807.84130859375, validation_loss = 4677.06005859375
 Trial: 13 Epoch: 41, train_loss: 8914.850830078125, validation_loss = 3318.601806640625
 Trial: 13 Epoch: 42, train_loss: 9284.3359375, validation_loss = 3529.050537109375
 Trial: 13 Epoch: 43, train_loss: 8169.928466796875, validation_loss = 3699.63818359375
 Trial: 13 Epoch: 44, train_loss: 8474.900634765625, validation_loss = 4474.6748046875
 Trial: 13 Epoch: 45, train_loss: 8707.476806640625, validation_loss = 3569.30126953125
 Trial: 13 Epoch: 46, train_loss: 7966.5654296875, validation_loss = 5227.6796875
 Trial: 13 Epoch: 47, train_loss: 9325.2646484375, validation_loss = 4396.3388671875
 Trial: 13 Epoch: 48, train_loss: 9430.216796875, validation_loss = 4479.79443359375
 Trial: 13 Epoch: 49, train_loss: 9600.3125, validation_loss = 3442.33935546875
 Trial: 13 Epoch: 50, train_loss: 8650.99462890625, validation_loss = 3964.93505859375
 Trial: 13 Epoch: 51, train_loss: 9166.92578125, validation_loss = 4782.4814453125
 Trial: 13 Epoch: 52, train_loss: 8287.9404296875, validation_loss = 4085.1162109375
 Trial: 13 Epoch: 53, train_loss: 8525.026123046875, validation_loss = 5884.08056640625
 Trial: 13 Epoch: 54, train_loss: 9585.33447265625, validation_loss = 3971.62255859375
 Trial: 13 Epoch: 55, train_loss: 9654.63037109375, validation_loss = 4087.495849609375
 Trial: 13 Epoch: 56, train_loss: 9281.2265625, validation_loss = 4541.1279296875
 Trial: 13 Epoch: 57, train_loss: 10590.12646484375, validation_loss = 3984.575439453125
 Trial: 13 Epoch: 58, train_loss: 9237.85302734375, validation_loss = 4353.7021484375
 Trial: 13 Epoch: 59, train_loss: 9790.775634765625, validation_loss = 3923.494140625
 Trial: 13 Epoch: 60, train_loss: 8285.879638671875, validation_loss = 3909.64697265625
 Trial: 13 Epoch: 61, train_loss: 7735.05615234375, validation_loss = 3501.3115234375
 Trial: 13 Epoch: 62, train_loss: 8082.708251953125, validation_loss = 4039.478515625
 Trial: 13 Epoch: 63, train_loss: 8567.814208984375, validation_loss = 3686.667724609375
 Trial: 13 Epoch: 64, train_loss: 7630.4814453125, validation_loss = 3696.679443359375
 Trial: 13 Epoch: 65, train_loss: 7657.78173828125, validation_loss = 3984.8115234375
 Trial: 13 Epoch: 66, train_loss: 7672.69091796875, validation_loss = 3605.025634765625
 Trial: 13 Epoch: 67, train_loss: 8225.63720703125, validation_loss = 3897.205322265625
 Trial: 13 Epoch: 68, train_loss: 8356.86962890625, validation_loss = 3421.472412109375
 Trial: 13 Epoch: 69, train_loss: 8534.990478515625, validation_loss = 3501.4853515625
 Trial: 13 Epoch: 70, train_loss: 8537.76025390625, validation_loss = 3862.43408203125
 Trial: 13 Epoch: 71, train_loss: 8312.123291015625, validation_loss = 3570.653076171875
 Trial: 13 Epoch: 72, train_loss: 7201.106201171875, validation_loss = 3974.527099609375
 Trial: 13 Epoch: 73, train_loss: 7873.505615234375, validation_loss = 3735.440185546875
 Trial: 13 Epoch: 74, train_loss: 8070.056396484375, validation_loss = 3929.45263671875
 Trial: 13 Epoch: 75, train_loss: 7228.600341796875, validation_loss = 4044.2509765625
 Trial: 13 Epoch: 76, train_loss: 7110.66259765625, validation_loss = 4168.8056640625
 Trial: 13 Epoch: 77, train_loss: 7680.251708984375, validation_loss = 3633.509033203125
 Trial: 13 Epoch: 78, train_loss: 7418.87744140625, validation_loss = 3673.980224609375
 Trial: 13 Epoch: 79, train_loss: 7393.2470703125, validation_loss = 3468.482177734375
 Trial: 13 Epoch: 80, train_loss: 7249.09423828125, validation_loss = 3910.083740234375
 Trial: 13 Epoch: 81, train_loss: 7348.015625, validation_loss = 3815.7451171875
 Trial: 13 Epoch: 82, train_loss: 7010.790283203125, validation_loss = 3714.68359375
 Trial: 13 Epoch: 83, train_loss: 7397.9677734375, validation_loss = 3654.95458984375
 Trial: 13 Epoch: 84, train_loss: 7540.42431640625, validation_loss = 3294.569580078125
 Trial: 13 Epoch: 85, train_loss: 7828.810546875, validation_loss = 3396.63623046875
 Trial: 13 Epoch: 86, train_loss: 7274.63427734375, validation_loss = 3273.997802734375
 Trial: 13 Epoch: 87, train_loss: 7785.62744140625, validation_loss = 3293.717529296875
 Trial: 13 Epoch: 88, train_loss: 7515.4365234375, validation_loss = 4051.80517578125
 Trial: 13 Epoch: 89, train_loss: 8587.26611328125, validation_loss = 3384.717041015625
 Trial: 13 Epoch: 90, train_loss: 7678.92724609375, validation_loss = 4184.96240234375
 Trial: 13 Epoch: 91, train_loss: 8220.925048828125, validation_loss = 4069.6728515625
 Trial: 13 Epoch: 92, train_loss: 7275.47412109375, validation_loss = 3490.03271484375
 Trial: 13 Epoch: 93, train_loss: 7594.659423828125, validation_loss = 3451.38623046875
 Trial: 13 Epoch: 94, train_loss: 7165.08642578125, validation_loss = 3545.14794921875
 Trial: 13 Epoch: 95, train_loss: 7953.40380859375, validation_loss = 3941.6767578125
 Trial: 13 Epoch: 96, train_loss: 6924.43994140625, validation_loss = 3721.235595703125
 Trial: 13 Epoch: 97, train_loss: 7445.5234375, validation_loss = 3233.389404296875
 Trial: 13 Epoch: 98, train_loss: 7279.100341796875, validation_loss = 3661.92578125
 Trial: 13 Epoch: 99, train_loss: 7190.26513671875, validation_loss = 3415.870361328125
========Trial 14 params: {'n_layers': 3, 'K': 3, 'Dropout': 0.3, 'Hidden0': 32, 'Hidden1': 128, 'Hidden2': 4, 'optimizer': 'Adam', 'lr': 0.015450983192582262, 'batch': 32}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |    2304    |
|    sptH.0.gcn.bias     |     32     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.sAtt.Vs     |    729     |
|     sptH.2.sAtt.bs     |    729     |
|     sptH.2.sAtt.W1     |     32     |
|     sptH.2.sAtt.W2     |     32     |
|     sptH.2.sAtt.W3     |     1      |
|   sptH.2.gcn.weight    |   12288    |
|    sptH.2.gcn.bias     |    128     |
| sptH.2.timeConv.weight |     3      |
|  sptH.2.timeConv.bias  |     1      |
|     sptH.4.sAtt.Vs     |    729     |
|     sptH.4.sAtt.bs     |    729     |
|     sptH.4.sAtt.W1     |    128     |
|     sptH.4.sAtt.W2     |    128     |
|     sptH.4.sAtt.W3     |     1      |
|   sptH.4.gcn.weight    |    1536    |
|    sptH.4.gcn.bias     |     4      |
| sptH.4.timeConv.weight |     3      |
|  sptH.4.timeConv.bias  |     1      |
|     sptH.6.weight      |     48     |
|      sptH.6.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    1152    |
|    sptD.0.gcn.bias     |     32     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.sAtt.Vs     |    729     |
|     sptD.2.sAtt.bs     |    729     |
|     sptD.2.sAtt.W1     |     32     |
|     sptD.2.sAtt.W2     |     32     |
|     sptD.2.sAtt.W3     |     1      |
|   sptD.2.gcn.weight    |   12288    |
|    sptD.2.gcn.bias     |    128     |
| sptD.2.timeConv.weight |     3      |
|  sptD.2.timeConv.bias  |     1      |
|     sptD.4.sAtt.Vs     |    729     |
|     sptD.4.sAtt.bs     |    729     |
|     sptD.4.sAtt.W1     |    128     |
|     sptD.4.sAtt.W2     |    128     |
|     sptD.4.sAtt.W3     |     1      |
|   sptD.4.gcn.weight    |    1536    |
|    sptD.4.gcn.bias     |     4      |
| sptD.4.timeConv.weight |     3      |
|  sptD.4.timeConv.bias  |     1      |
|     sptD.6.weight      |     48     |
|      sptD.6.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |    2304    |
|    sptW.0.gcn.bias     |     32     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.sAtt.Vs     |    729     |
|     sptW.2.sAtt.bs     |    729     |
|     sptW.2.sAtt.W1     |     32     |
|     sptW.2.sAtt.W2     |     32     |
|     sptW.2.sAtt.W3     |     1      |
|   sptW.2.gcn.weight    |   12288    |
|    sptW.2.gcn.bias     |    128     |
| sptW.2.timeConv.weight |     3      |
|  sptW.2.timeConv.bias  |     1      |
|     sptW.4.sAtt.Vs     |    729     |
|     sptW.4.sAtt.bs     |    729     |
|     sptW.4.sAtt.W1     |    128     |
|     sptW.4.sAtt.W2     |    128     |
|     sptW.4.sAtt.W3     |     1      |
|   sptW.4.gcn.weight    |    1536    |
|    sptW.4.gcn.bias     |     4      |
| sptW.4.timeConv.weight |     3      |
|  sptW.4.timeConv.bias  |     1      |
|     sptW.6.weight      |     48     |
|      sptW.6.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 62187
 Trial: 14 Epoch: 0, train_loss: 205774.6640625, validation_loss = 36673.17578125
========Trial 15 params: {'n_layers': 2, 'K': 2, 'Dropout': 0.3, 'Hidden0': 64, 'Hidden1': 128, 'optimizer': 'Adam', 'lr': 0.01332673118665073, 'batch': 32}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |    3072    |
|    sptH.0.gcn.bias     |     64     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.sAtt.Vs     |    729     |
|     sptH.2.sAtt.bs     |    729     |
|     sptH.2.sAtt.W1     |     64     |
|     sptH.2.sAtt.W2     |     64     |
|     sptH.2.sAtt.W3     |     1      |
|   sptH.2.gcn.weight    |   16384    |
|    sptH.2.gcn.bias     |    128     |
| sptH.2.timeConv.weight |     3      |
|  sptH.2.timeConv.bias  |     1      |
|     sptH.4.weight      |    1536    |
|      sptH.4.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    1536    |
|    sptD.0.gcn.bias     |     64     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.sAtt.Vs     |    729     |
|     sptD.2.sAtt.bs     |    729     |
|     sptD.2.sAtt.W1     |     64     |
|     sptD.2.sAtt.W2     |     64     |
|     sptD.2.sAtt.W3     |     1      |
|   sptD.2.gcn.weight    |   16384    |
|    sptD.2.gcn.bias     |    128     |
| sptD.2.timeConv.weight |     3      |
|  sptD.2.timeConv.bias  |     1      |
|     sptD.4.weight      |    1536    |
|      sptD.4.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |    3072    |
|    sptW.0.gcn.bias     |     64     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.sAtt.Vs     |    729     |
|     sptW.2.sAtt.bs     |    729     |
|     sptW.2.sAtt.W1     |     64     |
|     sptW.2.sAtt.W2     |     64     |
|     sptW.2.sAtt.W3     |     1      |
|   sptW.2.gcn.weight    |   16384    |
|    sptW.2.gcn.bias     |    128     |
| sptW.2.timeConv.weight |     3      |
|  sptW.2.timeConv.bias  |     1      |
|     sptW.4.weight      |    1536    |
|      sptW.4.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 71370
 Trial: 15 Epoch: 0, train_loss: 116663.36328125, validation_loss = 34581.046875
========Trial 16 params: {'n_layers': 2, 'K': 3, 'Dropout': 0.3, 'Hidden0': 128, 'Hidden1': 8, 'optimizer': 'Adam', 'lr': 0.030918739879884277, 'batch': 32}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |    9216    |
|    sptH.0.gcn.bias     |    128     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.sAtt.Vs     |    729     |
|     sptH.2.sAtt.bs     |    729     |
|     sptH.2.sAtt.W1     |    128     |
|     sptH.2.sAtt.W2     |    128     |
|     sptH.2.sAtt.W3     |     1      |
|   sptH.2.gcn.weight    |    3072    |
|    sptH.2.gcn.bias     |     8      |
| sptH.2.timeConv.weight |     3      |
|  sptH.2.timeConv.bias  |     1      |
|     sptH.4.weight      |     96     |
|      sptH.4.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    4608    |
|    sptD.0.gcn.bias     |    128     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.sAtt.Vs     |    729     |
|     sptD.2.sAtt.bs     |    729     |
|     sptD.2.sAtt.W1     |    128     |
|     sptD.2.sAtt.W2     |    128     |
|     sptD.2.sAtt.W3     |     1      |
|   sptD.2.gcn.weight    |    3072    |
|    sptD.2.gcn.bias     |     8      |
| sptD.2.timeConv.weight |     3      |
|  sptD.2.timeConv.bias  |     1      |
|     sptD.4.weight      |     96     |
|      sptD.4.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |    9216    |
|    sptW.0.gcn.bias     |    128     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.sAtt.Vs     |    729     |
|     sptW.2.sAtt.bs     |    729     |
|     sptW.2.sAtt.W1     |    128     |
|     sptW.2.sAtt.W2     |    128     |
|     sptW.2.sAtt.W3     |     1      |
|   sptW.2.gcn.weight    |    3072    |
|    sptW.2.gcn.bias     |     8      |
| sptW.2.timeConv.weight |     3      |
|  sptW.2.timeConv.bias  |     1      |
|     sptW.4.weight      |     96     |
|      sptW.4.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 42690
 Trial: 16 Epoch: 0, train_loss: 246200.7734375, validation_loss = 31398.220703125
========Trial 17 params: {'n_layers': 3, 'K': 3, 'Dropout': 0.3, 'Hidden0': 16, 'Hidden1': 32, 'Hidden2': 64, 'optimizer': 'Adam', 'lr': 0.06353904251512903, 'batch': 16}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |    1152    |
|    sptH.0.gcn.bias     |     16     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.sAtt.Vs     |    729     |
|     sptH.2.sAtt.bs     |    729     |
|     sptH.2.sAtt.W1     |     16     |
|     sptH.2.sAtt.W2     |     16     |
|     sptH.2.sAtt.W3     |     1      |
|   sptH.2.gcn.weight    |    1536    |
|    sptH.2.gcn.bias     |     32     |
| sptH.2.timeConv.weight |     3      |
|  sptH.2.timeConv.bias  |     1      |
|     sptH.4.sAtt.Vs     |    729     |
|     sptH.4.sAtt.bs     |    729     |
|     sptH.4.sAtt.W1     |     32     |
|     sptH.4.sAtt.W2     |     32     |
|     sptH.4.sAtt.W3     |     1      |
|   sptH.4.gcn.weight    |    6144    |
|    sptH.4.gcn.bias     |     64     |
| sptH.4.timeConv.weight |     3      |
|  sptH.4.timeConv.bias  |     1      |
|     sptH.6.weight      |    768     |
|      sptH.6.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    576     |
|    sptD.0.gcn.bias     |     16     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.sAtt.Vs     |    729     |
|     sptD.2.sAtt.bs     |    729     |
|     sptD.2.sAtt.W1     |     16     |
|     sptD.2.sAtt.W2     |     16     |
|     sptD.2.sAtt.W3     |     1      |
|   sptD.2.gcn.weight    |    1536    |
|    sptD.2.gcn.bias     |     32     |
| sptD.2.timeConv.weight |     3      |
|  sptD.2.timeConv.bias  |     1      |
|     sptD.4.sAtt.Vs     |    729     |
|     sptD.4.sAtt.bs     |    729     |
|     sptD.4.sAtt.W1     |     32     |
|     sptD.4.sAtt.W2     |     32     |
|     sptD.4.sAtt.W3     |     1      |
|   sptD.4.gcn.weight    |    6144    |
|    sptD.4.gcn.bias     |     64     |
| sptD.4.timeConv.weight |     3      |
|  sptD.4.timeConv.bias  |     1      |
|     sptD.6.weight      |    768     |
|      sptD.6.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |    1152    |
|    sptW.0.gcn.bias     |     16     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.sAtt.Vs     |    729     |
|     sptW.2.sAtt.bs     |    729     |
|     sptW.2.sAtt.W1     |     16     |
|     sptW.2.sAtt.W2     |     16     |
|     sptW.2.sAtt.W3     |     1      |
|   sptW.2.gcn.weight    |    1536    |
|    sptW.2.gcn.bias     |     32     |
| sptW.2.timeConv.weight |     3      |
|  sptW.2.timeConv.bias  |     1      |
|     sptW.4.sAtt.Vs     |    729     |
|     sptW.4.sAtt.bs     |    729     |
|     sptW.4.sAtt.W1     |     32     |
|     sptW.4.sAtt.W2     |     32     |
|     sptW.4.sAtt.W3     |     1      |
|   sptW.4.gcn.weight    |    6144    |
|    sptW.4.gcn.bias     |     64     |
| sptW.4.timeConv.weight |     3      |
|  sptW.4.timeConv.bias  |     1      |
|     sptW.6.weight      |    768     |
|      sptW.6.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 42207
 Trial: 17 Epoch: 0, train_loss: 2905503.265625, validation_loss = 46363.77734375
========Trial 18 params: {'n_layers': 2, 'K': 3, 'Dropout': 0.3, 'Hidden0': 8, 'Hidden1': 4, 'optimizer': 'Adam', 'lr': 0.004929605125525129, 'batch': 32}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |    576     |
|    sptH.0.gcn.bias     |     8      |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.sAtt.Vs     |    729     |
|     sptH.2.sAtt.bs     |    729     |
|     sptH.2.sAtt.W1     |     8      |
|     sptH.2.sAtt.W2     |     8      |
|     sptH.2.sAtt.W3     |     1      |
|   sptH.2.gcn.weight    |     96     |
|    sptH.2.gcn.bias     |     4      |
| sptH.2.timeConv.weight |     3      |
|  sptH.2.timeConv.bias  |     1      |
|     sptH.4.weight      |     48     |
|      sptH.4.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    288     |
|    sptD.0.gcn.bias     |     8      |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.sAtt.Vs     |    729     |
|     sptD.2.sAtt.bs     |    729     |
|     sptD.2.sAtt.W1     |     8      |
|     sptD.2.sAtt.W2     |     8      |
|     sptD.2.sAtt.W3     |     1      |
|   sptD.2.gcn.weight    |     96     |
|    sptD.2.gcn.bias     |     4      |
| sptD.2.timeConv.weight |     3      |
|  sptD.2.timeConv.bias  |     1      |
|     sptD.4.weight      |     48     |
|      sptD.4.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |    576     |
|    sptW.0.gcn.bias     |     8      |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.sAtt.Vs     |    729     |
|     sptW.2.sAtt.bs     |    729     |
|     sptW.2.sAtt.W1     |     8      |
|     sptW.2.sAtt.W2     |     8      |
|     sptW.2.sAtt.W3     |     1      |
|   sptW.2.gcn.weight    |     96     |
|    sptW.2.gcn.bias     |     4      |
| sptW.2.timeConv.weight |     3      |
|  sptW.2.timeConv.bias  |     1      |
|     sptW.4.weight      |     48     |
|      sptW.4.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 10926
 Trial: 18 Epoch: 0, train_loss: 122887.8359375, validation_loss = 41049.6484375
========Trial 19 params: {'n_layers': 3, 'K': 2, 'Dropout': 0.3, 'Hidden0': 32, 'Hidden1': 256, 'Hidden2': 32, 'optimizer': 'Adam', 'lr': 0.010013755952162305, 'batch': 32}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |    1536    |
|    sptH.0.gcn.bias     |     32     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.sAtt.Vs     |    729     |
|     sptH.2.sAtt.bs     |    729     |
|     sptH.2.sAtt.W1     |     32     |
|     sptH.2.sAtt.W2     |     32     |
|     sptH.2.sAtt.W3     |     1      |
|   sptH.2.gcn.weight    |   16384    |
|    sptH.2.gcn.bias     |    256     |
| sptH.2.timeConv.weight |     3      |
|  sptH.2.timeConv.bias  |     1      |
|     sptH.4.sAtt.Vs     |    729     |
|     sptH.4.sAtt.bs     |    729     |
|     sptH.4.sAtt.W1     |    256     |
|     sptH.4.sAtt.W2     |    256     |
|     sptH.4.sAtt.W3     |     1      |
|   sptH.4.gcn.weight    |   16384    |
|    sptH.4.gcn.bias     |     32     |
| sptH.4.timeConv.weight |     3      |
|  sptH.4.timeConv.bias  |     1      |
|     sptH.6.weight      |    384     |
|      sptH.6.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    768     |
|    sptD.0.gcn.bias     |     32     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.sAtt.Vs     |    729     |
|     sptD.2.sAtt.bs     |    729     |
|     sptD.2.sAtt.W1     |     32     |
|     sptD.2.sAtt.W2     |     32     |
|     sptD.2.sAtt.W3     |     1      |
|   sptD.2.gcn.weight    |   16384    |
|    sptD.2.gcn.bias     |    256     |
| sptD.2.timeConv.weight |     3      |
|  sptD.2.timeConv.bias  |     1      |
|     sptD.4.sAtt.Vs     |    729     |
|     sptD.4.sAtt.bs     |    729     |
|     sptD.4.sAtt.W1     |    256     |
|     sptD.4.sAtt.W2     |    256     |
|     sptD.4.sAtt.W3     |     1      |
|   sptD.4.gcn.weight    |   16384    |
|    sptD.4.gcn.bias     |     32     |
| sptD.4.timeConv.weight |     3      |
|  sptD.4.timeConv.bias  |     1      |
|     sptD.6.weight      |    384     |
|      sptD.6.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |    1536    |
|    sptW.0.gcn.bias     |     32     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.sAtt.Vs     |    729     |
|     sptW.2.sAtt.bs     |    729     |
|     sptW.2.sAtt.W1     |     32     |
|     sptW.2.sAtt.W2     |     32     |
|     sptW.2.sAtt.W3     |     1      |
|   sptW.2.gcn.weight    |   16384    |
|    sptW.2.gcn.bias     |    256     |
| sptW.2.timeConv.weight |     3      |
|  sptW.2.timeConv.bias  |     1      |
|     sptW.4.sAtt.Vs     |    729     |
|     sptW.4.sAtt.bs     |    729     |
|     sptW.4.sAtt.W1     |    256     |
|     sptW.4.sAtt.W2     |    256     |
|     sptW.4.sAtt.W3     |     1      |
|   sptW.4.gcn.weight    |   16384    |
|    sptW.4.gcn.bias     |     32     |
| sptW.4.timeConv.weight |     3      |
|  sptW.4.timeConv.bias  |     1      |
|     sptW.6.weight      |    384     |
|      sptW.6.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 119343
 Trial: 19 Epoch: 0, train_loss: 134591.94140625, validation_loss = 25804.849609375
========Trial 20 params: {'n_layers': 2, 'K': 3, 'Dropout': 0.4, 'Hidden0': 32, 'Hidden1': 128, 'optimizer': 'Adam', 'lr': 0.021969196574546282, 'batch': 32}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |    2304    |
|    sptH.0.gcn.bias     |     32     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.sAtt.Vs     |    729     |
|     sptH.2.sAtt.bs     |    729     |
|     sptH.2.sAtt.W1     |     32     |
|     sptH.2.sAtt.W2     |     32     |
|     sptH.2.sAtt.W3     |     1      |
|   sptH.2.gcn.weight    |   12288    |
|    sptH.2.gcn.bias     |    128     |
| sptH.2.timeConv.weight |     3      |
|  sptH.2.timeConv.bias  |     1      |
|     sptH.4.weight      |    1536    |
|      sptH.4.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    1152    |
|    sptD.0.gcn.bias     |     32     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.sAtt.Vs     |    729     |
|     sptD.2.sAtt.bs     |    729     |
|     sptD.2.sAtt.W1     |     32     |
|     sptD.2.sAtt.W2     |     32     |
|     sptD.2.sAtt.W3     |     1      |
|   sptD.2.gcn.weight    |   12288    |
|    sptD.2.gcn.bias     |    128     |
| sptD.2.timeConv.weight |     3      |
|  sptD.2.timeConv.bias  |     1      |
|     sptD.4.weight      |    1536    |
|      sptD.4.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |    2304    |
|    sptW.0.gcn.bias     |     32     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.sAtt.Vs     |    729     |
|     sptW.2.sAtt.bs     |    729     |
|     sptW.2.sAtt.W1     |     32     |
|     sptW.2.sAtt.W2     |     32     |
|     sptW.2.sAtt.W3     |     1      |
|   sptW.2.gcn.weight    |   12288    |
|    sptW.2.gcn.bias     |    128     |
| sptW.2.timeConv.weight |     3      |
|  sptW.2.timeConv.bias  |     1      |
|     sptW.4.weight      |    1536    |
|      sptW.4.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 56874
 Trial: 20 Epoch: 0, train_loss: 355909.6015625, validation_loss = 22514.873046875
 Trial: 20 Epoch: 1, train_loss: 85959.31640625, validation_loss = 43568.10546875
========Trial 21 params: {'n_layers': 2, 'K': 3, 'Dropout': 0.1, 'Hidden0': 32, 'Hidden1': 16, 'optimizer': 'Adam', 'lr': 0.0027244213853202413, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |    2304    |
|    sptH.0.gcn.bias     |     32     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.sAtt.Vs     |    729     |
|     sptH.2.sAtt.bs     |    729     |
|     sptH.2.sAtt.W1     |     32     |
|     sptH.2.sAtt.W2     |     32     |
|     sptH.2.sAtt.W3     |     1      |
|   sptH.2.gcn.weight    |    1536    |
|    sptH.2.gcn.bias     |     16     |
| sptH.2.timeConv.weight |     3      |
|  sptH.2.timeConv.bias  |     1      |
|     sptH.4.weight      |    192     |
|      sptH.4.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    1152    |
|    sptD.0.gcn.bias     |     32     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.sAtt.Vs     |    729     |
|     sptD.2.sAtt.bs     |    729     |
|     sptD.2.sAtt.W1     |     32     |
|     sptD.2.sAtt.W2     |     32     |
|     sptD.2.sAtt.W3     |     1      |
|   sptD.2.gcn.weight    |    1536    |
|    sptD.2.gcn.bias     |     16     |
| sptD.2.timeConv.weight |     3      |
|  sptD.2.timeConv.bias  |     1      |
|     sptD.4.weight      |    192     |
|      sptD.4.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |    2304    |
|    sptW.0.gcn.bias     |     32     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.sAtt.Vs     |    729     |
|     sptW.2.sAtt.bs     |    729     |
|     sptW.2.sAtt.W1     |     32     |
|     sptW.2.sAtt.W2     |     32     |
|     sptW.2.sAtt.W3     |     1      |
|   sptW.2.gcn.weight    |    1536    |
|    sptW.2.gcn.bias     |     16     |
| sptW.2.timeConv.weight |     3      |
|  sptW.2.timeConv.bias  |     1      |
|     sptW.4.weight      |    192     |
|      sptW.4.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 20250
 Trial: 21 Epoch: 0, train_loss: 268547.072265625, validation_loss = 22303.62841796875
 Trial: 21 Epoch: 1, train_loss: 145005.98876953125, validation_loss = 12607.5537109375
 Trial: 21 Epoch: 2, train_loss: 99351.37158203125, validation_loss = 8953.986328125
 Trial: 21 Epoch: 3, train_loss: 75935.66284179688, validation_loss = 8963.900634765625
 Trial: 21 Epoch: 4, train_loss: 57380.49951171875, validation_loss = 6677.60986328125
 Trial: 21 Epoch: 5, train_loss: 51242.119873046875, validation_loss = 8355.213623046875
 Trial: 21 Epoch: 6, train_loss: 49609.04150390625, validation_loss = 6936.943603515625
 Trial: 21 Epoch: 7, train_loss: 47390.1533203125, validation_loss = 6183.476806640625
 Trial: 21 Epoch: 8, train_loss: 43571.5986328125, validation_loss = 6137.677734375
 Trial: 21 Epoch: 9, train_loss: 42239.942626953125, validation_loss = 5876.8472900390625
 Trial: 21 Epoch: 10, train_loss: 38990.03955078125, validation_loss = 5756.105712890625
 Trial: 21 Epoch: 11, train_loss: 42725.922607421875, validation_loss = 5334.1739501953125
 Trial: 21 Epoch: 12, train_loss: 38950.025634765625, validation_loss = 6215.077880859375
 Trial: 21 Epoch: 13, train_loss: 39053.531982421875, validation_loss = 6880.99853515625
 Trial: 21 Epoch: 14, train_loss: 39168.099853515625, validation_loss = 5312.99462890625
 Trial: 21 Epoch: 15, train_loss: 36962.49365234375, validation_loss = 5086.740234375
 Trial: 21 Epoch: 16, train_loss: 34260.73352050781, validation_loss = 4751.3468017578125
 Trial: 21 Epoch: 17, train_loss: 36889.12939453125, validation_loss = 5243.03857421875
 Trial: 21 Epoch: 18, train_loss: 38328.35266113281, validation_loss = 6908.229736328125
 Trial: 21 Epoch: 19, train_loss: 36445.94689941406, validation_loss = 5523.8857421875
 Trial: 21 Epoch: 20, train_loss: 35365.70300292969, validation_loss = 4902.422119140625
 Trial: 21 Epoch: 21, train_loss: 35953.551025390625, validation_loss = 4434.0447998046875
 Trial: 21 Epoch: 22, train_loss: 37130.81201171875, validation_loss = 6874.1795654296875
 Trial: 21 Epoch: 23, train_loss: 36172.569091796875, validation_loss = 6065.2861328125
 Trial: 21 Epoch: 24, train_loss: 33784.07507324219, validation_loss = 5038.870849609375
 Trial: 21 Epoch: 25, train_loss: 33558.64392089844, validation_loss = 4835.176025390625
 Trial: 21 Epoch: 26, train_loss: 32854.19519042969, validation_loss = 4615.63623046875
 Trial: 21 Epoch: 27, train_loss: 31040.834594726562, validation_loss = 4659.47119140625
 Trial: 21 Epoch: 28, train_loss: 31633.029663085938, validation_loss = 5034.8763427734375
 Trial: 21 Epoch: 29, train_loss: 32028.13037109375, validation_loss = 4990.7413330078125
 Trial: 21 Epoch: 30, train_loss: 30918.942993164062, validation_loss = 5314.950439453125
 Trial: 21 Epoch: 31, train_loss: 34065.87438964844, validation_loss = 4836.8387451171875
 Trial: 21 Epoch: 32, train_loss: 31482.392333984375, validation_loss = 4814.5712890625
 Trial: 21 Epoch: 33, train_loss: 31044.188842773438, validation_loss = 4832.2750244140625
 Trial: 21 Epoch: 34, train_loss: 31404.696533203125, validation_loss = 4978.885986328125
========Trial 22 params: {'n_layers': 2, 'K': 3, 'Dropout': 0.1, 'Hidden0': 32, 'Hidden1': 64, 'optimizer': 'Adam', 'lr': 0.010480949896344634, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |    2304    |
|    sptH.0.gcn.bias     |     32     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.sAtt.Vs     |    729     |
|     sptH.2.sAtt.bs     |    729     |
|     sptH.2.sAtt.W1     |     32     |
|     sptH.2.sAtt.W2     |     32     |
|     sptH.2.sAtt.W3     |     1      |
|   sptH.2.gcn.weight    |    6144    |
|    sptH.2.gcn.bias     |     64     |
| sptH.2.timeConv.weight |     3      |
|  sptH.2.timeConv.bias  |     1      |
|     sptH.4.weight      |    768     |
|      sptH.4.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    1152    |
|    sptD.0.gcn.bias     |     32     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.sAtt.Vs     |    729     |
|     sptD.2.sAtt.bs     |    729     |
|     sptD.2.sAtt.W1     |     32     |
|     sptD.2.sAtt.W2     |     32     |
|     sptD.2.sAtt.W3     |     1      |
|   sptD.2.gcn.weight    |    6144    |
|    sptD.2.gcn.bias     |     64     |
| sptD.2.timeConv.weight |     3      |
|  sptD.2.timeConv.bias  |     1      |
|     sptD.4.weight      |    768     |
|      sptD.4.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |    2304    |
|    sptW.0.gcn.bias     |     32     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.sAtt.Vs     |    729     |
|     sptW.2.sAtt.bs     |    729     |
|     sptW.2.sAtt.W1     |     32     |
|     sptW.2.sAtt.W2     |     32     |
|     sptW.2.sAtt.W3     |     1      |
|   sptW.2.gcn.weight    |    6144    |
|    sptW.2.gcn.bias     |     64     |
| sptW.2.timeConv.weight |     3      |
|  sptW.2.timeConv.bias  |     1      |
|     sptW.4.weight      |    768     |
|      sptW.4.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 35946
 Trial: 22 Epoch: 0, train_loss: 131726.41040039062, validation_loss = 5552.394775390625
 Trial: 22 Epoch: 1, train_loss: 48384.4306640625, validation_loss = 7558.16650390625
 Trial: 22 Epoch: 2, train_loss: 39392.041748046875, validation_loss = 4489.0687255859375
 Trial: 22 Epoch: 3, train_loss: 33477.92419433594, validation_loss = 6673.813720703125
 Trial: 22 Epoch: 4, train_loss: 31149.22607421875, validation_loss = 4805.1256103515625
 Trial: 22 Epoch: 5, train_loss: 28916.234252929688, validation_loss = 3959.3941650390625
 Trial: 22 Epoch: 6, train_loss: 30372.776123046875, validation_loss = 4278.1739501953125
 Trial: 22 Epoch: 7, train_loss: 26388.970825195312, validation_loss = 4238.11474609375
 Trial: 22 Epoch: 8, train_loss: 28705.185668945312, validation_loss = 4538.36572265625
 Trial: 22 Epoch: 9, train_loss: 27836.761962890625, validation_loss = 4240.1678466796875
 Trial: 22 Epoch: 10, train_loss: 28353.555786132812, validation_loss = 9237.415283203125
 Trial: 22 Epoch: 11, train_loss: 32272.915893554688, validation_loss = 3764.4833984375
 Trial: 22 Epoch: 12, train_loss: 25690.507751464844, validation_loss = 3745.24267578125
 Trial: 22 Epoch: 13, train_loss: 23620.63525390625, validation_loss = 4227.5855712890625
 Trial: 22 Epoch: 14, train_loss: 30584.05224609375, validation_loss = 4525.469482421875
 Trial: 22 Epoch: 15, train_loss: 25426.767456054688, validation_loss = 4354.703125
 Trial: 22 Epoch: 16, train_loss: 32444.377685546875, validation_loss = 3766.6246337890625
 Trial: 22 Epoch: 17, train_loss: 28557.183471679688, validation_loss = 4697.881591796875
 Trial: 22 Epoch: 18, train_loss: 25205.078857421875, validation_loss = 3518.666015625
 Trial: 22 Epoch: 19, train_loss: 27372.9482421875, validation_loss = 5085.68603515625
 Trial: 22 Epoch: 20, train_loss: 26314.006591796875, validation_loss = 6553.38671875
 Trial: 22 Epoch: 21, train_loss: 35780.1865234375, validation_loss = 6048.669921875
 Trial: 22 Epoch: 22, train_loss: 32055.607055664062, validation_loss = 4140.04345703125
 Trial: 22 Epoch: 23, train_loss: 25685.78173828125, validation_loss = 3762.903564453125
 Trial: 22 Epoch: 24, train_loss: 25369.083129882812, validation_loss = 4353.8804931640625
 Trial: 22 Epoch: 25, train_loss: 24890.882873535156, validation_loss = 3418.6251220703125
 Trial: 22 Epoch: 26, train_loss: 22557.490844726562, validation_loss = 4145.6201171875
 Trial: 22 Epoch: 27, train_loss: 20455.73828125, validation_loss = 3382.412841796875
 Trial: 22 Epoch: 28, train_loss: 22051.024780273438, validation_loss = 3527.06298828125
 Trial: 22 Epoch: 29, train_loss: 23969.715698242188, validation_loss = 4951.1112060546875
 Trial: 22 Epoch: 30, train_loss: 22058.881103515625, validation_loss = 3364.6190185546875
 Trial: 22 Epoch: 31, train_loss: 22788.761962890625, validation_loss = 3503.301025390625
 Trial: 22 Epoch: 32, train_loss: 23402.055786132812, validation_loss = 4785.3529052734375
 Trial: 22 Epoch: 33, train_loss: 23796.99737548828, validation_loss = 3418.2359619140625
 Trial: 22 Epoch: 34, train_loss: 24364.086303710938, validation_loss = 3386.643310546875
 Trial: 22 Epoch: 35, train_loss: 25683.589965820312, validation_loss = 3636.40625
 Trial: 22 Epoch: 36, train_loss: 24950.21466064453, validation_loss = 4135.56787109375
 Trial: 22 Epoch: 37, train_loss: 21632.156860351562, validation_loss = 3302.44873046875
 Trial: 22 Epoch: 38, train_loss: 20835.09600830078, validation_loss = 3192.077392578125
 Trial: 22 Epoch: 39, train_loss: 22830.157836914062, validation_loss = 3528.1243896484375
 Trial: 22 Epoch: 40, train_loss: 24560.286193847656, validation_loss = 4340.989990234375
 Trial: 22 Epoch: 41, train_loss: 25654.39990234375, validation_loss = 4831.55859375
 Trial: 22 Epoch: 42, train_loss: 21537.524047851562, validation_loss = 3455.6563720703125
 Trial: 22 Epoch: 43, train_loss: 20995.976440429688, validation_loss = 4228.8067626953125
 Trial: 22 Epoch: 44, train_loss: 24974.245361328125, validation_loss = 3503.3958740234375
 Trial: 22 Epoch: 45, train_loss: 21594.98162841797, validation_loss = 3449.049560546875
 Trial: 22 Epoch: 46, train_loss: 19933.418090820312, validation_loss = 2943.5186767578125
 Trial: 22 Epoch: 47, train_loss: 23921.2548828125, validation_loss = 3822.732666015625
 Trial: 22 Epoch: 48, train_loss: 22039.438232421875, validation_loss = 3280.526123046875
 Trial: 22 Epoch: 49, train_loss: 19297.15301513672, validation_loss = 3327.4256591796875
 Trial: 22 Epoch: 50, train_loss: 19165.240112304688, validation_loss = 3157.716552734375
 Trial: 22 Epoch: 51, train_loss: 18425.935424804688, validation_loss = 3172.37255859375
 Trial: 22 Epoch: 52, train_loss: 19632.742553710938, validation_loss = 3519.555908203125
 Trial: 22 Epoch: 53, train_loss: 19349.858032226562, validation_loss = 3015.2947998046875
 Trial: 22 Epoch: 54, train_loss: 20066.220764160156, validation_loss = 3213.8092041015625
 Trial: 22 Epoch: 55, train_loss: 19135.216186523438, validation_loss = 3613.0245361328125
 Trial: 22 Epoch: 56, train_loss: 19329.95037841797, validation_loss = 3483.2237548828125
 Trial: 22 Epoch: 57, train_loss: 19324.760803222656, validation_loss = 3223.5264892578125
 Trial: 22 Epoch: 58, train_loss: 20890.532592773438, validation_loss = 3412.1578369140625
 Trial: 22 Epoch: 59, train_loss: 22411.99542236328, validation_loss = 3957.56982421875
 Trial: 22 Epoch: 60, train_loss: 20578.313598632812, validation_loss = 3365.4722900390625
 Trial: 22 Epoch: 61, train_loss: 18600.13299560547, validation_loss = 3262.2659912109375
 Trial: 22 Epoch: 62, train_loss: 18358.713989257812, validation_loss = 3191.904296875
 Trial: 22 Epoch: 63, train_loss: 18237.433532714844, validation_loss = 3150.48779296875
 Trial: 22 Epoch: 64, train_loss: 18275.620727539062, validation_loss = 3175.9227294921875
 Trial: 22 Epoch: 65, train_loss: 18759.737060546875, validation_loss = 3383.486328125
 Trial: 22 Epoch: 66, train_loss: 17638.934448242188, validation_loss = 3075.7855224609375
 Trial: 22 Epoch: 67, train_loss: 18099.896423339844, validation_loss = 3726.8592529296875
 Trial: 22 Epoch: 68, train_loss: 19130.54541015625, validation_loss = 3622.91552734375
 Trial: 22 Epoch: 69, train_loss: 19379.961364746094, validation_loss = 4147.400634765625
 Trial: 22 Epoch: 70, train_loss: 22441.627563476562, validation_loss = 2868.0142822265625
 Trial: 22 Epoch: 71, train_loss: 21608.957641601562, validation_loss = 3882.894775390625
 Trial: 22 Epoch: 72, train_loss: 18716.912170410156, validation_loss = 4070.09130859375
 Trial: 22 Epoch: 73, train_loss: 21592.938903808594, validation_loss = 4044.1446533203125
 Trial: 22 Epoch: 74, train_loss: 20099.855407714844, validation_loss = 3166.076416015625
 Trial: 22 Epoch: 75, train_loss: 19065.95947265625, validation_loss = 3345.2845458984375
 Trial: 22 Epoch: 76, train_loss: 19800.908935546875, validation_loss = 3834.359375
 Trial: 22 Epoch: 77, train_loss: 18764.29345703125, validation_loss = 2930.7587890625
 Trial: 22 Epoch: 78, train_loss: 19064.603942871094, validation_loss = 3106.2724609375
 Trial: 22 Epoch: 79, train_loss: 18795.552124023438, validation_loss = 3069.4345703125
 Trial: 22 Epoch: 80, train_loss: 18270.55712890625, validation_loss = 3362.273193359375
 Trial: 22 Epoch: 81, train_loss: 18629.754028320312, validation_loss = 3579.8404541015625
 Trial: 22 Epoch: 82, train_loss: 19230.805297851562, validation_loss = 3446.1199951171875
 Trial: 22 Epoch: 83, train_loss: 18188.892822265625, validation_loss = 3121.650146484375
 Trial: 22 Epoch: 84, train_loss: 19215.026123046875, validation_loss = 4044.1978759765625
 Trial: 22 Epoch: 85, train_loss: 20371.180419921875, validation_loss = 3295.296142578125
 Trial: 22 Epoch: 86, train_loss: 20759.600830078125, validation_loss = 2952.10107421875
 Trial: 22 Epoch: 87, train_loss: 18085.539672851562, validation_loss = 3004.4814453125
 Trial: 22 Epoch: 88, train_loss: 17053.488708496094, validation_loss = 3238.0906982421875
 Trial: 22 Epoch: 89, train_loss: 18285.160217285156, validation_loss = 3551.6492919921875
 Trial: 22 Epoch: 90, train_loss: 18029.256469726562, validation_loss = 3031.664306640625
 Trial: 22 Epoch: 91, train_loss: 17489.417602539062, validation_loss = 2954.27490234375
 Trial: 22 Epoch: 92, train_loss: 17367.452697753906, validation_loss = 3501.55712890625
 Trial: 22 Epoch: 93, train_loss: 18886.80108642578, validation_loss = 3665.05712890625
 Trial: 22 Epoch: 94, train_loss: 17589.534729003906, validation_loss = 3088.30615234375
 Trial: 22 Epoch: 95, train_loss: 17089.593872070312, validation_loss = 3077.9456787109375
 Trial: 22 Epoch: 96, train_loss: 17112.89276123047, validation_loss = 3015.9942626953125
 Trial: 22 Epoch: 97, train_loss: 17734.2822265625, validation_loss = 3166.841064453125
 Trial: 22 Epoch: 98, train_loss: 18394.20526123047, validation_loss = 3238.9254150390625
 Trial: 22 Epoch: 99, train_loss: 18357.078979492188, validation_loss = 3534.897705078125
========Trial 23 params: {'n_layers': 2, 'K': 3, 'Dropout': 0.2, 'Hidden0': 32, 'Hidden1': 64, 'optimizer': 'Adam', 'lr': 0.010048064721992189, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |    2304    |
|    sptH.0.gcn.bias     |     32     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.sAtt.Vs     |    729     |
|     sptH.2.sAtt.bs     |    729     |
|     sptH.2.sAtt.W1     |     32     |
|     sptH.2.sAtt.W2     |     32     |
|     sptH.2.sAtt.W3     |     1      |
|   sptH.2.gcn.weight    |    6144    |
|    sptH.2.gcn.bias     |     64     |
| sptH.2.timeConv.weight |     3      |
|  sptH.2.timeConv.bias  |     1      |
|     sptH.4.weight      |    768     |
|      sptH.4.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    1152    |
|    sptD.0.gcn.bias     |     32     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.sAtt.Vs     |    729     |
|     sptD.2.sAtt.bs     |    729     |
|     sptD.2.sAtt.W1     |     32     |
|     sptD.2.sAtt.W2     |     32     |
|     sptD.2.sAtt.W3     |     1      |
|   sptD.2.gcn.weight    |    6144    |
|    sptD.2.gcn.bias     |     64     |
| sptD.2.timeConv.weight |     3      |
|  sptD.2.timeConv.bias  |     1      |
|     sptD.4.weight      |    768     |
|      sptD.4.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |    2304    |
|    sptW.0.gcn.bias     |     32     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.sAtt.Vs     |    729     |
|     sptW.2.sAtt.bs     |    729     |
|     sptW.2.sAtt.W1     |     32     |
|     sptW.2.sAtt.W2     |     32     |
|     sptW.2.sAtt.W3     |     1      |
|   sptW.2.gcn.weight    |    6144    |
|    sptW.2.gcn.bias     |     64     |
| sptW.2.timeConv.weight |     3      |
|  sptW.2.timeConv.bias  |     1      |
|     sptW.4.weight      |    768     |
|      sptW.4.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 35946
 Trial: 23 Epoch: 0, train_loss: 137909.49731445312, validation_loss = 6072.459228515625
 Trial: 23 Epoch: 1, train_loss: 53649.319091796875, validation_loss = 7764.73974609375
 Trial: 23 Epoch: 2, train_loss: 41594.758056640625, validation_loss = 5256.54345703125
 Trial: 23 Epoch: 3, train_loss: 37593.99609375, validation_loss = 6649.246826171875
 Trial: 23 Epoch: 4, train_loss: 32303.598266601562, validation_loss = 5681.809326171875
 Trial: 23 Epoch: 5, train_loss: 32666.3359375, validation_loss = 4367.84765625
 Trial: 23 Epoch: 6, train_loss: 31379.36572265625, validation_loss = 4265.90234375
 Trial: 23 Epoch: 7, train_loss: 28750.192138671875, validation_loss = 4883.089111328125
 Trial: 23 Epoch: 8, train_loss: 30336.535766601562, validation_loss = 6145.148193359375
 Trial: 23 Epoch: 9, train_loss: 32932.91809082031, validation_loss = 4859.9814453125
 Trial: 23 Epoch: 10, train_loss: 34319.74035644531, validation_loss = 9416.706298828125
 Trial: 23 Epoch: 11, train_loss: 32027.073486328125, validation_loss = 4491.5733642578125
 Trial: 23 Epoch: 12, train_loss: 28218.408203125, validation_loss = 4153.31689453125
 Trial: 23 Epoch: 13, train_loss: 28593.863403320312, validation_loss = 4291.7509765625
 Trial: 23 Epoch: 14, train_loss: 32675.072631835938, validation_loss = 5116.824462890625
 Trial: 23 Epoch: 15, train_loss: 28383.092407226562, validation_loss = 4471.244140625
 Trial: 23 Epoch: 16, train_loss: 27925.731811523438, validation_loss = 4022.9093017578125
 Trial: 23 Epoch: 17, train_loss: 29166.754150390625, validation_loss = 5719.07470703125
 Trial: 23 Epoch: 18, train_loss: 30976.201904296875, validation_loss = 4251.9246826171875
 Trial: 23 Epoch: 19, train_loss: 31629.254638671875, validation_loss = 4579.269775390625
 Trial: 23 Epoch: 20, train_loss: 32815.93151855469, validation_loss = 11980.560302734375
 Trial: 23 Epoch: 21, train_loss: 50477.3193359375, validation_loss = 4041.6048583984375
 Trial: 23 Epoch: 22, train_loss: 33121.40661621094, validation_loss = 4847.735595703125
 Trial: 23 Epoch: 23, train_loss: 28222.271606445312, validation_loss = 4086.7479248046875
 Trial: 23 Epoch: 24, train_loss: 28044.7294921875, validation_loss = 5773.1761474609375
 Trial: 23 Epoch: 25, train_loss: 29532.379272460938, validation_loss = 3974.641845703125
 Trial: 23 Epoch: 26, train_loss: 25765.694091796875, validation_loss = 4390.8936767578125
 Trial: 23 Epoch: 27, train_loss: 25046.15380859375, validation_loss = 4264.1475830078125
 Trial: 23 Epoch: 28, train_loss: 25586.077209472656, validation_loss = 4190.7642822265625
 Trial: 23 Epoch: 29, train_loss: 29990.975952148438, validation_loss = 5681.861083984375
 Trial: 23 Epoch: 30, train_loss: 25308.887084960938, validation_loss = 3657.6986083984375
 Trial: 23 Epoch: 31, train_loss: 25434.037963867188, validation_loss = 4203.80810546875
 Trial: 23 Epoch: 32, train_loss: 29654.909545898438, validation_loss = 3750.31689453125
 Trial: 23 Epoch: 33, train_loss: 26560.412841796875, validation_loss = 4391.689453125
 Trial: 23 Epoch: 34, train_loss: 23791.521850585938, validation_loss = 3707.669677734375
 Trial: 23 Epoch: 35, train_loss: 23831.348266601562, validation_loss = 3770.3095703125
 Trial: 23 Epoch: 36, train_loss: 24663.970275878906, validation_loss = 3690.83447265625
 Trial: 23 Epoch: 37, train_loss: 23235.425048828125, validation_loss = 3178.6158447265625
 Trial: 23 Epoch: 38, train_loss: 23906.614013671875, validation_loss = 4350.1207275390625
 Trial: 23 Epoch: 39, train_loss: 25364.59619140625, validation_loss = 3659.1611328125
 Trial: 23 Epoch: 40, train_loss: 25659.702758789062, validation_loss = 3884.1793212890625
 Trial: 23 Epoch: 41, train_loss: 24997.974609375, validation_loss = 4096.9775390625
 Trial: 23 Epoch: 42, train_loss: 21863.205200195312, validation_loss = 4034.314208984375
 Trial: 23 Epoch: 43, train_loss: 24349.461303710938, validation_loss = 4462.00732421875
 Trial: 23 Epoch: 44, train_loss: 30538.390014648438, validation_loss = 3486.103759765625
 Trial: 23 Epoch: 45, train_loss: 24611.63916015625, validation_loss = 4405.6761474609375
 Trial: 23 Epoch: 46, train_loss: 22761.089965820312, validation_loss = 3500.7972412109375
 Trial: 23 Epoch: 47, train_loss: 29147.108642578125, validation_loss = 3533.7298583984375
 Trial: 23 Epoch: 48, train_loss: 22987.059997558594, validation_loss = 3310.7874755859375
 Trial: 23 Epoch: 49, train_loss: 21391.45721435547, validation_loss = 3754.45166015625
 Trial: 23 Epoch: 50, train_loss: 21025.602783203125, validation_loss = 3706.7955322265625
 Trial: 23 Epoch: 51, train_loss: 21485.692138671875, validation_loss = 3221.45068359375
 Trial: 23 Epoch: 52, train_loss: 21035.950073242188, validation_loss = 3605.857666015625
 Trial: 23 Epoch: 53, train_loss: 20971.816345214844, validation_loss = 3025.5479736328125
 Trial: 23 Epoch: 54, train_loss: 21926.02655029297, validation_loss = 3458.1422119140625
 Trial: 23 Epoch: 55, train_loss: 19857.026916503906, validation_loss = 3821.2626953125
 Trial: 23 Epoch: 56, train_loss: 20110.556701660156, validation_loss = 3342.7437744140625
 Trial: 23 Epoch: 57, train_loss: 22228.345458984375, validation_loss = 3763.406005859375
 Trial: 23 Epoch: 58, train_loss: 22353.213134765625, validation_loss = 3273.905517578125
 Trial: 23 Epoch: 59, train_loss: 22526.151000976562, validation_loss = 3962.7882080078125
 Trial: 23 Epoch: 60, train_loss: 22601.77294921875, validation_loss = 3299.1712646484375
 Trial: 23 Epoch: 61, train_loss: 20767.362670898438, validation_loss = 3252.896240234375
 Trial: 23 Epoch: 62, train_loss: 21572.299072265625, validation_loss = 3328.5933837890625
 Trial: 23 Epoch: 63, train_loss: 20713.052673339844, validation_loss = 3246.0714111328125
 Trial: 23 Epoch: 64, train_loss: 20370.292541503906, validation_loss = 3627.5723876953125
 Trial: 23 Epoch: 65, train_loss: 21551.721435546875, validation_loss = 3411.505859375
 Trial: 23 Epoch: 66, train_loss: 20051.351684570312, validation_loss = 3235.978515625
 Trial: 23 Epoch: 67, train_loss: 20090.837524414062, validation_loss = 4733.4869384765625
 Trial: 23 Epoch: 68, train_loss: 22891.218627929688, validation_loss = 3674.782470703125
 Trial: 23 Epoch: 69, train_loss: 22242.793701171875, validation_loss = 3801.438232421875
 Trial: 23 Epoch: 70, train_loss: 21664.77880859375, validation_loss = 3579.1370849609375
 Trial: 23 Epoch: 71, train_loss: 20358.2109375, validation_loss = 3323.5706787109375
 Trial: 23 Epoch: 72, train_loss: 20112.02569580078, validation_loss = 3156.197021484375
 Trial: 23 Epoch: 73, train_loss: 20365.276428222656, validation_loss = 3392.5401611328125
 Trial: 23 Epoch: 74, train_loss: 19751.913146972656, validation_loss = 4007.306396484375
 Trial: 23 Epoch: 75, train_loss: 20129.740783691406, validation_loss = 3359.4185791015625
 Trial: 23 Epoch: 76, train_loss: 21511.903686523438, validation_loss = 3706.9815673828125
 Trial: 23 Epoch: 77, train_loss: 18866.991088867188, validation_loss = 3987.6572265625
 Trial: 23 Epoch: 78, train_loss: 19969.076782226562, validation_loss = 3412.280517578125
 Trial: 23 Epoch: 79, train_loss: 21184.127685546875, validation_loss = 3525.5185546875
 Trial: 23 Epoch: 80, train_loss: 20463.330932617188, validation_loss = 3345.5562744140625
 Trial: 23 Epoch: 81, train_loss: 19719.387573242188, validation_loss = 3351.5360107421875
 Trial: 23 Epoch: 82, train_loss: 24211.30224609375, validation_loss = 3138.9058837890625
 Trial: 23 Epoch: 83, train_loss: 22262.27362060547, validation_loss = 3056.290771484375
 Trial: 23 Epoch: 84, train_loss: 22872.600463867188, validation_loss = 3995.2579345703125
 Trial: 23 Epoch: 85, train_loss: 21564.53253173828, validation_loss = 3279.4613037109375
 Trial: 23 Epoch: 86, train_loss: 22438.210205078125, validation_loss = 3198.3917236328125
 Trial: 23 Epoch: 87, train_loss: 21572.215942382812, validation_loss = 3740.960693359375
 Trial: 23 Epoch: 88, train_loss: 20393.33917236328, validation_loss = 3295.1253662109375
 Trial: 23 Epoch: 89, train_loss: 22135.608032226562, validation_loss = 3268.3482666015625
 Trial: 23 Epoch: 90, train_loss: 19800.832641601562, validation_loss = 3558.937255859375
 Trial: 23 Epoch: 91, train_loss: 19747.611572265625, validation_loss = 3199.7442626953125
 Trial: 23 Epoch: 92, train_loss: 19485.5126953125, validation_loss = 3446.63818359375
 Trial: 23 Epoch: 93, train_loss: 20815.80096435547, validation_loss = 3525.8482666015625
 Trial: 23 Epoch: 94, train_loss: 19250.033569335938, validation_loss = 3225.5029296875
 Trial: 23 Epoch: 95, train_loss: 18486.791442871094, validation_loss = 3821.7513427734375
 Trial: 23 Epoch: 96, train_loss: 20889.098999023438, validation_loss = 3710.390869140625
 Trial: 23 Epoch: 97, train_loss: 21057.061157226562, validation_loss = 3407.8050537109375
 Trial: 23 Epoch: 98, train_loss: 20016.206787109375, validation_loss = 3533.7657470703125
 Trial: 23 Epoch: 99, train_loss: 19930.87451171875, validation_loss = 3232.0517578125
========Trial 24 params: {'n_layers': 2, 'K': 2, 'Dropout': 0.2, 'Hidden0': 32, 'Hidden1': 64, 'optimizer': 'Adam', 'lr': 0.010501244232612846, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |    1536    |
|    sptH.0.gcn.bias     |     32     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.sAtt.Vs     |    729     |
|     sptH.2.sAtt.bs     |    729     |
|     sptH.2.sAtt.W1     |     32     |
|     sptH.2.sAtt.W2     |     32     |
|     sptH.2.sAtt.W3     |     1      |
|   sptH.2.gcn.weight    |    4096    |
|    sptH.2.gcn.bias     |     64     |
| sptH.2.timeConv.weight |     3      |
|  sptH.2.timeConv.bias  |     1      |
|     sptH.4.weight      |    768     |
|      sptH.4.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    768     |
|    sptD.0.gcn.bias     |     32     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.sAtt.Vs     |    729     |
|     sptD.2.sAtt.bs     |    729     |
|     sptD.2.sAtt.W1     |     32     |
|     sptD.2.sAtt.W2     |     32     |
|     sptD.2.sAtt.W3     |     1      |
|   sptD.2.gcn.weight    |    4096    |
|    sptD.2.gcn.bias     |     64     |
| sptD.2.timeConv.weight |     3      |
|  sptD.2.timeConv.bias  |     1      |
|     sptD.4.weight      |    768     |
|      sptD.4.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |    1536    |
|    sptW.0.gcn.bias     |     32     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.sAtt.Vs     |    729     |
|     sptW.2.sAtt.bs     |    729     |
|     sptW.2.sAtt.W1     |     32     |
|     sptW.2.sAtt.W2     |     32     |
|     sptW.2.sAtt.W3     |     1      |
|   sptW.2.gcn.weight    |    4096    |
|    sptW.2.gcn.bias     |     64     |
| sptW.2.timeConv.weight |     3      |
|  sptW.2.timeConv.bias  |     1      |
|     sptW.4.weight      |    768     |
|      sptW.4.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 27882
 Trial: 24 Epoch: 0, train_loss: 227247.654296875, validation_loss = 11643.46728515625
 Trial: 24 Epoch: 1, train_loss: 71948.05639648438, validation_loss = 7036.121826171875
 Trial: 24 Epoch: 2, train_loss: 51152.8798828125, validation_loss = 6883.9453125
 Trial: 24 Epoch: 3, train_loss: 48702.39501953125, validation_loss = 5237.61474609375
 Trial: 24 Epoch: 4, train_loss: 42008.546142578125, validation_loss = 4802.19970703125
 Trial: 24 Epoch: 5, train_loss: 39840.02575683594, validation_loss = 4904.19482421875
 Trial: 24 Epoch: 6, train_loss: 35941.576904296875, validation_loss = 6812.628662109375
 Trial: 24 Epoch: 7, train_loss: 34836.02294921875, validation_loss = 5862.3406982421875
 Trial: 24 Epoch: 8, train_loss: 33880.48498535156, validation_loss = 4389.7900390625
 Trial: 24 Epoch: 9, train_loss: 35119.15380859375, validation_loss = 4567.974609375
 Trial: 24 Epoch: 10, train_loss: 34947.18957519531, validation_loss = 5030.3253173828125
 Trial: 24 Epoch: 11, train_loss: 33991.925048828125, validation_loss = 6404.612060546875
 Trial: 24 Epoch: 12, train_loss: 34490.96337890625, validation_loss = 4400.203857421875
 Trial: 24 Epoch: 13, train_loss: 29063.13134765625, validation_loss = 3838.21484375
 Trial: 24 Epoch: 14, train_loss: 28568.628784179688, validation_loss = 4203.5308837890625
 Trial: 24 Epoch: 15, train_loss: 28660.21923828125, validation_loss = 4205.5299072265625
 Trial: 24 Epoch: 16, train_loss: 27497.914916992188, validation_loss = 3536.4261474609375
 Trial: 24 Epoch: 17, train_loss: 28829.804748535156, validation_loss = 3760.465576171875
 Trial: 24 Epoch: 18, train_loss: 26944.57861328125, validation_loss = 3978.902587890625
 Trial: 24 Epoch: 19, train_loss: 29419.6650390625, validation_loss = 3681.1236572265625
 Trial: 24 Epoch: 20, train_loss: 31608.436645507812, validation_loss = 4142.3465576171875
 Trial: 24 Epoch: 21, train_loss: 30554.70147705078, validation_loss = 4287.5164794921875
 Trial: 24 Epoch: 22, train_loss: 28171.4130859375, validation_loss = 4115.6180419921875
 Trial: 24 Epoch: 23, train_loss: 26729.109130859375, validation_loss = 3887.9442138671875
 Trial: 24 Epoch: 24, train_loss: 25666.438232421875, validation_loss = 4690.4854736328125
 Trial: 24 Epoch: 25, train_loss: 25120.851440429688, validation_loss = 3609.2689208984375
 Trial: 24 Epoch: 26, train_loss: 26635.333374023438, validation_loss = 3478.502685546875
 Trial: 24 Epoch: 27, train_loss: 29057.330932617188, validation_loss = 6051.326416015625
 Trial: 24 Epoch: 28, train_loss: 30587.6015625, validation_loss = 4312.142822265625
 Trial: 24 Epoch: 29, train_loss: 29999.494995117188, validation_loss = 5095.15673828125
 Trial: 24 Epoch: 30, train_loss: 29711.99072265625, validation_loss = 4868.5352783203125
 Trial: 24 Epoch: 31, train_loss: 26249.128662109375, validation_loss = 3898.0948486328125
 Trial: 24 Epoch: 32, train_loss: 25060.412475585938, validation_loss = 3440.436767578125
 Trial: 24 Epoch: 33, train_loss: 24101.620361328125, validation_loss = 3681.0098876953125
 Trial: 24 Epoch: 34, train_loss: 24348.67236328125, validation_loss = 4219.75390625
 Trial: 24 Epoch: 35, train_loss: 26587.276977539062, validation_loss = 3402.4747314453125
 Trial: 24 Epoch: 36, train_loss: 24007.6650390625, validation_loss = 3967.259033203125
 Trial: 24 Epoch: 37, train_loss: 23722.795166015625, validation_loss = 3507.6661376953125
 Trial: 24 Epoch: 38, train_loss: 23294.51611328125, validation_loss = 3493.267578125
 Trial: 24 Epoch: 39, train_loss: 23442.291015625, validation_loss = 3617.21142578125
 Trial: 24 Epoch: 40, train_loss: 24084.772827148438, validation_loss = 3340.48779296875
 Trial: 24 Epoch: 41, train_loss: 24926.452270507812, validation_loss = 4577.4229736328125
 Trial: 24 Epoch: 42, train_loss: 26138.169189453125, validation_loss = 3470.84423828125
 Trial: 24 Epoch: 43, train_loss: 28065.411865234375, validation_loss = 3646.7974853515625
 Trial: 24 Epoch: 44, train_loss: 22217.31463623047, validation_loss = 3202.9012451171875
 Trial: 24 Epoch: 45, train_loss: 24037.89483642578, validation_loss = 3635.6737060546875
 Trial: 24 Epoch: 46, train_loss: 21961.86181640625, validation_loss = 3250.33984375
 Trial: 24 Epoch: 47, train_loss: 21834.885192871094, validation_loss = 3486.150146484375
 Trial: 24 Epoch: 48, train_loss: 24031.094482421875, validation_loss = 2932.2037353515625
 Trial: 24 Epoch: 49, train_loss: 25357.665649414062, validation_loss = 3603.3765869140625
 Trial: 24 Epoch: 50, train_loss: 21309.431640625, validation_loss = 3196.8779296875
 Trial: 24 Epoch: 51, train_loss: 21399.24951171875, validation_loss = 3208.11962890625
 Trial: 24 Epoch: 52, train_loss: 22238.954711914062, validation_loss = 2938.0101318359375
 Trial: 24 Epoch: 53, train_loss: 22297.789794921875, validation_loss = 3255.900634765625
 Trial: 24 Epoch: 54, train_loss: 21636.317016601562, validation_loss = 3263.37109375
 Trial: 24 Epoch: 55, train_loss: 22788.05291748047, validation_loss = 3546.171630859375
 Trial: 24 Epoch: 56, train_loss: 21896.063720703125, validation_loss = 3313.033203125
 Trial: 24 Epoch: 57, train_loss: 21430.21270751953, validation_loss = 3060.1053466796875
 Trial: 24 Epoch: 58, train_loss: 20891.853881835938, validation_loss = 3825.8646240234375
 Trial: 24 Epoch: 59, train_loss: 21666.311462402344, validation_loss = 3091.4410400390625
 Trial: 24 Epoch: 60, train_loss: 21034.763671875, validation_loss = 3746.8792724609375
 Trial: 24 Epoch: 61, train_loss: 23271.66827392578, validation_loss = 4314.9677734375
 Trial: 24 Epoch: 62, train_loss: 23001.70654296875, validation_loss = 3608.877685546875
 Trial: 24 Epoch: 63, train_loss: 23756.561279296875, validation_loss = 3126.4385986328125
 Trial: 24 Epoch: 64, train_loss: 30890.7490234375, validation_loss = 4029.14990234375
 Trial: 24 Epoch: 65, train_loss: 24823.03302001953, validation_loss = 4591.00537109375
 Trial: 24 Epoch: 66, train_loss: 24215.922119140625, validation_loss = 3384.124755859375
 Trial: 24 Epoch: 67, train_loss: 23539.914672851562, validation_loss = 3615.7010498046875
 Trial: 24 Epoch: 68, train_loss: 20423.439331054688, validation_loss = 3205.2066650390625
 Trial: 24 Epoch: 69, train_loss: 20838.14520263672, validation_loss = 3332.926513671875
 Trial: 24 Epoch: 70, train_loss: 20229.366638183594, validation_loss = 3563.1162109375
 Trial: 24 Epoch: 71, train_loss: 21137.0361328125, validation_loss = 3613.045166015625
 Trial: 24 Epoch: 72, train_loss: 21298.1005859375, validation_loss = 3367.4818115234375
 Trial: 24 Epoch: 73, train_loss: 19887.969482421875, validation_loss = 3131.9395751953125
 Trial: 24 Epoch: 74, train_loss: 21511.141845703125, validation_loss = 3626.862060546875
 Trial: 24 Epoch: 75, train_loss: 21356.901306152344, validation_loss = 3986.639892578125
 Trial: 24 Epoch: 76, train_loss: 20814.146057128906, validation_loss = 3168.757080078125
 Trial: 24 Epoch: 77, train_loss: 20644.98651123047, validation_loss = 2778.74951171875
 Trial: 24 Epoch: 78, train_loss: 20945.528686523438, validation_loss = 3756.822265625
 Trial: 24 Epoch: 79, train_loss: 21530.857666015625, validation_loss = 3149.8416748046875
 Trial: 24 Epoch: 80, train_loss: 20759.224670410156, validation_loss = 3035.8543701171875
 Trial: 24 Epoch: 81, train_loss: 20169.744506835938, validation_loss = 3245.4757080078125
 Trial: 24 Epoch: 82, train_loss: 20920.726806640625, validation_loss = 3217.89599609375
 Trial: 24 Epoch: 83, train_loss: 23347.599487304688, validation_loss = 3878.5687255859375
 Trial: 24 Epoch: 84, train_loss: 24260.74169921875, validation_loss = 3127.4622802734375
 Trial: 24 Epoch: 85, train_loss: 20108.251342773438, validation_loss = 4090.708984375
 Trial: 24 Epoch: 86, train_loss: 21020.790771484375, validation_loss = 2997.6590576171875
 Trial: 24 Epoch: 87, train_loss: 20994.734924316406, validation_loss = 3502.0802001953125
 Trial: 24 Epoch: 88, train_loss: 21010.287841796875, validation_loss = 4505.0848388671875
 Trial: 24 Epoch: 89, train_loss: 22010.1005859375, validation_loss = 3286.2247314453125
 Trial: 24 Epoch: 90, train_loss: 21284.982666015625, validation_loss = 3826.77685546875
 Trial: 24 Epoch: 91, train_loss: 22516.308837890625, validation_loss = 3344.654296875
 Trial: 24 Epoch: 92, train_loss: 23214.8935546875, validation_loss = 3381.318359375
 Trial: 24 Epoch: 93, train_loss: 25035.847900390625, validation_loss = 3987.774169921875
 Trial: 24 Epoch: 94, train_loss: 23424.401794433594, validation_loss = 3901.31005859375
 Trial: 24 Epoch: 95, train_loss: 23147.899658203125, validation_loss = 3086.214111328125
 Trial: 24 Epoch: 96, train_loss: 19769.061950683594, validation_loss = 3329.76171875
 Trial: 24 Epoch: 97, train_loss: 19809.512817382812, validation_loss = 3156.08154296875
 Trial: 24 Epoch: 98, train_loss: 19834.27227783203, validation_loss = 2976.4229736328125
 Trial: 24 Epoch: 99, train_loss: 20540.256896972656, validation_loss = 3261.4388427734375
========Trial 25 params: {'n_layers': 2, 'K': 2, 'Dropout': 0.2, 'Hidden0': 32, 'Hidden1': 64, 'optimizer': 'Adam', 'lr': 0.007034107482635035, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |    1536    |
|    sptH.0.gcn.bias     |     32     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.sAtt.Vs     |    729     |
|     sptH.2.sAtt.bs     |    729     |
|     sptH.2.sAtt.W1     |     32     |
|     sptH.2.sAtt.W2     |     32     |
|     sptH.2.sAtt.W3     |     1      |
|   sptH.2.gcn.weight    |    4096    |
|    sptH.2.gcn.bias     |     64     |
| sptH.2.timeConv.weight |     3      |
|  sptH.2.timeConv.bias  |     1      |
|     sptH.4.weight      |    768     |
|      sptH.4.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    768     |
|    sptD.0.gcn.bias     |     32     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.sAtt.Vs     |    729     |
|     sptD.2.sAtt.bs     |    729     |
|     sptD.2.sAtt.W1     |     32     |
|     sptD.2.sAtt.W2     |     32     |
|     sptD.2.sAtt.W3     |     1      |
|   sptD.2.gcn.weight    |    4096    |
|    sptD.2.gcn.bias     |     64     |
| sptD.2.timeConv.weight |     3      |
|  sptD.2.timeConv.bias  |     1      |
|     sptD.4.weight      |    768     |
|      sptD.4.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |    1536    |
|    sptW.0.gcn.bias     |     32     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.sAtt.Vs     |    729     |
|     sptW.2.sAtt.bs     |    729     |
|     sptW.2.sAtt.W1     |     32     |
|     sptW.2.sAtt.W2     |     32     |
|     sptW.2.sAtt.W3     |     1      |
|   sptW.2.gcn.weight    |    4096    |
|    sptW.2.gcn.bias     |     64     |
| sptW.2.timeConv.weight |     3      |
|  sptW.2.timeConv.bias  |     1      |
|     sptW.4.weight      |    768     |
|      sptW.4.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 27882
 Trial: 25 Epoch: 0, train_loss: 243367.34814453125, validation_loss = 9557.885986328125
 Trial: 25 Epoch: 1, train_loss: 88196.20263671875, validation_loss = 7518.149169921875
 Trial: 25 Epoch: 2, train_loss: 53158.28955078125, validation_loss = 6349.8603515625
 Trial: 25 Epoch: 3, train_loss: 48210.394287109375, validation_loss = 9374.6689453125
 Trial: 25 Epoch: 4, train_loss: 39897.67626953125, validation_loss = 4958.2540283203125
 Trial: 25 Epoch: 5, train_loss: 37563.947509765625, validation_loss = 4929.3499755859375
 Trial: 25 Epoch: 6, train_loss: 34008.09973144531, validation_loss = 6597.8271484375
 Trial: 25 Epoch: 7, train_loss: 34950.11120605469, validation_loss = 5603.7984619140625
 Trial: 25 Epoch: 8, train_loss: 33244.970947265625, validation_loss = 4300.4296875
 Trial: 25 Epoch: 9, train_loss: 35679.81140136719, validation_loss = 5131.11181640625
 Trial: 25 Epoch: 10, train_loss: 41359.48486328125, validation_loss = 4482.11865234375
 Trial: 25 Epoch: 11, train_loss: 34800.89514160156, validation_loss = 4713.775634765625
 Trial: 25 Epoch: 12, train_loss: 30310.1044921875, validation_loss = 5845.4017333984375
 Trial: 25 Epoch: 13, train_loss: 30465.988403320312, validation_loss = 4571.513427734375
 Trial: 25 Epoch: 14, train_loss: 28064.541259765625, validation_loss = 3940.53076171875
 Trial: 25 Epoch: 15, train_loss: 27810.48291015625, validation_loss = 4163.5467529296875
 Trial: 25 Epoch: 16, train_loss: 26835.603637695312, validation_loss = 3766.7000732421875
 Trial: 25 Epoch: 17, train_loss: 28136.3232421875, validation_loss = 3895.0650634765625
 Trial: 25 Epoch: 18, train_loss: 26656.324584960938, validation_loss = 3855.888671875
 Trial: 25 Epoch: 19, train_loss: 27572.672485351562, validation_loss = 3627.4454345703125
 Trial: 25 Epoch: 20, train_loss: 28216.141357421875, validation_loss = 3675.748291015625
 Trial: 25 Epoch: 21, train_loss: 28597.2607421875, validation_loss = 3789.8436279296875
 Trial: 25 Epoch: 22, train_loss: 26103.2861328125, validation_loss = 4152.9051513671875
 Trial: 25 Epoch: 23, train_loss: 28869.74298095703, validation_loss = 3502.4677734375
 Trial: 25 Epoch: 24, train_loss: 25806.30126953125, validation_loss = 6372.946533203125
 Trial: 25 Epoch: 25, train_loss: 27175.706665039062, validation_loss = 3662.5523681640625
 Trial: 25 Epoch: 26, train_loss: 28847.19384765625, validation_loss = 3807.7142333984375
 Trial: 25 Epoch: 27, train_loss: 32221.590698242188, validation_loss = 4646.3468017578125
 Trial: 25 Epoch: 28, train_loss: 28381.32196044922, validation_loss = 4321.1231689453125
 Trial: 25 Epoch: 29, train_loss: 25119.94091796875, validation_loss = 4018.2896728515625
 Trial: 25 Epoch: 30, train_loss: 24121.374755859375, validation_loss = 3997.67431640625
 Trial: 25 Epoch: 31, train_loss: 25214.49560546875, validation_loss = 4892.5123291015625
 Trial: 25 Epoch: 32, train_loss: 26675.632446289062, validation_loss = 3480.586669921875
 Trial: 25 Epoch: 33, train_loss: 24958.435546875, validation_loss = 3528.416748046875
 Trial: 25 Epoch: 34, train_loss: 25734.614501953125, validation_loss = 5146.53173828125
 Trial: 25 Epoch: 35, train_loss: 28384.829833984375, validation_loss = 4100.363037109375
 Trial: 25 Epoch: 36, train_loss: 26666.641357421875, validation_loss = 4582.39111328125
 Trial: 25 Epoch: 37, train_loss: 26036.443969726562, validation_loss = 3912.6829833984375
 Trial: 25 Epoch: 38, train_loss: 23280.213256835938, validation_loss = 3400.5113525390625
 Trial: 25 Epoch: 39, train_loss: 23658.952514648438, validation_loss = 3544.2247314453125
 Trial: 25 Epoch: 40, train_loss: 24801.122802734375, validation_loss = 3758.74462890625
 Trial: 25 Epoch: 41, train_loss: 25621.07666015625, validation_loss = 5662.501708984375
 Trial: 25 Epoch: 42, train_loss: 25690.672973632812, validation_loss = 3587.1837158203125
 Trial: 25 Epoch: 43, train_loss: 23393.411743164062, validation_loss = 3722.5347900390625
 Trial: 25 Epoch: 44, train_loss: 22454.667053222656, validation_loss = 3290.668701171875
 Trial: 25 Epoch: 45, train_loss: 23842.2197265625, validation_loss = 3438.2470703125
 Trial: 25 Epoch: 46, train_loss: 21722.406311035156, validation_loss = 3326.410888671875
 Trial: 25 Epoch: 47, train_loss: 21960.41583251953, validation_loss = 3471.1824951171875
 Trial: 25 Epoch: 48, train_loss: 25129.132446289062, validation_loss = 3229.5455322265625
 Trial: 25 Epoch: 49, train_loss: 24079.842224121094, validation_loss = 3672.980224609375
 Trial: 25 Epoch: 50, train_loss: 21955.626220703125, validation_loss = 3084.708740234375
 Trial: 25 Epoch: 51, train_loss: 22363.311157226562, validation_loss = 3451.3592529296875
 Trial: 25 Epoch: 52, train_loss: 22071.478454589844, validation_loss = 3029.6966552734375
 Trial: 25 Epoch: 53, train_loss: 22609.313842773438, validation_loss = 3227.634033203125
 Trial: 25 Epoch: 54, train_loss: 23507.896484375, validation_loss = 3639.48095703125
 Trial: 25 Epoch: 55, train_loss: 22306.406982421875, validation_loss = 3492.83203125
 Trial: 25 Epoch: 56, train_loss: 22518.362670898438, validation_loss = 3418.4403076171875
 Trial: 25 Epoch: 57, train_loss: 22243.86328125, validation_loss = 3364.2080078125
 Trial: 25 Epoch: 58, train_loss: 21645.495239257812, validation_loss = 4049.7862548828125
 Trial: 25 Epoch: 59, train_loss: 22028.9423828125, validation_loss = 3147.843505859375
 Trial: 25 Epoch: 60, train_loss: 22393.948364257812, validation_loss = 3445.5933837890625
 Trial: 25 Epoch: 61, train_loss: 22680.647827148438, validation_loss = 3779.89453125
 Trial: 25 Epoch: 62, train_loss: 24067.29052734375, validation_loss = 3383.927001953125
 Trial: 25 Epoch: 63, train_loss: 22056.39520263672, validation_loss = 3238.28759765625
 Trial: 25 Epoch: 64, train_loss: 28584.42626953125, validation_loss = 4213.4781494140625
 Trial: 25 Epoch: 65, train_loss: 24796.923095703125, validation_loss = 3659.2393798828125
 Trial: 25 Epoch: 66, train_loss: 22458.904174804688, validation_loss = 3032.5262451171875
 Trial: 25 Epoch: 67, train_loss: 21635.841674804688, validation_loss = 3159.1744384765625
 Trial: 25 Epoch: 68, train_loss: 20165.83154296875, validation_loss = 3408.9349365234375
 Trial: 25 Epoch: 69, train_loss: 21065.399169921875, validation_loss = 3255.0025634765625
 Trial: 25 Epoch: 70, train_loss: 20480.373657226562, validation_loss = 3806.1971435546875
 Trial: 25 Epoch: 71, train_loss: 22076.28887939453, validation_loss = 3850.0059814453125
 Trial: 25 Epoch: 72, train_loss: 21744.263427734375, validation_loss = 3305.91064453125
 Trial: 25 Epoch: 73, train_loss: 21053.67742919922, validation_loss = 3100.8602294921875
 Trial: 25 Epoch: 74, train_loss: 21254.227905273438, validation_loss = 3282.9241943359375
 Trial: 25 Epoch: 75, train_loss: 21328.81494140625, validation_loss = 3937.236572265625
 Trial: 25 Epoch: 76, train_loss: 21049.519958496094, validation_loss = 3569.272216796875
 Trial: 25 Epoch: 77, train_loss: 20259.75128173828, validation_loss = 2935.498291015625
 Trial: 25 Epoch: 78, train_loss: 20674.724975585938, validation_loss = 3702.2974853515625
 Trial: 25 Epoch: 79, train_loss: 21011.826049804688, validation_loss = 3127.8104248046875
 Trial: 25 Epoch: 80, train_loss: 20405.49334716797, validation_loss = 2908.6650390625
 Trial: 25 Epoch: 81, train_loss: 20024.048095703125, validation_loss = 3161.9285888671875
 Trial: 25 Epoch: 82, train_loss: 21733.275512695312, validation_loss = 3060.98876953125
 Trial: 25 Epoch: 83, train_loss: 21821.644409179688, validation_loss = 3970.0408935546875
 Trial: 25 Epoch: 84, train_loss: 22828.993041992188, validation_loss = 3240.83056640625
 Trial: 25 Epoch: 85, train_loss: 21462.229370117188, validation_loss = 4534.3414306640625
 Trial: 25 Epoch: 86, train_loss: 22961.5166015625, validation_loss = 3060.6123046875
 Trial: 25 Epoch: 87, train_loss: 21073.18145751953, validation_loss = 3261.6990966796875
 Trial: 25 Epoch: 88, train_loss: 22046.131774902344, validation_loss = 4329.4735107421875
 Trial: 25 Epoch: 89, train_loss: 24186.770751953125, validation_loss = 3615.3453369140625
 Trial: 25 Epoch: 90, train_loss: 30007.938110351562, validation_loss = 3471.5562744140625
 Trial: 25 Epoch: 91, train_loss: 26176.268981933594, validation_loss = 4081.435302734375
 Trial: 25 Epoch: 92, train_loss: 22809.316040039062, validation_loss = 3590.2615966796875
 Trial: 25 Epoch: 93, train_loss: 24025.008911132812, validation_loss = 3721.663330078125
 Trial: 25 Epoch: 94, train_loss: 22121.5234375, validation_loss = 3370.62890625
 Trial: 25 Epoch: 95, train_loss: 23425.10821533203, validation_loss = 3065.2872314453125
 Trial: 25 Epoch: 96, train_loss: 19968.27960205078, validation_loss = 3409.9923095703125
 Trial: 25 Epoch: 97, train_loss: 20368.115478515625, validation_loss = 3097.71875
 Trial: 25 Epoch: 98, train_loss: 20306.350463867188, validation_loss = 3141.9603271484375
 Trial: 25 Epoch: 99, train_loss: 20488.80548095703, validation_loss = 3120.1837158203125
========Trial 26 params: {'n_layers': 3, 'K': 2, 'Dropout': 0.2, 'Hidden0': 4, 'Hidden1': 64, 'Hidden2': 128, 'optimizer': 'Adam', 'lr': 0.008291120007239226, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |    192     |
|    sptH.0.gcn.bias     |     4      |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.sAtt.Vs     |    729     |
|     sptH.2.sAtt.bs     |    729     |
|     sptH.2.sAtt.W1     |     4      |
|     sptH.2.sAtt.W2     |     4      |
|     sptH.2.sAtt.W3     |     1      |
|   sptH.2.gcn.weight    |    512     |
|    sptH.2.gcn.bias     |     64     |
| sptH.2.timeConv.weight |     3      |
|  sptH.2.timeConv.bias  |     1      |
|     sptH.4.sAtt.Vs     |    729     |
|     sptH.4.sAtt.bs     |    729     |
|     sptH.4.sAtt.W1     |     64     |
|     sptH.4.sAtt.W2     |     64     |
|     sptH.4.sAtt.W3     |     1      |
|   sptH.4.gcn.weight    |   16384    |
|    sptH.4.gcn.bias     |    128     |
| sptH.4.timeConv.weight |     3      |
|  sptH.4.timeConv.bias  |     1      |
|     sptH.6.weight      |    1536    |
|      sptH.6.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |     96     |
|    sptD.0.gcn.bias     |     4      |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.sAtt.Vs     |    729     |
|     sptD.2.sAtt.bs     |    729     |
|     sptD.2.sAtt.W1     |     4      |
|     sptD.2.sAtt.W2     |     4      |
|     sptD.2.sAtt.W3     |     1      |
|   sptD.2.gcn.weight    |    512     |
|    sptD.2.gcn.bias     |     64     |
| sptD.2.timeConv.weight |     3      |
|  sptD.2.timeConv.bias  |     1      |
|     sptD.4.sAtt.Vs     |    729     |
|     sptD.4.sAtt.bs     |    729     |
|     sptD.4.sAtt.W1     |     64     |
|     sptD.4.sAtt.W2     |     64     |
|     sptD.4.sAtt.W3     |     1      |
|   sptD.4.gcn.weight    |   16384    |
|    sptD.4.gcn.bias     |    128     |
| sptD.4.timeConv.weight |     3      |
|  sptD.4.timeConv.bias  |     1      |
|     sptD.6.weight      |    1536    |
|      sptD.6.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |    192     |
|    sptW.0.gcn.bias     |     4      |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.sAtt.Vs     |    729     |
|     sptW.2.sAtt.bs     |    729     |
|     sptW.2.sAtt.W1     |     4      |
|     sptW.2.sAtt.W2     |     4      |
|     sptW.2.sAtt.W3     |     1      |
|   sptW.2.gcn.weight    |    512     |
|    sptW.2.gcn.bias     |     64     |
| sptW.2.timeConv.weight |     3      |
|  sptW.2.timeConv.bias  |     1      |
|     sptW.4.sAtt.Vs     |    729     |
|     sptW.4.sAtt.bs     |    729     |
|     sptW.4.sAtt.W1     |     64     |
|     sptW.4.sAtt.W2     |     64     |
|     sptW.4.sAtt.W3     |     1      |
|   sptW.4.gcn.weight    |   16384    |
|    sptW.4.gcn.bias     |    128     |
| sptW.4.timeConv.weight |     3      |
|  sptW.4.timeConv.bias  |     1      |
|     sptW.6.weight      |    1536    |
|      sptW.6.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 70131
 Trial: 26 Epoch: 0, train_loss: 250321.51171875, validation_loss = 16607.892578125
========Trial 27 params: {'n_layers': 2, 'K': 2, 'Dropout': 0.2, 'Hidden0': 64, 'Hidden1': 64, 'optimizer': 'Adam', 'lr': 0.005168748929693061, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |    3072    |
|    sptH.0.gcn.bias     |     64     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.sAtt.Vs     |    729     |
|     sptH.2.sAtt.bs     |    729     |
|     sptH.2.sAtt.W1     |     64     |
|     sptH.2.sAtt.W2     |     64     |
|     sptH.2.sAtt.W3     |     1      |
|   sptH.2.gcn.weight    |    8192    |
|    sptH.2.gcn.bias     |     64     |
| sptH.2.timeConv.weight |     3      |
|  sptH.2.timeConv.bias  |     1      |
|     sptH.4.weight      |    768     |
|      sptH.4.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    1536    |
|    sptD.0.gcn.bias     |     64     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.sAtt.Vs     |    729     |
|     sptD.2.sAtt.bs     |    729     |
|     sptD.2.sAtt.W1     |     64     |
|     sptD.2.sAtt.W2     |     64     |
|     sptD.2.sAtt.W3     |     1      |
|   sptD.2.gcn.weight    |    8192    |
|    sptD.2.gcn.bias     |     64     |
| sptD.2.timeConv.weight |     3      |
|  sptD.2.timeConv.bias  |     1      |
|     sptD.4.weight      |    768     |
|      sptD.4.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |    3072    |
|    sptW.0.gcn.bias     |     64     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.sAtt.Vs     |    729     |
|     sptW.2.sAtt.bs     |    729     |
|     sptW.2.sAtt.W1     |     64     |
|     sptW.2.sAtt.W2     |     64     |
|     sptW.2.sAtt.W3     |     1      |
|   sptW.2.gcn.weight    |    8192    |
|    sptW.2.gcn.bias     |     64     |
| sptW.2.timeConv.weight |     3      |
|  sptW.2.timeConv.bias  |     1      |
|     sptW.4.weight      |    768     |
|      sptW.4.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 44298
 Trial: 27 Epoch: 0, train_loss: 227636.32763671875, validation_loss = 10826.6357421875
 Trial: 27 Epoch: 1, train_loss: 70566.20703125, validation_loss = 8120.224609375
 Trial: 27 Epoch: 2, train_loss: 47675.104736328125, validation_loss = 6216.330078125
 Trial: 27 Epoch: 3, train_loss: 41130.408935546875, validation_loss = 6472.977783203125
 Trial: 27 Epoch: 4, train_loss: 45305.51525878906, validation_loss = 4569.1201171875
 Trial: 27 Epoch: 5, train_loss: 35489.32019042969, validation_loss = 4897.4375
 Trial: 27 Epoch: 6, train_loss: 35476.02990722656, validation_loss = 5788.0242919921875
 Trial: 27 Epoch: 7, train_loss: 34817.08825683594, validation_loss = 4949.6795654296875
 Trial: 27 Epoch: 8, train_loss: 35668.98486328125, validation_loss = 6358.6260986328125
 Trial: 27 Epoch: 9, train_loss: 36929.560546875, validation_loss = 4125.8121337890625
 Trial: 27 Epoch: 10, train_loss: 34520.294189453125, validation_loss = 4655.6790771484375
 Trial: 27 Epoch: 11, train_loss: 30741.260131835938, validation_loss = 4891.035400390625
 Trial: 27 Epoch: 12, train_loss: 30324.7333984375, validation_loss = 4133.20703125
 Trial: 27 Epoch: 13, train_loss: 28405.811279296875, validation_loss = 6323.61376953125
 Trial: 27 Epoch: 14, train_loss: 28127.652954101562, validation_loss = 4443.7152099609375
 Trial: 27 Epoch: 15, train_loss: 27949.52197265625, validation_loss = 3831.643310546875
 Trial: 27 Epoch: 16, train_loss: 28958.918579101562, validation_loss = 3579.3280029296875
 Trial: 27 Epoch: 17, train_loss: 29576.066284179688, validation_loss = 5764.0853271484375
 Trial: 27 Epoch: 18, train_loss: 30233.427612304688, validation_loss = 4782.4984130859375
 Trial: 27 Epoch: 19, train_loss: 31389.203491210938, validation_loss = 5135.865478515625
 Trial: 27 Epoch: 20, train_loss: 26256.06689453125, validation_loss = 3507.068115234375
 Trial: 27 Epoch: 21, train_loss: 25097.569946289062, validation_loss = 3918.4451904296875
 Trial: 27 Epoch: 22, train_loss: 26119.632202148438, validation_loss = 3474.8919677734375
 Trial: 27 Epoch: 23, train_loss: 23534.75067138672, validation_loss = 3540.4644775390625
 Trial: 27 Epoch: 24, train_loss: 23497.792602539062, validation_loss = 3240.5618896484375
 Trial: 27 Epoch: 25, train_loss: 24215.714111328125, validation_loss = 3905.6966552734375
 Trial: 27 Epoch: 26, train_loss: 25517.240844726562, validation_loss = 3878.282470703125
 Trial: 27 Epoch: 27, train_loss: 23357.621337890625, validation_loss = 3494.554443359375
 Trial: 27 Epoch: 28, train_loss: 24443.902099609375, validation_loss = 3737.1168212890625
 Trial: 27 Epoch: 29, train_loss: 23515.731079101562, validation_loss = 3448.8582763671875
 Trial: 27 Epoch: 30, train_loss: 23986.77490234375, validation_loss = 3996.68212890625
 Trial: 27 Epoch: 31, train_loss: 25980.7744140625, validation_loss = 3312.656982421875
 Trial: 27 Epoch: 32, train_loss: 23805.218994140625, validation_loss = 3655.3572998046875
 Trial: 27 Epoch: 33, train_loss: 22747.380859375, validation_loss = 3469.207275390625
 Trial: 27 Epoch: 34, train_loss: 25483.4580078125, validation_loss = 3577.45947265625
 Trial: 27 Epoch: 35, train_loss: 27382.984802246094, validation_loss = 3585.063720703125
 Trial: 27 Epoch: 36, train_loss: 23984.220092773438, validation_loss = 3034.216552734375
 Trial: 27 Epoch: 37, train_loss: 22978.584716796875, validation_loss = 3316.2574462890625
 Trial: 27 Epoch: 38, train_loss: 22375.619018554688, validation_loss = 3621.3974609375
 Trial: 27 Epoch: 39, train_loss: 23645.701171875, validation_loss = 3448.863525390625
 Trial: 27 Epoch: 40, train_loss: 22213.210998535156, validation_loss = 3957.8992919921875
 Trial: 27 Epoch: 41, train_loss: 24349.657104492188, validation_loss = 3995.084228515625
 Trial: 27 Epoch: 42, train_loss: 22068.172485351562, validation_loss = 3167.98486328125
 Trial: 27 Epoch: 43, train_loss: 23548.97607421875, validation_loss = 3750.4564208984375
 Trial: 27 Epoch: 44, train_loss: 23138.871826171875, validation_loss = 3726.51708984375
 Trial: 27 Epoch: 45, train_loss: 24231.480102539062, validation_loss = 3685.882568359375
 Trial: 27 Epoch: 46, train_loss: 23413.47332763672, validation_loss = 3487.4041748046875
 Trial: 27 Epoch: 47, train_loss: 25758.34521484375, validation_loss = 3479.29345703125
 Trial: 27 Epoch: 48, train_loss: 22134.455505371094, validation_loss = 3554.1138916015625
 Trial: 27 Epoch: 49, train_loss: 21840.162658691406, validation_loss = 4205.4879150390625
 Trial: 27 Epoch: 50, train_loss: 23768.75030517578, validation_loss = 3300.176025390625
 Trial: 27 Epoch: 51, train_loss: 21930.700256347656, validation_loss = 3427.32568359375
 Trial: 27 Epoch: 52, train_loss: 22020.472290039062, validation_loss = 3082.700439453125
 Trial: 27 Epoch: 53, train_loss: 21626.012329101562, validation_loss = 3632.573486328125
 Trial: 27 Epoch: 54, train_loss: 22765.9873046875, validation_loss = 3261.2301025390625
 Trial: 27 Epoch: 55, train_loss: 20234.320068359375, validation_loss = 3262.7960205078125
 Trial: 27 Epoch: 56, train_loss: 20854.134155273438, validation_loss = 3684.9041748046875
 Trial: 27 Epoch: 57, train_loss: 20707.579467773438, validation_loss = 2934.570068359375
 Trial: 27 Epoch: 58, train_loss: 21156.952026367188, validation_loss = 3567.932373046875
 Trial: 27 Epoch: 59, train_loss: 19527.178100585938, validation_loss = 3452.780029296875
 Trial: 27 Epoch: 60, train_loss: 21832.103881835938, validation_loss = 3038.189208984375
 Trial: 27 Epoch: 61, train_loss: 24407.872619628906, validation_loss = 4715.8748779296875
 Trial: 27 Epoch: 62, train_loss: 28083.556762695312, validation_loss = 3240.3511962890625
 Trial: 27 Epoch: 63, train_loss: 26601.28253173828, validation_loss = 3930.977294921875
 Trial: 27 Epoch: 64, train_loss: 28705.938842773438, validation_loss = 4762.470703125
 Trial: 27 Epoch: 65, train_loss: 26103.43914794922, validation_loss = 3045.1231689453125
 Trial: 27 Epoch: 66, train_loss: 22301.22052001953, validation_loss = 3619.29248046875
 Trial: 27 Epoch: 67, train_loss: 21516.655395507812, validation_loss = 3279.429443359375
 Trial: 27 Epoch: 68, train_loss: 19794.339599609375, validation_loss = 3235.548095703125
 Trial: 27 Epoch: 69, train_loss: 19956.623168945312, validation_loss = 3326.9539794921875
 Trial: 27 Epoch: 70, train_loss: 20698.462341308594, validation_loss = 3012.2305908203125
 Trial: 27 Epoch: 71, train_loss: 20029.868408203125, validation_loss = 3244.31787109375
 Trial: 27 Epoch: 72, train_loss: 20269.390625, validation_loss = 3220.8580322265625
 Trial: 27 Epoch: 73, train_loss: 20332.588745117188, validation_loss = 3388.7440185546875
 Trial: 27 Epoch: 74, train_loss: 20664.738525390625, validation_loss = 3628.66748046875
 Trial: 27 Epoch: 75, train_loss: 19960.854614257812, validation_loss = 3265.0892333984375
 Trial: 27 Epoch: 76, train_loss: 20108.71856689453, validation_loss = 3565.962890625
 Trial: 27 Epoch: 77, train_loss: 20745.71661376953, validation_loss = 3116.007080078125
 Trial: 27 Epoch: 78, train_loss: 19828.494262695312, validation_loss = 4283.2344970703125
 Trial: 27 Epoch: 79, train_loss: 20527.337036132812, validation_loss = 3546.1656494140625
 Trial: 27 Epoch: 80, train_loss: 21005.56396484375, validation_loss = 3215.98193359375
 Trial: 27 Epoch: 81, train_loss: 21042.7275390625, validation_loss = 3086.8367919921875
 Trial: 27 Epoch: 82, train_loss: 20476.570922851562, validation_loss = 3380.068603515625
 Trial: 27 Epoch: 83, train_loss: 20553.09600830078, validation_loss = 3403.41064453125
 Trial: 27 Epoch: 84, train_loss: 20612.723876953125, validation_loss = 3195.2850341796875
 Trial: 27 Epoch: 85, train_loss: 19336.4296875, validation_loss = 2976.1788330078125
 Trial: 27 Epoch: 86, train_loss: 19790.531188964844, validation_loss = 2895.1864013671875
 Trial: 27 Epoch: 87, train_loss: 20578.07208251953, validation_loss = 3664.4072265625
 Trial: 27 Epoch: 88, train_loss: 20378.636108398438, validation_loss = 3232.3709716796875
 Trial: 27 Epoch: 89, train_loss: 20831.484985351562, validation_loss = 3434.0218505859375
 Trial: 27 Epoch: 90, train_loss: 19577.92724609375, validation_loss = 2963.41259765625
 Trial: 27 Epoch: 91, train_loss: 21068.55987548828, validation_loss = 3692.330810546875
 Trial: 27 Epoch: 92, train_loss: 20035.036743164062, validation_loss = 2954.265625
 Trial: 27 Epoch: 93, train_loss: 18957.34619140625, validation_loss = 2836.69873046875
 Trial: 27 Epoch: 94, train_loss: 19668.221557617188, validation_loss = 3155.9130859375
 Trial: 27 Epoch: 95, train_loss: 18524.104614257812, validation_loss = 3013.4818115234375
 Trial: 27 Epoch: 96, train_loss: 19507.307556152344, validation_loss = 2946.2838134765625
 Trial: 27 Epoch: 97, train_loss: 19409.71258544922, validation_loss = 3484.135009765625
 Trial: 27 Epoch: 98, train_loss: 19568.14776611328, validation_loss = 3293.1304931640625
 Trial: 27 Epoch: 99, train_loss: 19576.396911621094, validation_loss = 3203.6282958984375
========Trial 28 params: {'n_layers': 2, 'K': 2, 'Dropout': 0.2, 'Hidden0': 64, 'Hidden1': 64, 'optimizer': 'Adam', 'lr': 0.004908121177266308, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |    3072    |
|    sptH.0.gcn.bias     |     64     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.sAtt.Vs     |    729     |
|     sptH.2.sAtt.bs     |    729     |
|     sptH.2.sAtt.W1     |     64     |
|     sptH.2.sAtt.W2     |     64     |
|     sptH.2.sAtt.W3     |     1      |
|   sptH.2.gcn.weight    |    8192    |
|    sptH.2.gcn.bias     |     64     |
| sptH.2.timeConv.weight |     3      |
|  sptH.2.timeConv.bias  |     1      |
|     sptH.4.weight      |    768     |
|      sptH.4.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    1536    |
|    sptD.0.gcn.bias     |     64     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.sAtt.Vs     |    729     |
|     sptD.2.sAtt.bs     |    729     |
|     sptD.2.sAtt.W1     |     64     |
|     sptD.2.sAtt.W2     |     64     |
|     sptD.2.sAtt.W3     |     1      |
|   sptD.2.gcn.weight    |    8192    |
|    sptD.2.gcn.bias     |     64     |
| sptD.2.timeConv.weight |     3      |
|  sptD.2.timeConv.bias  |     1      |
|     sptD.4.weight      |    768     |
|      sptD.4.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |    3072    |
|    sptW.0.gcn.bias     |     64     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.sAtt.Vs     |    729     |
|     sptW.2.sAtt.bs     |    729     |
|     sptW.2.sAtt.W1     |     64     |
|     sptW.2.sAtt.W2     |     64     |
|     sptW.2.sAtt.W3     |     1      |
|   sptW.2.gcn.weight    |    8192    |
|    sptW.2.gcn.bias     |     64     |
| sptW.2.timeConv.weight |     3      |
|  sptW.2.timeConv.bias  |     1      |
|     sptW.4.weight      |    768     |
|      sptW.4.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 44298
 Trial: 28 Epoch: 0, train_loss: 233893.8671875, validation_loss = 12027.030029296875
========Trial 29 params: {'n_layers': 1, 'K': 2, 'Dropout': 0.2, 'Hidden0': 64, 'optimizer': 'Adam', 'lr': 0.0028559448161383466, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |    3072    |
|    sptH.0.gcn.bias     |     64     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |    768     |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    1536    |
|    sptD.0.gcn.bias     |     64     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |    768     |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |    3072    |
|    sptW.0.gcn.bias     |     64     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |    768     |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 14757
 Trial: 29 Epoch: 0, train_loss: 258140.1064453125, validation_loss = 21384.08203125
========Trial 30 params: {'n_layers': 3, 'K': 2, 'Dropout': 0.2, 'Hidden0': 64, 'Hidden1': 64, 'Hidden2': 8, 'optimizer': 'Adam', 'lr': 0.0019430762419663326, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |    3072    |
|    sptH.0.gcn.bias     |     64     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.sAtt.Vs     |    729     |
|     sptH.2.sAtt.bs     |    729     |
|     sptH.2.sAtt.W1     |     64     |
|     sptH.2.sAtt.W2     |     64     |
|     sptH.2.sAtt.W3     |     1      |
|   sptH.2.gcn.weight    |    8192    |
|    sptH.2.gcn.bias     |     64     |
| sptH.2.timeConv.weight |     3      |
|  sptH.2.timeConv.bias  |     1      |
|     sptH.4.sAtt.Vs     |    729     |
|     sptH.4.sAtt.bs     |    729     |
|     sptH.4.sAtt.W1     |     64     |
|     sptH.4.sAtt.W2     |     64     |
|     sptH.4.sAtt.W3     |     1      |
|   sptH.4.gcn.weight    |    1024    |
|    sptH.4.gcn.bias     |     8      |
| sptH.4.timeConv.weight |     3      |
|  sptH.4.timeConv.bias  |     1      |
|     sptH.6.weight      |     96     |
|      sptH.6.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    1536    |
|    sptD.0.gcn.bias     |     64     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.sAtt.Vs     |    729     |
|     sptD.2.sAtt.bs     |    729     |
|     sptD.2.sAtt.W1     |     64     |
|     sptD.2.sAtt.W2     |     64     |
|     sptD.2.sAtt.W3     |     1      |
|   sptD.2.gcn.weight    |    8192    |
|    sptD.2.gcn.bias     |     64     |
| sptD.2.timeConv.weight |     3      |
|  sptD.2.timeConv.bias  |     1      |
|     sptD.4.sAtt.Vs     |    729     |
|     sptD.4.sAtt.bs     |    729     |
|     sptD.4.sAtt.W1     |     64     |
|     sptD.4.sAtt.W2     |     64     |
|     sptD.4.sAtt.W3     |     1      |
|   sptD.4.gcn.weight    |    1024    |
|    sptD.4.gcn.bias     |     8      |
| sptD.4.timeConv.weight |     3      |
|  sptD.4.timeConv.bias  |     1      |
|     sptD.6.weight      |     96     |
|      sptD.6.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |    3072    |
|    sptW.0.gcn.bias     |     64     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.sAtt.Vs     |    729     |
|     sptW.2.sAtt.bs     |    729     |
|     sptW.2.sAtt.W1     |     64     |
|     sptW.2.sAtt.W2     |     64     |
|     sptW.2.sAtt.W3     |     1      |
|   sptW.2.gcn.weight    |    8192    |
|    sptW.2.gcn.bias     |     64     |
| sptW.2.timeConv.weight |     3      |
|  sptW.2.timeConv.bias  |     1      |
|     sptW.4.sAtt.Vs     |    729     |
|     sptW.4.sAtt.bs     |    729     |
|     sptW.4.sAtt.W1     |     64     |
|     sptW.4.sAtt.W2     |     64     |
|     sptW.4.sAtt.W3     |     1      |
|   sptW.4.gcn.weight    |    1024    |
|    sptW.4.gcn.bias     |     8      |
| sptW.4.timeConv.weight |     3      |
|  sptW.4.timeConv.bias  |     1      |
|     sptW.6.weight      |     96     |
|      sptW.6.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 50151
 Trial: 30 Epoch: 0, train_loss: 441163.943359375, validation_loss = 48199.05078125
========Trial 31 params: {'n_layers': 2, 'K': 2, 'Dropout': 0.2, 'Hidden0': 32, 'Hidden1': 64, 'optimizer': 'Adam', 'lr': 0.004040665231964375, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |    1536    |
|    sptH.0.gcn.bias     |     32     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.sAtt.Vs     |    729     |
|     sptH.2.sAtt.bs     |    729     |
|     sptH.2.sAtt.W1     |     32     |
|     sptH.2.sAtt.W2     |     32     |
|     sptH.2.sAtt.W3     |     1      |
|   sptH.2.gcn.weight    |    4096    |
|    sptH.2.gcn.bias     |     64     |
| sptH.2.timeConv.weight |     3      |
|  sptH.2.timeConv.bias  |     1      |
|     sptH.4.weight      |    768     |
|      sptH.4.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    768     |
|    sptD.0.gcn.bias     |     32     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.sAtt.Vs     |    729     |
|     sptD.2.sAtt.bs     |    729     |
|     sptD.2.sAtt.W1     |     32     |
|     sptD.2.sAtt.W2     |     32     |
|     sptD.2.sAtt.W3     |     1      |
|   sptD.2.gcn.weight    |    4096    |
|    sptD.2.gcn.bias     |     64     |
| sptD.2.timeConv.weight |     3      |
|  sptD.2.timeConv.bias  |     1      |
|     sptD.4.weight      |    768     |
|      sptD.4.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |    1536    |
|    sptW.0.gcn.bias     |     32     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.sAtt.Vs     |    729     |
|     sptW.2.sAtt.bs     |    729     |
|     sptW.2.sAtt.W1     |     32     |
|     sptW.2.sAtt.W2     |     32     |
|     sptW.2.sAtt.W3     |     1      |
|   sptW.2.gcn.weight    |    4096    |
|    sptW.2.gcn.bias     |     64     |
| sptW.2.timeConv.weight |     3      |
|  sptW.2.timeConv.bias  |     1      |
|     sptW.4.weight      |    768     |
|      sptW.4.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 27882
 Trial: 31 Epoch: 0, train_loss: 313367.765625, validation_loss = 22726.01708984375
========Trial 32 params: {'n_layers': 2, 'K': 2, 'Dropout': 0.2, 'Hidden0': 8, 'Hidden1': 64, 'optimizer': 'Adam', 'lr': 0.00609830635517611, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |    384     |
|    sptH.0.gcn.bias     |     8      |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.sAtt.Vs     |    729     |
|     sptH.2.sAtt.bs     |    729     |
|     sptH.2.sAtt.W1     |     8      |
|     sptH.2.sAtt.W2     |     8      |
|     sptH.2.sAtt.W3     |     1      |
|   sptH.2.gcn.weight    |    1024    |
|    sptH.2.gcn.bias     |     64     |
| sptH.2.timeConv.weight |     3      |
|  sptH.2.timeConv.bias  |     1      |
|     sptH.4.weight      |    768     |
|      sptH.4.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    192     |
|    sptD.0.gcn.bias     |     8      |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.sAtt.Vs     |    729     |
|     sptD.2.sAtt.bs     |    729     |
|     sptD.2.sAtt.W1     |     8      |
|     sptD.2.sAtt.W2     |     8      |
|     sptD.2.sAtt.W3     |     1      |
|   sptD.2.gcn.weight    |    1024    |
|    sptD.2.gcn.bias     |     64     |
| sptD.2.timeConv.weight |     3      |
|  sptD.2.timeConv.bias  |     1      |
|     sptD.4.weight      |    768     |
|      sptD.4.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |    384     |
|    sptW.0.gcn.bias     |     8      |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.sAtt.Vs     |    729     |
|     sptW.2.sAtt.bs     |    729     |
|     sptW.2.sAtt.W1     |     8      |
|     sptW.2.sAtt.W2     |     8      |
|     sptW.2.sAtt.W3     |     1      |
|   sptW.2.gcn.weight    |    1024    |
|    sptW.2.gcn.bias     |     64     |
| sptW.2.timeConv.weight |     3      |
|  sptW.2.timeConv.bias  |     1      |
|     sptW.4.weight      |    768     |
|      sptW.4.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 15570
 Trial: 32 Epoch: 0, train_loss: 297118.21484375, validation_loss = 17872.27001953125
========Trial 33 params: {'n_layers': 2, 'K': 2, 'Dropout': 0.2, 'Hidden0': 128, 'Hidden1': 64, 'optimizer': 'Adam', 'lr': 0.007297762845229909, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |    6144    |
|    sptH.0.gcn.bias     |    128     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.sAtt.Vs     |    729     |
|     sptH.2.sAtt.bs     |    729     |
|     sptH.2.sAtt.W1     |    128     |
|     sptH.2.sAtt.W2     |    128     |
|     sptH.2.sAtt.W3     |     1      |
|   sptH.2.gcn.weight    |   16384    |
|    sptH.2.gcn.bias     |     64     |
| sptH.2.timeConv.weight |     3      |
|  sptH.2.timeConv.bias  |     1      |
|     sptH.4.weight      |    768     |
|      sptH.4.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    3072    |
|    sptD.0.gcn.bias     |    128     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.sAtt.Vs     |    729     |
|     sptD.2.sAtt.bs     |    729     |
|     sptD.2.sAtt.W1     |    128     |
|     sptD.2.sAtt.W2     |    128     |
|     sptD.2.sAtt.W3     |     1      |
|   sptD.2.gcn.weight    |   16384    |
|    sptD.2.gcn.bias     |     64     |
| sptD.2.timeConv.weight |     3      |
|  sptD.2.timeConv.bias  |     1      |
|     sptD.4.weight      |    768     |
|      sptD.4.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |    6144    |
|    sptW.0.gcn.bias     |    128     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.sAtt.Vs     |    729     |
|     sptW.2.sAtt.bs     |    729     |
|     sptW.2.sAtt.W1     |    128     |
|     sptW.2.sAtt.W2     |    128     |
|     sptW.2.sAtt.W3     |     1      |
|   sptW.2.gcn.weight    |   16384    |
|    sptW.2.gcn.bias     |     64     |
| sptW.2.timeConv.weight |     3      |
|  sptW.2.timeConv.bias  |     1      |
|     sptW.4.weight      |    768     |
|      sptW.4.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 77130
 Trial: 33 Epoch: 0, train_loss: 191777.9033203125, validation_loss = 10083.386962890625
 Trial: 33 Epoch: 1, train_loss: 59602.922119140625, validation_loss = 9936.787109375
========Trial 34 params: {'n_layers': 2, 'K': 2, 'Dropout': 0.2, 'Hidden0': 256, 'Hidden1': 64, 'optimizer': 'Adam', 'lr': 0.011536517099493075, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |   12288    |
|    sptH.0.gcn.bias     |    256     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.sAtt.Vs     |    729     |
|     sptH.2.sAtt.bs     |    729     |
|     sptH.2.sAtt.W1     |    256     |
|     sptH.2.sAtt.W2     |    256     |
|     sptH.2.sAtt.W3     |     1      |
|   sptH.2.gcn.weight    |   32768    |
|    sptH.2.gcn.bias     |     64     |
| sptH.2.timeConv.weight |     3      |
|  sptH.2.timeConv.bias  |     1      |
|     sptH.4.weight      |    768     |
|      sptH.4.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    6144    |
|    sptD.0.gcn.bias     |    256     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.sAtt.Vs     |    729     |
|     sptD.2.sAtt.bs     |    729     |
|     sptD.2.sAtt.W1     |    256     |
|     sptD.2.sAtt.W2     |    256     |
|     sptD.2.sAtt.W3     |     1      |
|   sptD.2.gcn.weight    |   32768    |
|    sptD.2.gcn.bias     |     64     |
| sptD.2.timeConv.weight |     3      |
|  sptD.2.timeConv.bias  |     1      |
|     sptD.4.weight      |    768     |
|      sptD.4.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |   12288    |
|    sptW.0.gcn.bias     |    256     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.sAtt.Vs     |    729     |
|     sptW.2.sAtt.bs     |    729     |
|     sptW.2.sAtt.W1     |    256     |
|     sptW.2.sAtt.W2     |    256     |
|     sptW.2.sAtt.W3     |     1      |
|   sptW.2.gcn.weight    |   32768    |
|    sptW.2.gcn.bias     |     64     |
| sptW.2.timeConv.weight |     3      |
|  sptW.2.timeConv.bias  |     1      |
|     sptW.4.weight      |    768     |
|      sptW.4.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 142794
 Trial: 34 Epoch: 0, train_loss: 168293.32421875, validation_loss = 6400.2021484375
 Trial: 34 Epoch: 1, train_loss: 53680.697998046875, validation_loss = 5881.9814453125
 Trial: 34 Epoch: 2, train_loss: 45661.09326171875, validation_loss = 5318.2237548828125
 Trial: 34 Epoch: 3, train_loss: 39843.16149902344, validation_loss = 7095.72900390625
 Trial: 34 Epoch: 4, train_loss: 39288.54748535156, validation_loss = 6040.74609375
 Trial: 34 Epoch: 5, train_loss: 40882.443115234375, validation_loss = 4036.24267578125
 Trial: 34 Epoch: 6, train_loss: 33985.16247558594, validation_loss = 5814.166748046875
 Trial: 34 Epoch: 7, train_loss: 38031.50427246094, validation_loss = 4197.5640869140625
 Trial: 34 Epoch: 8, train_loss: 32973.26452636719, validation_loss = 5629.780517578125
 Trial: 34 Epoch: 9, train_loss: 37402.477783203125, validation_loss = 4381.289794921875
 Trial: 34 Epoch: 10, train_loss: 32649.294799804688, validation_loss = 5919.5645751953125
 Trial: 34 Epoch: 11, train_loss: 33931.406982421875, validation_loss = 4429.4764404296875
 Trial: 34 Epoch: 12, train_loss: 31850.205444335938, validation_loss = 3955.0479736328125
 Trial: 34 Epoch: 13, train_loss: 31302.183471679688, validation_loss = 4426.0360107421875
 Trial: 34 Epoch: 14, train_loss: 32433.282592773438, validation_loss = 3941.41796875
 Trial: 34 Epoch: 15, train_loss: 35692.85107421875, validation_loss = 4384.5289306640625
 Trial: 34 Epoch: 16, train_loss: 39368.90576171875, validation_loss = 4034.8121337890625
 Trial: 34 Epoch: 17, train_loss: 30130.267944335938, validation_loss = 4967.0845947265625
 Trial: 34 Epoch: 18, train_loss: 30773.3876953125, validation_loss = 4634.0888671875
 Trial: 34 Epoch: 19, train_loss: 35306.87683105469, validation_loss = 3278.921875
 Trial: 34 Epoch: 20, train_loss: 36035.219970703125, validation_loss = 4583.343994140625
 Trial: 34 Epoch: 21, train_loss: 42759.93566894531, validation_loss = 6530.519287109375
 Trial: 34 Epoch: 22, train_loss: 43496.93933105469, validation_loss = 7659.852783203125
 Trial: 34 Epoch: 23, train_loss: 31691.40478515625, validation_loss = 4651.5096435546875
 Trial: 34 Epoch: 24, train_loss: 27851.378051757812, validation_loss = 3682.397216796875
 Trial: 34 Epoch: 25, train_loss: 29257.940185546875, validation_loss = 3754.5870361328125
 Trial: 34 Epoch: 26, train_loss: 28344.75048828125, validation_loss = 6405.87353515625
 Trial: 34 Epoch: 27, train_loss: 27251.337524414062, validation_loss = 3326.2979736328125
 Trial: 34 Epoch: 28, train_loss: 26229.02587890625, validation_loss = 3674.605224609375
 Trial: 34 Epoch: 29, train_loss: 29532.005004882812, validation_loss = 4784.9393310546875
 Trial: 34 Epoch: 30, train_loss: 27182.466552734375, validation_loss = 3506.1646728515625
 Trial: 34 Epoch: 31, train_loss: 26644.880126953125, validation_loss = 3836.8004150390625
 Trial: 34 Epoch: 32, train_loss: 25230.019409179688, validation_loss = 4087.79052734375
 Trial: 34 Epoch: 33, train_loss: 27450.643188476562, validation_loss = 4716.8460693359375
 Trial: 34 Epoch: 34, train_loss: 31570.095703125, validation_loss = 3732.589111328125
 Trial: 34 Epoch: 35, train_loss: 30645.51611328125, validation_loss = 3552.3118896484375
 Trial: 34 Epoch: 36, train_loss: 27961.377197265625, validation_loss = 4763.9705810546875
 Trial: 34 Epoch: 37, train_loss: 26589.977172851562, validation_loss = 3674.0887451171875
 Trial: 34 Epoch: 38, train_loss: 25195.3994140625, validation_loss = 3854.9140625
 Trial: 34 Epoch: 39, train_loss: 27985.96630859375, validation_loss = 4221.201904296875
 Trial: 34 Epoch: 40, train_loss: 30164.594360351562, validation_loss = 3609.541748046875
 Trial: 34 Epoch: 41, train_loss: 28628.77978515625, validation_loss = 5762.8427734375
 Trial: 34 Epoch: 42, train_loss: 29875.7333984375, validation_loss = 3052.61962890625
 Trial: 34 Epoch: 43, train_loss: 26671.574951171875, validation_loss = 4352.0067138671875
 Trial: 34 Epoch: 44, train_loss: 26548.24951171875, validation_loss = 3172.447509765625
 Trial: 34 Epoch: 45, train_loss: 24259.749938964844, validation_loss = 3657.7861328125
 Trial: 34 Epoch: 46, train_loss: 24729.4755859375, validation_loss = 3401.714111328125
 Trial: 34 Epoch: 47, train_loss: 26024.7685546875, validation_loss = 3142.830322265625
 Trial: 34 Epoch: 48, train_loss: 26321.14599609375, validation_loss = 3617.819091796875
 Trial: 34 Epoch: 49, train_loss: 25423.266723632812, validation_loss = 4100.73291015625
 Trial: 34 Epoch: 50, train_loss: 23436.605102539062, validation_loss = 4681.541259765625
 Trial: 34 Epoch: 51, train_loss: 23638.818481445312, validation_loss = 3864.443603515625
 Trial: 34 Epoch: 52, train_loss: 25649.232055664062, validation_loss = 3412.5718994140625
 Trial: 34 Epoch: 53, train_loss: 24274.952026367188, validation_loss = 3031.725830078125
 Trial: 34 Epoch: 54, train_loss: 24587.8056640625, validation_loss = 3327.768798828125
 Trial: 34 Epoch: 55, train_loss: 25173.655395507812, validation_loss = 4436.3551025390625
 Trial: 34 Epoch: 56, train_loss: 27574.617309570312, validation_loss = 3279.912353515625
 Trial: 34 Epoch: 57, train_loss: 28555.82080078125, validation_loss = 4303.9761962890625
 Trial: 34 Epoch: 58, train_loss: 26542.238647460938, validation_loss = 3958.48828125
 Trial: 34 Epoch: 59, train_loss: 29104.577392578125, validation_loss = 4869.282470703125
 Trial: 34 Epoch: 60, train_loss: 26155.820251464844, validation_loss = 2954.3302001953125
 Trial: 34 Epoch: 61, train_loss: 29611.145629882812, validation_loss = 6202.7490234375
 Trial: 34 Epoch: 62, train_loss: 25340.217163085938, validation_loss = 3561.8115234375
 Trial: 34 Epoch: 63, train_loss: 24015.8671875, validation_loss = 3685.3485107421875
 Trial: 34 Epoch: 64, train_loss: 25433.348754882812, validation_loss = 3703.0673828125
 Trial: 34 Epoch: 65, train_loss: 23933.148559570312, validation_loss = 4225.2064208984375
 Trial: 34 Epoch: 66, train_loss: 27223.369018554688, validation_loss = 3204.7423095703125
 Trial: 34 Epoch: 67, train_loss: 27350.465209960938, validation_loss = 4654.056884765625
 Trial: 34 Epoch: 68, train_loss: 27117.13232421875, validation_loss = 4134.979736328125
 Trial: 34 Epoch: 69, train_loss: 30064.1552734375, validation_loss = 3481.766845703125
 Trial: 34 Epoch: 70, train_loss: 32917.87158203125, validation_loss = 3795.33935546875
 Trial: 34 Epoch: 71, train_loss: 26800.211791992188, validation_loss = 6365.883056640625
 Trial: 34 Epoch: 72, train_loss: 26688.482421875, validation_loss = 2937.951904296875
 Trial: 34 Epoch: 73, train_loss: 24895.962524414062, validation_loss = 3380.0198974609375
 Trial: 34 Epoch: 74, train_loss: 24124.456909179688, validation_loss = 4731.574462890625
 Trial: 34 Epoch: 75, train_loss: 24328.189086914062, validation_loss = 2975.8809814453125
 Trial: 34 Epoch: 76, train_loss: 30142.66064453125, validation_loss = 4550.7484130859375
 Trial: 34 Epoch: 77, train_loss: 28223.766723632812, validation_loss = 4435.473876953125
 Trial: 34 Epoch: 78, train_loss: 32989.518798828125, validation_loss = 3929.544921875
 Trial: 34 Epoch: 79, train_loss: 28880.702758789062, validation_loss = 3499.4132080078125
 Trial: 34 Epoch: 80, train_loss: 27593.770385742188, validation_loss = 3302.2794189453125
 Trial: 34 Epoch: 81, train_loss: 27439.964599609375, validation_loss = 3100.1563720703125
 Trial: 34 Epoch: 82, train_loss: 23477.646545410156, validation_loss = 4154.0506591796875
 Trial: 34 Epoch: 83, train_loss: 24142.59881591797, validation_loss = 3337.310791015625
 Trial: 34 Epoch: 84, train_loss: 25678.210205078125, validation_loss = 3512.8868408203125
 Trial: 34 Epoch: 85, train_loss: 25476.426635742188, validation_loss = 3503.5118408203125
 Trial: 34 Epoch: 86, train_loss: 22952.984130859375, validation_loss = 3278.444091796875
 Trial: 34 Epoch: 87, train_loss: 28862.318969726562, validation_loss = 3032.8460693359375
 Trial: 34 Epoch: 88, train_loss: 26013.548706054688, validation_loss = 4364.68994140625
 Trial: 34 Epoch: 89, train_loss: 24910.000854492188, validation_loss = 4123.44580078125
 Trial: 34 Epoch: 90, train_loss: 23069.766845703125, validation_loss = 5293.2777099609375
 Trial: 34 Epoch: 91, train_loss: 29426.082885742188, validation_loss = 3379.9881591796875
 Trial: 34 Epoch: 92, train_loss: 27920.241455078125, validation_loss = 3242.01123046875
 Trial: 34 Epoch: 93, train_loss: 23909.823486328125, validation_loss = 3131.2735595703125
 Trial: 34 Epoch: 94, train_loss: 23334.858764648438, validation_loss = 3171.17529296875
 Trial: 34 Epoch: 95, train_loss: 22953.381591796875, validation_loss = 3935.385498046875
 Trial: 34 Epoch: 96, train_loss: 23266.615112304688, validation_loss = 2925.763671875
 Trial: 34 Epoch: 97, train_loss: 21939.049560546875, validation_loss = 3177.9598388671875
 Trial: 34 Epoch: 98, train_loss: 22566.117065429688, validation_loss = 3510.6776123046875
 Trial: 34 Epoch: 99, train_loss: 31338.175415039062, validation_loss = 5389.74560546875
========Trial 35 params: {'n_layers': 2, 'K': 2, 'Dropout': 0.2, 'Hidden0': 16, 'Hidden1': 32, 'optimizer': 'Adam', 'lr': 0.008618602841003523, 'batch': 16}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |    768     |
|    sptH.0.gcn.bias     |     16     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.sAtt.Vs     |    729     |
|     sptH.2.sAtt.bs     |    729     |
|     sptH.2.sAtt.W1     |     16     |
|     sptH.2.sAtt.W2     |     16     |
|     sptH.2.sAtt.W3     |     1      |
|   sptH.2.gcn.weight    |    1024    |
|    sptH.2.gcn.bias     |     32     |
| sptH.2.timeConv.weight |     3      |
|  sptH.2.timeConv.bias  |     1      |
|     sptH.4.weight      |    384     |
|      sptH.4.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    384     |
|    sptD.0.gcn.bias     |     16     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.sAtt.Vs     |    729     |
|     sptD.2.sAtt.bs     |    729     |
|     sptD.2.sAtt.W1     |     16     |
|     sptD.2.sAtt.W2     |     16     |
|     sptD.2.sAtt.W3     |     1      |
|   sptD.2.gcn.weight    |    1024    |
|    sptD.2.gcn.bias     |     32     |
| sptD.2.timeConv.weight |     3      |
|  sptD.2.timeConv.bias  |     1      |
|     sptD.4.weight      |    384     |
|      sptD.4.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |    768     |
|    sptW.0.gcn.bias     |     16     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.sAtt.Vs     |    729     |
|     sptW.2.sAtt.bs     |    729     |
|     sptW.2.sAtt.W1     |     16     |
|     sptW.2.sAtt.W2     |     16     |
|     sptW.2.sAtt.W3     |     1      |
|   sptW.2.gcn.weight    |    1024    |
|    sptW.2.gcn.bias     |     32     |
| sptW.2.timeConv.weight |     3      |
|  sptW.2.timeConv.bias  |     1      |
|     sptW.4.weight      |    384     |
|      sptW.4.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 15354
 Trial: 35 Epoch: 0, train_loss: 167526.26171875, validation_loss = 23770.216796875
========Trial 36 params: {'n_layers': 2, 'K': 2, 'Dropout': 0.2, 'Hidden0': 32, 'Hidden1': 4, 'optimizer': 'Adam', 'lr': 0.005870288768885201, 'batch': 64}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |    1536    |
|    sptH.0.gcn.bias     |     32     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.sAtt.Vs     |    729     |
|     sptH.2.sAtt.bs     |    729     |
|     sptH.2.sAtt.W1     |     32     |
|     sptH.2.sAtt.W2     |     32     |
|     sptH.2.sAtt.W3     |     1      |
|   sptH.2.gcn.weight    |    256     |
|    sptH.2.gcn.bias     |     4      |
| sptH.2.timeConv.weight |     3      |
|  sptH.2.timeConv.bias  |     1      |
|     sptH.4.weight      |     48     |
|      sptH.4.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    768     |
|    sptD.0.gcn.bias     |     32     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.sAtt.Vs     |    729     |
|     sptD.2.sAtt.bs     |    729     |
|     sptD.2.sAtt.W1     |     32     |
|     sptD.2.sAtt.W2     |     32     |
|     sptD.2.sAtt.W3     |     1      |
|   sptD.2.gcn.weight    |    256     |
|    sptD.2.gcn.bias     |     4      |
| sptD.2.timeConv.weight |     3      |
|  sptD.2.timeConv.bias  |     1      |
|     sptD.4.weight      |     48     |
|      sptD.4.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |    1536    |
|    sptW.0.gcn.bias     |     32     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.sAtt.Vs     |    729     |
|     sptW.2.sAtt.bs     |    729     |
|     sptW.2.sAtt.W1     |     32     |
|     sptW.2.sAtt.W2     |     32     |
|     sptW.2.sAtt.W3     |     1      |
|   sptW.2.gcn.weight    |    256     |
|    sptW.2.gcn.bias     |     4      |
| sptW.2.timeConv.weight |     3      |
|  sptW.2.timeConv.bias  |     1      |
|     sptW.4.weight      |     48     |
|      sptW.4.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 14022
 Trial: 36 Epoch: 0, train_loss: 72397.4296875, validation_loss = 47696.5078125
========Trial 37 params: {'n_layers': 2, 'K': 2, 'Dropout': 0.4, 'Hidden0': 8, 'Hidden1': 64, 'optimizer': 'Adam', 'lr': 0.0037394196743705433, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |    384     |
|    sptH.0.gcn.bias     |     8      |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.sAtt.Vs     |    729     |
|     sptH.2.sAtt.bs     |    729     |
|     sptH.2.sAtt.W1     |     8      |
|     sptH.2.sAtt.W2     |     8      |
|     sptH.2.sAtt.W3     |     1      |
|   sptH.2.gcn.weight    |    1024    |
|    sptH.2.gcn.bias     |     64     |
| sptH.2.timeConv.weight |     3      |
|  sptH.2.timeConv.bias  |     1      |
|     sptH.4.weight      |    768     |
|      sptH.4.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    192     |
|    sptD.0.gcn.bias     |     8      |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.sAtt.Vs     |    729     |
|     sptD.2.sAtt.bs     |    729     |
|     sptD.2.sAtt.W1     |     8      |
|     sptD.2.sAtt.W2     |     8      |
|     sptD.2.sAtt.W3     |     1      |
|   sptD.2.gcn.weight    |    1024    |
|    sptD.2.gcn.bias     |     64     |
| sptD.2.timeConv.weight |     3      |
|  sptD.2.timeConv.bias  |     1      |
|     sptD.4.weight      |    768     |
|      sptD.4.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |    384     |
|    sptW.0.gcn.bias     |     8      |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.sAtt.Vs     |    729     |
|     sptW.2.sAtt.bs     |    729     |
|     sptW.2.sAtt.W1     |     8      |
|     sptW.2.sAtt.W2     |     8      |
|     sptW.2.sAtt.W3     |     1      |
|   sptW.2.gcn.weight    |    1024    |
|    sptW.2.gcn.bias     |     64     |
| sptW.2.timeConv.weight |     3      |
|  sptW.2.timeConv.bias  |     1      |
|     sptW.4.weight      |    768     |
|      sptW.4.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 15570
 Trial: 37 Epoch: 0, train_loss: 351130.365234375, validation_loss = 25761.80078125
========Trial 38 params: {'n_layers': 2, 'K': 2, 'Dropout': 0.2, 'Hidden0': 256, 'Hidden1': 64, 'optimizer': 'Adam', 'lr': 0.01846876525228559, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |   12288    |
|    sptH.0.gcn.bias     |    256     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.sAtt.Vs     |    729     |
|     sptH.2.sAtt.bs     |    729     |
|     sptH.2.sAtt.W1     |    256     |
|     sptH.2.sAtt.W2     |    256     |
|     sptH.2.sAtt.W3     |     1      |
|   sptH.2.gcn.weight    |   32768    |
|    sptH.2.gcn.bias     |     64     |
| sptH.2.timeConv.weight |     3      |
|  sptH.2.timeConv.bias  |     1      |
|     sptH.4.weight      |    768     |
|      sptH.4.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    6144    |
|    sptD.0.gcn.bias     |    256     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.sAtt.Vs     |    729     |
|     sptD.2.sAtt.bs     |    729     |
|     sptD.2.sAtt.W1     |    256     |
|     sptD.2.sAtt.W2     |    256     |
|     sptD.2.sAtt.W3     |     1      |
|   sptD.2.gcn.weight    |   32768    |
|    sptD.2.gcn.bias     |     64     |
| sptD.2.timeConv.weight |     3      |
|  sptD.2.timeConv.bias  |     1      |
|     sptD.4.weight      |    768     |
|      sptD.4.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |   12288    |
|    sptW.0.gcn.bias     |    256     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.sAtt.Vs     |    729     |
|     sptW.2.sAtt.bs     |    729     |
|     sptW.2.sAtt.W1     |    256     |
|     sptW.2.sAtt.W2     |    256     |
|     sptW.2.sAtt.W3     |     1      |
|   sptW.2.gcn.weight    |   32768    |
|    sptW.2.gcn.bias     |     64     |
| sptW.2.timeConv.weight |     3      |
|  sptW.2.timeConv.bias  |     1      |
|     sptW.4.weight      |    768     |
|      sptW.4.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 142794
 Trial: 38 Epoch: 0, train_loss: 260549.85888671875, validation_loss = 7611.54150390625
 Trial: 38 Epoch: 1, train_loss: 70619.36328125, validation_loss = 5615.13427734375
 Trial: 38 Epoch: 2, train_loss: 53543.407958984375, validation_loss = 5205.0751953125
 Trial: 38 Epoch: 3, train_loss: 43635.77160644531, validation_loss = 6224.2373046875
 Trial: 38 Epoch: 4, train_loss: 41977.465087890625, validation_loss = 4410.0712890625
 Trial: 38 Epoch: 5, train_loss: 39153.44384765625, validation_loss = 4999.4476318359375
 Trial: 38 Epoch: 6, train_loss: 34601.12683105469, validation_loss = 4468.695068359375
 Trial: 38 Epoch: 7, train_loss: 38472.80847167969, validation_loss = 3657.7720947265625
 Trial: 38 Epoch: 8, train_loss: 36547.38916015625, validation_loss = 6343.50732421875
 Trial: 38 Epoch: 9, train_loss: 38590.84069824219, validation_loss = 8731.94970703125
 Trial: 38 Epoch: 10, train_loss: 39302.51525878906, validation_loss = 7635.96826171875
 Trial: 38 Epoch: 11, train_loss: 39865.80310058594, validation_loss = 4541.639404296875
 Trial: 38 Epoch: 12, train_loss: 43367.88317871094, validation_loss = 3180.3990478515625
 Trial: 38 Epoch: 13, train_loss: 33906.614501953125, validation_loss = 6995.45654296875
 Trial: 38 Epoch: 14, train_loss: 35822.49694824219, validation_loss = 4177.64208984375
 Trial: 38 Epoch: 15, train_loss: 31523.910400390625, validation_loss = 3902.47412109375
 Trial: 38 Epoch: 16, train_loss: 35828.151611328125, validation_loss = 3626.3173828125
 Trial: 38 Epoch: 17, train_loss: 33518.00793457031, validation_loss = 3476.977294921875
 Trial: 38 Epoch: 18, train_loss: 31373.793212890625, validation_loss = 3933.4686279296875
 Trial: 38 Epoch: 19, train_loss: 31194.492553710938, validation_loss = 3265.193115234375
 Trial: 38 Epoch: 20, train_loss: 30559.900390625, validation_loss = 4992.022216796875
 Trial: 38 Epoch: 21, train_loss: 31796.639404296875, validation_loss = 4144.2255859375
 Trial: 38 Epoch: 22, train_loss: 32229.772827148438, validation_loss = 4374.563720703125
 Trial: 38 Epoch: 23, train_loss: 28047.939086914062, validation_loss = 4258.1068115234375
 Trial: 38 Epoch: 24, train_loss: 28438.276733398438, validation_loss = 3572.6798095703125
 Trial: 38 Epoch: 25, train_loss: 29727.502685546875, validation_loss = 3727.247314453125
 Trial: 38 Epoch: 26, train_loss: 29003.148315429688, validation_loss = 4576.29296875
 Trial: 38 Epoch: 27, train_loss: 25952.7880859375, validation_loss = 3645.1181640625
 Trial: 38 Epoch: 28, train_loss: 25966.335327148438, validation_loss = 4114.287109375
 Trial: 38 Epoch: 29, train_loss: 30374.480346679688, validation_loss = 3648.2357177734375
 Trial: 38 Epoch: 30, train_loss: 27761.225708007812, validation_loss = 3446.2103271484375
 Trial: 38 Epoch: 31, train_loss: 29039.240966796875, validation_loss = 4183.0899658203125
 Trial: 38 Epoch: 32, train_loss: 27427.134643554688, validation_loss = 6223.482666015625
 Trial: 38 Epoch: 33, train_loss: 41180.2841796875, validation_loss = 4795.633544921875
 Trial: 38 Epoch: 34, train_loss: 39672.65002441406, validation_loss = 4491.8167724609375
 Trial: 38 Epoch: 35, train_loss: 46578.98059082031, validation_loss = 6293.968994140625
 Trial: 38 Epoch: 36, train_loss: 33565.764892578125, validation_loss = 7516.64111328125
 Trial: 38 Epoch: 37, train_loss: 33726.375732421875, validation_loss = 3571.90234375
 Trial: 38 Epoch: 38, train_loss: 33497.96569824219, validation_loss = 5154.62646484375
 Trial: 38 Epoch: 39, train_loss: 28312.888549804688, validation_loss = 3716.782470703125
 Trial: 38 Epoch: 40, train_loss: 32729.732666015625, validation_loss = 4448.5338134765625
 Trial: 38 Epoch: 41, train_loss: 33304.33459472656, validation_loss = 4268.50439453125
 Trial: 38 Epoch: 42, train_loss: 28827.737915039062, validation_loss = 3833.38330078125
 Trial: 38 Epoch: 43, train_loss: 28052.035400390625, validation_loss = 4509.073974609375
 Trial: 38 Epoch: 44, train_loss: 32053.858520507812, validation_loss = 3941.3118896484375
 Trial: 38 Epoch: 45, train_loss: 29982.2216796875, validation_loss = 4189.105224609375
 Trial: 38 Epoch: 46, train_loss: 27046.522583007812, validation_loss = 4065.0003662109375
 Trial: 38 Epoch: 47, train_loss: 28187.541259765625, validation_loss = 3656.4818115234375
 Trial: 38 Epoch: 48, train_loss: 31835.593872070312, validation_loss = 4242.8800048828125
 Trial: 38 Epoch: 49, train_loss: 29788.808471679688, validation_loss = 3609.5106201171875
 Trial: 38 Epoch: 50, train_loss: 28211.664306640625, validation_loss = 6268.7392578125
 Trial: 38 Epoch: 51, train_loss: 27478.427490234375, validation_loss = 4007.53173828125
 Trial: 38 Epoch: 52, train_loss: 27403.044555664062, validation_loss = 4212.958984375
 Trial: 38 Epoch: 53, train_loss: 27815.52587890625, validation_loss = 3061.479736328125
 Trial: 38 Epoch: 54, train_loss: 29587.755004882812, validation_loss = 4604.5379638671875
 Trial: 38 Epoch: 55, train_loss: 32386.582153320312, validation_loss = 4378.649658203125
 Trial: 38 Epoch: 56, train_loss: 27873.405517578125, validation_loss = 3431.712646484375
 Trial: 38 Epoch: 57, train_loss: 28743.57568359375, validation_loss = 4057.9969482421875
 Trial: 38 Epoch: 58, train_loss: 27808.799926757812, validation_loss = 3627.9571533203125
 Trial: 38 Epoch: 59, train_loss: 32161.6982421875, validation_loss = 4181.56396484375
 Trial: 38 Epoch: 60, train_loss: 27119.18243408203, validation_loss = 3633.511962890625
 Trial: 38 Epoch: 61, train_loss: 25818.22314453125, validation_loss = 3581.524658203125
 Trial: 38 Epoch: 62, train_loss: 26398.464965820312, validation_loss = 4203.67138671875
 Trial: 38 Epoch: 63, train_loss: 27456.41845703125, validation_loss = 3852.8624267578125
 Trial: 38 Epoch: 64, train_loss: 26445.727905273438, validation_loss = 3796.8607177734375
 Trial: 38 Epoch: 65, train_loss: 27702.595947265625, validation_loss = 3575.2705078125
 Trial: 38 Epoch: 66, train_loss: 26154.163696289062, validation_loss = 3249.60205078125
 Trial: 38 Epoch: 67, train_loss: 26010.7333984375, validation_loss = 4372.5987548828125
 Trial: 38 Epoch: 68, train_loss: 26192.004028320312, validation_loss = 5005.0048828125
 Trial: 38 Epoch: 69, train_loss: 29470.24462890625, validation_loss = 3629.7117919921875
 Trial: 38 Epoch: 70, train_loss: 32499.266723632812, validation_loss = 3454.4267578125
 Trial: 38 Epoch: 71, train_loss: 28697.760498046875, validation_loss = 5898.01123046875
 Trial: 38 Epoch: 72, train_loss: 29074.728271484375, validation_loss = 3263.911376953125
 Trial: 38 Epoch: 73, train_loss: 27333.220458984375, validation_loss = 5076.49658203125
 Trial: 38 Epoch: 74, train_loss: 28225.506713867188, validation_loss = 4245.3682861328125
 Trial: 38 Epoch: 75, train_loss: 25805.00341796875, validation_loss = 3410.49072265625
 Trial: 38 Epoch: 76, train_loss: 33478.317626953125, validation_loss = 4973.016357421875
 Trial: 38 Epoch: 77, train_loss: 31680.83203125, validation_loss = 8153.871337890625
 Trial: 38 Epoch: 78, train_loss: 36449.181396484375, validation_loss = 3628.6640625
 Trial: 38 Epoch: 79, train_loss: 30415.454467773438, validation_loss = 3992.0018310546875
 Trial: 38 Epoch: 80, train_loss: 32367.271484375, validation_loss = 3600.0361328125
 Trial: 38 Epoch: 81, train_loss: 32006.63525390625, validation_loss = 3526.7291259765625
 Trial: 38 Epoch: 82, train_loss: 27337.808837890625, validation_loss = 4973.121337890625
 Trial: 38 Epoch: 83, train_loss: 25750.520385742188, validation_loss = 3235.4976806640625
 Trial: 38 Epoch: 84, train_loss: 27698.422729492188, validation_loss = 4888.400390625
 Trial: 38 Epoch: 85, train_loss: 25935.574096679688, validation_loss = 4075.8907470703125
 Trial: 38 Epoch: 86, train_loss: 26755.143188476562, validation_loss = 4169.1260986328125
 Trial: 38 Epoch: 87, train_loss: 32856.407470703125, validation_loss = 3189.401611328125
 Trial: 38 Epoch: 88, train_loss: 29206.727905273438, validation_loss = 5967.0810546875
 Trial: 38 Epoch: 89, train_loss: 28623.719604492188, validation_loss = 5062.013427734375
 Trial: 38 Epoch: 90, train_loss: 24348.976684570312, validation_loss = 6046.466064453125
 Trial: 38 Epoch: 91, train_loss: 30746.971801757812, validation_loss = 3627.8616943359375
 Trial: 38 Epoch: 92, train_loss: 29935.573364257812, validation_loss = 3625.177490234375
 Trial: 38 Epoch: 93, train_loss: 27430.4248046875, validation_loss = 3592.6046142578125
 Trial: 38 Epoch: 94, train_loss: 24832.385375976562, validation_loss = 3262.5355224609375
 Trial: 38 Epoch: 95, train_loss: 24783.895385742188, validation_loss = 4106.267333984375
 Trial: 38 Epoch: 96, train_loss: 24542.50048828125, validation_loss = 3323.1815185546875
 Trial: 38 Epoch: 97, train_loss: 24172.139892578125, validation_loss = 3440.6798095703125
 Trial: 38 Epoch: 98, train_loss: 24038.80517578125, validation_loss = 3921.5565185546875
 Trial: 38 Epoch: 99, train_loss: 25089.147338867188, validation_loss = 3085.751220703125
========Trial 39 params: {'n_layers': 1, 'K': 4, 'Dropout': 0.5, 'Hidden0': 256, 'optimizer': 'Adam', 'lr': 0.021965396519307904, 'batch': 64}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |   24576    |
|    sptH.0.gcn.bias     |    256     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |    3072    |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |   12288    |
|    sptD.0.gcn.bias     |    256     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |    3072    |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |   24576    |
|    sptW.0.gcn.bias     |    256     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |    3072    |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 76005
 Trial: 39 Epoch: 0, train_loss: 69016.75, validation_loss = 135220.5625
========Trial 40 params: {'n_layers': 2, 'K': 2, 'Dropout': 0.2, 'Hidden0': 256, 'Hidden1': 8, 'optimizer': 'Adam', 'lr': 0.007325058153246312, 'batch': 8}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |   12288    |
|    sptH.0.gcn.bias     |    256     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.sAtt.Vs     |    729     |
|     sptH.2.sAtt.bs     |    729     |
|     sptH.2.sAtt.W1     |    256     |
|     sptH.2.sAtt.W2     |    256     |
|     sptH.2.sAtt.W3     |     1      |
|   sptH.2.gcn.weight    |    4096    |
|    sptH.2.gcn.bias     |     8      |
| sptH.2.timeConv.weight |     3      |
|  sptH.2.timeConv.bias  |     1      |
|     sptH.4.weight      |     96     |
|      sptH.4.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    6144    |
|    sptD.0.gcn.bias     |    256     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.sAtt.Vs     |    729     |
|     sptD.2.sAtt.bs     |    729     |
|     sptD.2.sAtt.W1     |    256     |
|     sptD.2.sAtt.W2     |    256     |
|     sptD.2.sAtt.W3     |     1      |
|   sptD.2.gcn.weight    |    4096    |
|    sptD.2.gcn.bias     |     8      |
| sptD.2.timeConv.weight |     3      |
|  sptD.2.timeConv.bias  |     1      |
|     sptD.4.weight      |     96     |
|      sptD.4.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |   12288    |
|    sptW.0.gcn.bias     |    256     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.sAtt.Vs     |    729     |
|     sptW.2.sAtt.bs     |    729     |
|     sptW.2.sAtt.W1     |    256     |
|     sptW.2.sAtt.W2     |    256     |
|     sptW.2.sAtt.W3     |     1      |
|   sptW.2.gcn.weight    |    4096    |
|    sptW.2.gcn.bias     |     8      |
| sptW.2.timeConv.weight |     3      |
|  sptW.2.timeConv.bias  |     1      |
|     sptW.4.weight      |     96     |
|      sptW.4.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 54594
 Trial: 40 Epoch: 0, train_loss: 409386.97265625, validation_loss = 34726.609375
========Trial 41 params: {'n_layers': 2, 'K': 2, 'Dropout': 0.2, 'Hidden0': 256, 'Hidden1': 64, 'optimizer': 'Adam', 'lr': 0.013308880254941218, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |   12288    |
|    sptH.0.gcn.bias     |    256     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.sAtt.Vs     |    729     |
|     sptH.2.sAtt.bs     |    729     |
|     sptH.2.sAtt.W1     |    256     |
|     sptH.2.sAtt.W2     |    256     |
|     sptH.2.sAtt.W3     |     1      |
|   sptH.2.gcn.weight    |   32768    |
|    sptH.2.gcn.bias     |     64     |
| sptH.2.timeConv.weight |     3      |
|  sptH.2.timeConv.bias  |     1      |
|     sptH.4.weight      |    768     |
|      sptH.4.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    6144    |
|    sptD.0.gcn.bias     |    256     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.sAtt.Vs     |    729     |
|     sptD.2.sAtt.bs     |    729     |
|     sptD.2.sAtt.W1     |    256     |
|     sptD.2.sAtt.W2     |    256     |
|     sptD.2.sAtt.W3     |     1      |
|   sptD.2.gcn.weight    |   32768    |
|    sptD.2.gcn.bias     |     64     |
| sptD.2.timeConv.weight |     3      |
|  sptD.2.timeConv.bias  |     1      |
|     sptD.4.weight      |    768     |
|      sptD.4.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |   12288    |
|    sptW.0.gcn.bias     |    256     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.sAtt.Vs     |    729     |
|     sptW.2.sAtt.bs     |    729     |
|     sptW.2.sAtt.W1     |    256     |
|     sptW.2.sAtt.W2     |    256     |
|     sptW.2.sAtt.W3     |     1      |
|   sptW.2.gcn.weight    |   32768    |
|    sptW.2.gcn.bias     |     64     |
| sptW.2.timeConv.weight |     3      |
|  sptW.2.timeConv.bias  |     1      |
|     sptW.4.weight      |    768     |
|      sptW.4.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 142794
 Trial: 41 Epoch: 0, train_loss: 157629.05126953125, validation_loss = 7528.5849609375
 Trial: 41 Epoch: 1, train_loss: 61464.05615234375, validation_loss = 5124.855712890625
 Trial: 41 Epoch: 2, train_loss: 49396.7939453125, validation_loss = 4808.039794921875
 Trial: 41 Epoch: 3, train_loss: 38986.79187011719, validation_loss = 7316.826171875
 Trial: 41 Epoch: 4, train_loss: 38923.00939941406, validation_loss = 5196.24072265625
 Trial: 41 Epoch: 5, train_loss: 38244.22570800781, validation_loss = 4610.4095458984375
 Trial: 41 Epoch: 6, train_loss: 33504.38037109375, validation_loss = 5082.1671142578125
 Trial: 41 Epoch: 7, train_loss: 37226.404052734375, validation_loss = 3986.518310546875
 Trial: 41 Epoch: 8, train_loss: 34434.85681152344, validation_loss = 6633.5010986328125
 Trial: 41 Epoch: 9, train_loss: 35173.26477050781, validation_loss = 4135.615966796875
 Trial: 41 Epoch: 10, train_loss: 40108.276611328125, validation_loss = 7074.341552734375
 Trial: 41 Epoch: 11, train_loss: 34207.951171875, validation_loss = 4914.504638671875
 Trial: 41 Epoch: 12, train_loss: 33274.68957519531, validation_loss = 4501.4310302734375
 Trial: 41 Epoch: 13, train_loss: 30971.29736328125, validation_loss = 4539.877197265625
 Trial: 41 Epoch: 14, train_loss: 31407.644409179688, validation_loss = 4572.371826171875
 Trial: 41 Epoch: 15, train_loss: 34261.26696777344, validation_loss = 5264.106689453125
 Trial: 41 Epoch: 16, train_loss: 42169.7138671875, validation_loss = 3658.0089111328125
 Trial: 41 Epoch: 17, train_loss: 30656.477416992188, validation_loss = 4938.7021484375
 Trial: 41 Epoch: 18, train_loss: 32057.639526367188, validation_loss = 4501.748291015625
 Trial: 41 Epoch: 19, train_loss: 33390.46435546875, validation_loss = 3212.2291259765625
 Trial: 41 Epoch: 20, train_loss: 33254.21862792969, validation_loss = 4905.063720703125
 Trial: 41 Epoch: 21, train_loss: 40138.052978515625, validation_loss = 6353.549560546875
 Trial: 41 Epoch: 22, train_loss: 38378.70397949219, validation_loss = 4207.85693359375
 Trial: 41 Epoch: 23, train_loss: 28124.653442382812, validation_loss = 4724.822265625
 Trial: 41 Epoch: 24, train_loss: 27526.144653320312, validation_loss = 3590.380615234375
 Trial: 41 Epoch: 25, train_loss: 28236.46142578125, validation_loss = 3706.208740234375
 Trial: 41 Epoch: 26, train_loss: 29047.481201171875, validation_loss = 5237.581298828125
 Trial: 41 Epoch: 27, train_loss: 25415.202026367188, validation_loss = 3328.042236328125
 Trial: 41 Epoch: 28, train_loss: 24985.134155273438, validation_loss = 3894.2647705078125
 Trial: 41 Epoch: 29, train_loss: 27543.794189453125, validation_loss = 4288.9503173828125
 Trial: 41 Epoch: 30, train_loss: 26622.777709960938, validation_loss = 3382.947021484375
 Trial: 41 Epoch: 31, train_loss: 27008.95703125, validation_loss = 3608.493896484375
 Trial: 41 Epoch: 32, train_loss: 24958.681762695312, validation_loss = 4044.38525390625
 Trial: 41 Epoch: 33, train_loss: 26362.952880859375, validation_loss = 5331.0556640625
 Trial: 41 Epoch: 34, train_loss: 31779.666015625, validation_loss = 4200.341064453125
 Trial: 41 Epoch: 35, train_loss: 32098.136962890625, validation_loss = 4104.684326171875
 Trial: 41 Epoch: 36, train_loss: 26200.993774414062, validation_loss = 5645.19287109375
 Trial: 41 Epoch: 37, train_loss: 28178.44677734375, validation_loss = 3188.1812744140625
 Trial: 41 Epoch: 38, train_loss: 28561.390502929688, validation_loss = 4127.0345458984375
 Trial: 41 Epoch: 39, train_loss: 25897.64306640625, validation_loss = 3948.0020751953125
 Trial: 41 Epoch: 40, train_loss: 26717.6591796875, validation_loss = 3628.3636474609375
 Trial: 41 Epoch: 41, train_loss: 26609.652587890625, validation_loss = 4681.7005615234375
 Trial: 41 Epoch: 42, train_loss: 26549.984497070312, validation_loss = 3223.293701171875
 Trial: 41 Epoch: 43, train_loss: 25540.689575195312, validation_loss = 4675.6199951171875
 Trial: 41 Epoch: 44, train_loss: 26729.767822265625, validation_loss = 3381.7972412109375
 Trial: 41 Epoch: 45, train_loss: 25385.12109375, validation_loss = 3705.3905029296875
 Trial: 41 Epoch: 46, train_loss: 24579.401733398438, validation_loss = 3387.5657958984375
 Trial: 41 Epoch: 47, train_loss: 26259.7216796875, validation_loss = 3195.3817138671875
 Trial: 41 Epoch: 48, train_loss: 26878.056396484375, validation_loss = 3883.5489501953125
 Trial: 41 Epoch: 49, train_loss: 24941.597778320312, validation_loss = 3954.05517578125
 Trial: 41 Epoch: 50, train_loss: 23421.233520507812, validation_loss = 4237.5986328125
 Trial: 41 Epoch: 51, train_loss: 23455.152587890625, validation_loss = 3637.2750244140625
 Trial: 41 Epoch: 52, train_loss: 24593.988037109375, validation_loss = 3594.9442138671875
 Trial: 41 Epoch: 53, train_loss: 23622.166259765625, validation_loss = 3044.1085205078125
 Trial: 41 Epoch: 54, train_loss: 23866.658325195312, validation_loss = 3428.426513671875
 Trial: 41 Epoch: 55, train_loss: 24079.759643554688, validation_loss = 3461.4580078125
 Trial: 41 Epoch: 56, train_loss: 25104.014404296875, validation_loss = 3119.4754638671875
 Trial: 41 Epoch: 57, train_loss: 23711.810424804688, validation_loss = 3665.826171875
 Trial: 41 Epoch: 58, train_loss: 23419.998046875, validation_loss = 4124.76708984375
 Trial: 41 Epoch: 59, train_loss: 30200.337646484375, validation_loss = 4860.767578125
 Trial: 41 Epoch: 60, train_loss: 24996.49969482422, validation_loss = 2860.209716796875
 Trial: 41 Epoch: 61, train_loss: 26213.30029296875, validation_loss = 5178.677734375
 Trial: 41 Epoch: 62, train_loss: 23144.359741210938, validation_loss = 3318.2796630859375
 Trial: 41 Epoch: 63, train_loss: 23240.228393554688, validation_loss = 3819.853759765625
 Trial: 41 Epoch: 64, train_loss: 25160.669189453125, validation_loss = 3775.040771484375
 Trial: 41 Epoch: 65, train_loss: 23932.40673828125, validation_loss = 3627.32763671875
 Trial: 41 Epoch: 66, train_loss: 25309.239379882812, validation_loss = 2939.5867919921875
 Trial: 41 Epoch: 67, train_loss: 24608.284545898438, validation_loss = 3894.065673828125
 Trial: 41 Epoch: 68, train_loss: 24132.508056640625, validation_loss = 4345.28564453125
 Trial: 41 Epoch: 69, train_loss: 24687.82177734375, validation_loss = 3861.3350830078125
 Trial: 41 Epoch: 70, train_loss: 25065.315795898438, validation_loss = 2718.739501953125
 Trial: 41 Epoch: 71, train_loss: 23556.036010742188, validation_loss = 4909.857421875
 Trial: 41 Epoch: 72, train_loss: 23530.53662109375, validation_loss = 2934.3294677734375
 Trial: 41 Epoch: 73, train_loss: 23541.714599609375, validation_loss = 4273.2420654296875
 Trial: 41 Epoch: 74, train_loss: 23974.898071289062, validation_loss = 3995.456787109375
 Trial: 41 Epoch: 75, train_loss: 22785.328735351562, validation_loss = 3080.8509521484375
 Trial: 41 Epoch: 76, train_loss: 27587.822387695312, validation_loss = 4127.871826171875
 Trial: 41 Epoch: 77, train_loss: 24656.663818359375, validation_loss = 3784.3189697265625
 Trial: 41 Epoch: 78, train_loss: 30811.096801757812, validation_loss = 4142.4998779296875
 Trial: 41 Epoch: 79, train_loss: 26920.946044921875, validation_loss = 4092.3126220703125
 Trial: 41 Epoch: 80, train_loss: 25116.643310546875, validation_loss = 3185.9400634765625
 Trial: 41 Epoch: 81, train_loss: 24225.603759765625, validation_loss = 3087.810791015625
 Trial: 41 Epoch: 82, train_loss: 22233.920166015625, validation_loss = 3556.5042724609375
 Trial: 41 Epoch: 83, train_loss: 22781.99969482422, validation_loss = 3050.7696533203125
 Trial: 41 Epoch: 84, train_loss: 24896.710327148438, validation_loss = 3740.7257080078125
 Trial: 41 Epoch: 85, train_loss: 23733.636962890625, validation_loss = 3572.668212890625
 Trial: 41 Epoch: 86, train_loss: 21892.7568359375, validation_loss = 3670.654296875
 Trial: 41 Epoch: 87, train_loss: 26172.335205078125, validation_loss = 3758.9962158203125
 Trial: 41 Epoch: 88, train_loss: 23656.832397460938, validation_loss = 3901.8917236328125
 Trial: 41 Epoch: 89, train_loss: 22395.11279296875, validation_loss = 3405.6895751953125
 Trial: 41 Epoch: 90, train_loss: 21433.09735107422, validation_loss = 4332.0400390625
 Trial: 41 Epoch: 91, train_loss: 25221.548217773438, validation_loss = 3206.182861328125
 Trial: 41 Epoch: 92, train_loss: 23464.0234375, validation_loss = 3181.577880859375
 Trial: 41 Epoch: 93, train_loss: 23035.487060546875, validation_loss = 3203.482421875
 Trial: 41 Epoch: 94, train_loss: 21924.93017578125, validation_loss = 2912.553955078125
 Trial: 41 Epoch: 95, train_loss: 21684.546142578125, validation_loss = 3937.420654296875
 Trial: 41 Epoch: 96, train_loss: 22215.265747070312, validation_loss = 2928.04638671875
 Trial: 41 Epoch: 97, train_loss: 20875.615356445312, validation_loss = 3073.3924560546875
 Trial: 41 Epoch: 98, train_loss: 20591.242553710938, validation_loss = 3587.285888671875
 Trial: 41 Epoch: 99, train_loss: 21041.914306640625, validation_loss = 2792.49072265625
========Trial 42 params: {'n_layers': 2, 'K': 2, 'Dropout': 0.2, 'Hidden0': 256, 'Hidden1': 64, 'optimizer': 'Adam', 'lr': 0.019496842303312348, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |   12288    |
|    sptH.0.gcn.bias     |    256     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.sAtt.Vs     |    729     |
|     sptH.2.sAtt.bs     |    729     |
|     sptH.2.sAtt.W1     |    256     |
|     sptH.2.sAtt.W2     |    256     |
|     sptH.2.sAtt.W3     |     1      |
|   sptH.2.gcn.weight    |   32768    |
|    sptH.2.gcn.bias     |     64     |
| sptH.2.timeConv.weight |     3      |
|  sptH.2.timeConv.bias  |     1      |
|     sptH.4.weight      |    768     |
|      sptH.4.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    6144    |
|    sptD.0.gcn.bias     |    256     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.sAtt.Vs     |    729     |
|     sptD.2.sAtt.bs     |    729     |
|     sptD.2.sAtt.W1     |    256     |
|     sptD.2.sAtt.W2     |    256     |
|     sptD.2.sAtt.W3     |     1      |
|   sptD.2.gcn.weight    |   32768    |
|    sptD.2.gcn.bias     |     64     |
| sptD.2.timeConv.weight |     3      |
|  sptD.2.timeConv.bias  |     1      |
|     sptD.4.weight      |    768     |
|      sptD.4.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |   12288    |
|    sptW.0.gcn.bias     |    256     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.sAtt.Vs     |    729     |
|     sptW.2.sAtt.bs     |    729     |
|     sptW.2.sAtt.W1     |    256     |
|     sptW.2.sAtt.W2     |    256     |
|     sptW.2.sAtt.W3     |     1      |
|   sptW.2.gcn.weight    |   32768    |
|    sptW.2.gcn.bias     |     64     |
| sptW.2.timeConv.weight |     3      |
|  sptW.2.timeConv.bias  |     1      |
|     sptW.4.weight      |    768     |
|      sptW.4.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 142794
 Trial: 42 Epoch: 0, train_loss: 318574.9296875, validation_loss = 7166.7978515625
 Trial: 42 Epoch: 1, train_loss: 72174.98217773438, validation_loss = 7636.966064453125
 Trial: 42 Epoch: 2, train_loss: 55761.88525390625, validation_loss = 5574.939453125
 Trial: 42 Epoch: 3, train_loss: 45849.06872558594, validation_loss = 8373.17138671875
 Trial: 42 Epoch: 4, train_loss: 45917.9736328125, validation_loss = 4799.106689453125
 Trial: 42 Epoch: 5, train_loss: 41726.69079589844, validation_loss = 6123.2476806640625
 Trial: 42 Epoch: 6, train_loss: 39179.74523925781, validation_loss = 4585.214111328125
 Trial: 42 Epoch: 7, train_loss: 46534.95324707031, validation_loss = 7020.244140625
 Trial: 42 Epoch: 8, train_loss: 37629.56896972656, validation_loss = 4279.498291015625
 Trial: 42 Epoch: 9, train_loss: 36778.362060546875, validation_loss = 7399.3291015625
 Trial: 42 Epoch: 10, train_loss: 33726.78076171875, validation_loss = 4670.5701904296875
 Trial: 42 Epoch: 11, train_loss: 32882.45947265625, validation_loss = 4074.9267578125
 Trial: 42 Epoch: 12, train_loss: 36611.1337890625, validation_loss = 3847.9874267578125
 Trial: 42 Epoch: 13, train_loss: 34367.78369140625, validation_loss = 6125.81005859375
 Trial: 42 Epoch: 14, train_loss: 37027.64196777344, validation_loss = 3693.5548095703125
 Trial: 42 Epoch: 15, train_loss: 32353.339233398438, validation_loss = 3704.0428466796875
 Trial: 42 Epoch: 16, train_loss: 33725.8994140625, validation_loss = 3629.399658203125
 Trial: 42 Epoch: 17, train_loss: 32387.598876953125, validation_loss = 3755.3543701171875
 Trial: 42 Epoch: 18, train_loss: 31214.666748046875, validation_loss = 4348.761962890625
 Trial: 42 Epoch: 19, train_loss: 31722.076293945312, validation_loss = 3294.2598876953125
 Trial: 42 Epoch: 20, train_loss: 30996.839721679688, validation_loss = 4743.945068359375
 Trial: 42 Epoch: 21, train_loss: 34712.11181640625, validation_loss = 5158.52392578125
 Trial: 42 Epoch: 22, train_loss: 39109.408935546875, validation_loss = 7473.6279296875
 Trial: 42 Epoch: 23, train_loss: 33176.188232421875, validation_loss = 4403.0980224609375
 Trial: 42 Epoch: 24, train_loss: 29851.50927734375, validation_loss = 3803.6370849609375
 Trial: 42 Epoch: 25, train_loss: 28880.200073242188, validation_loss = 3827.761474609375
 Trial: 42 Epoch: 26, train_loss: 29267.155029296875, validation_loss = 4199.2271728515625
 Trial: 42 Epoch: 27, train_loss: 26408.891967773438, validation_loss = 3425.7635498046875
 Trial: 42 Epoch: 28, train_loss: 26332.705810546875, validation_loss = 4156.9171142578125
 Trial: 42 Epoch: 29, train_loss: 30817.85546875, validation_loss = 4130.5810546875
 Trial: 42 Epoch: 30, train_loss: 28608.9580078125, validation_loss = 3613.0970458984375
 Trial: 42 Epoch: 31, train_loss: 30498.065795898438, validation_loss = 3617.8017578125
 Trial: 42 Epoch: 32, train_loss: 26608.680908203125, validation_loss = 4237.1192626953125
 Trial: 42 Epoch: 33, train_loss: 258111.07165527344, validation_loss = 18354.212890625
 Trial: 42 Epoch: 34, train_loss: 118057.01342773438, validation_loss = 5332.999267578125
 Trial: 42 Epoch: 35, train_loss: 50019.449462890625, validation_loss = 5625.6220703125
 Trial: 42 Epoch: 36, train_loss: 38977.09375, validation_loss = 6953.898681640625
 Trial: 42 Epoch: 37, train_loss: 35424.23876953125, validation_loss = 4450.9725341796875
 Trial: 42 Epoch: 38, train_loss: 30830.920654296875, validation_loss = 4213.6094970703125
 Trial: 42 Epoch: 39, train_loss: 30464.746826171875, validation_loss = 3867.3419189453125
 Trial: 42 Epoch: 40, train_loss: 34531.29089355469, validation_loss = 3878.4827880859375
 Trial: 42 Epoch: 41, train_loss: 35443.362060546875, validation_loss = 5451.1279296875
 Trial: 42 Epoch: 42, train_loss: 30944.799926757812, validation_loss = 3755.407958984375
 Trial: 42 Epoch: 43, train_loss: 30706.742919921875, validation_loss = 4754.063720703125
 Trial: 42 Epoch: 44, train_loss: 30837.51611328125, validation_loss = 5069.16015625
 Trial: 42 Epoch: 45, train_loss: 28839.3193359375, validation_loss = 4748.741943359375
 Trial: 42 Epoch: 46, train_loss: 27394.705810546875, validation_loss = 4639.899169921875
 Trial: 42 Epoch: 47, train_loss: 31235.140258789062, validation_loss = 3782.4716796875
 Trial: 42 Epoch: 48, train_loss: 31579.223510742188, validation_loss = 3948.041259765625
 Trial: 42 Epoch: 49, train_loss: 31876.206787109375, validation_loss = 4650.03125
 Trial: 42 Epoch: 50, train_loss: 27585.823364257812, validation_loss = 5693.470947265625
 Trial: 42 Epoch: 51, train_loss: 28020.278198242188, validation_loss = 5327.388916015625
 Trial: 42 Epoch: 52, train_loss: 30372.668334960938, validation_loss = 5596.992919921875
 Trial: 42 Epoch: 53, train_loss: 39109.47619628906, validation_loss = 6088.9560546875
 Trial: 42 Epoch: 54, train_loss: 39204.311767578125, validation_loss = 6238.642333984375
 Trial: 42 Epoch: 55, train_loss: 38617.58544921875, validation_loss = 4465.081298828125
 Trial: 42 Epoch: 56, train_loss: 38939.1162109375, validation_loss = 4064.53125
 Trial: 42 Epoch: 57, train_loss: 35672.94982910156, validation_loss = 5534.463623046875
 Trial: 42 Epoch: 58, train_loss: 31472.628051757812, validation_loss = 4598.961181640625
 Trial: 42 Epoch: 59, train_loss: 34095.52685546875, validation_loss = 4534.6917724609375
 Trial: 42 Epoch: 60, train_loss: 31191.731323242188, validation_loss = 3705.7550048828125
 Trial: 42 Epoch: 61, train_loss: 30046.43115234375, validation_loss = 4954.089111328125
 Trial: 42 Epoch: 62, train_loss: 30757.100463867188, validation_loss = 4352.35546875
 Trial: 42 Epoch: 63, train_loss: 30659.563598632812, validation_loss = 4945.197265625
 Trial: 42 Epoch: 64, train_loss: 29256.579467773438, validation_loss = 5094.583740234375
 Trial: 42 Epoch: 65, train_loss: 29501.422241210938, validation_loss = 4231.1455078125
 Trial: 42 Epoch: 66, train_loss: 29261.0546875, validation_loss = 3516.5848388671875
 Trial: 42 Epoch: 67, train_loss: 30261.9521484375, validation_loss = 4902.229736328125
 Trial: 42 Epoch: 68, train_loss: 28324.753173828125, validation_loss = 4690.25146484375
 Trial: 42 Epoch: 69, train_loss: 33598.23913574219, validation_loss = 5080.6484375
 Trial: 42 Epoch: 70, train_loss: 39937.40441894531, validation_loss = 4938.967529296875
 Trial: 42 Epoch: 71, train_loss: 35271.87927246094, validation_loss = 5029.0400390625
 Trial: 42 Epoch: 72, train_loss: 28290.52294921875, validation_loss = 4497.320068359375
 Trial: 42 Epoch: 73, train_loss: 28370.824340820312, validation_loss = 5084.733642578125
 Trial: 42 Epoch: 74, train_loss: 28574.45263671875, validation_loss = 4885.947509765625
 Trial: 42 Epoch: 75, train_loss: 29004.685302734375, validation_loss = 3730.1622314453125
 Trial: 42 Epoch: 76, train_loss: 34043.68103027344, validation_loss = 4478.3970947265625
 Trial: 42 Epoch: 77, train_loss: 32856.7919921875, validation_loss = 5428.63330078125
 Trial: 42 Epoch: 78, train_loss: 36425.292236328125, validation_loss = 4599.548583984375
 Trial: 42 Epoch: 79, train_loss: 35422.700927734375, validation_loss = 4708.0693359375
 Trial: 42 Epoch: 80, train_loss: 36918.05358886719, validation_loss = 4637.589599609375
 Trial: 42 Epoch: 81, train_loss: 37276.994873046875, validation_loss = 3663.646240234375
 Trial: 42 Epoch: 82, train_loss: 32447.31591796875, validation_loss = 4989.938232421875
 Trial: 42 Epoch: 83, train_loss: 30207.544189453125, validation_loss = 4194.91259765625
 Trial: 42 Epoch: 84, train_loss: 29716.18798828125, validation_loss = 6248.40234375
 Trial: 42 Epoch: 85, train_loss: 30645.804931640625, validation_loss = 4931.5947265625
 Trial: 42 Epoch: 86, train_loss: 29310.18017578125, validation_loss = 4310.3377685546875
 Trial: 42 Epoch: 87, train_loss: 33290.72619628906, validation_loss = 4166.830810546875
 Trial: 42 Epoch: 88, train_loss: 30732.654663085938, validation_loss = 4107.0654296875
 Trial: 42 Epoch: 89, train_loss: 30987.591674804688, validation_loss = 5147.5810546875
 Trial: 42 Epoch: 90, train_loss: 28817.5625, validation_loss = 6620.303466796875
 Trial: 42 Epoch: 91, train_loss: 34506.384521484375, validation_loss = 5811.330810546875
 Trial: 42 Epoch: 92, train_loss: 32345.432373046875, validation_loss = 4668.4951171875
 Trial: 42 Epoch: 93, train_loss: 32033.722290039062, validation_loss = 3817.2474365234375
 Trial: 42 Epoch: 94, train_loss: 31716.664306640625, validation_loss = 3864.035888671875
 Trial: 42 Epoch: 95, train_loss: 30042.466552734375, validation_loss = 4608.372802734375
 Trial: 42 Epoch: 96, train_loss: 29605.836181640625, validation_loss = 4424.9033203125
 Trial: 42 Epoch: 97, train_loss: 30014.04638671875, validation_loss = 4203.9478759765625
 Trial: 42 Epoch: 98, train_loss: 29808.8876953125, validation_loss = 4604.3438720703125
 Trial: 42 Epoch: 99, train_loss: 30266.31640625, validation_loss = 4010.8709716796875
========Trial 43 params: {'n_layers': 2, 'K': 2, 'Dropout': 0.2, 'Hidden0': 256, 'Hidden1': 64, 'optimizer': 'Adam', 'lr': 0.01408873835223248, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |   12288    |
|    sptH.0.gcn.bias     |    256     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.sAtt.Vs     |    729     |
|     sptH.2.sAtt.bs     |    729     |
|     sptH.2.sAtt.W1     |    256     |
|     sptH.2.sAtt.W2     |    256     |
|     sptH.2.sAtt.W3     |     1      |
|   sptH.2.gcn.weight    |   32768    |
|    sptH.2.gcn.bias     |     64     |
| sptH.2.timeConv.weight |     3      |
|  sptH.2.timeConv.bias  |     1      |
|     sptH.4.weight      |    768     |
|      sptH.4.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    6144    |
|    sptD.0.gcn.bias     |    256     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.sAtt.Vs     |    729     |
|     sptD.2.sAtt.bs     |    729     |
|     sptD.2.sAtt.W1     |    256     |
|     sptD.2.sAtt.W2     |    256     |
|     sptD.2.sAtt.W3     |     1      |
|   sptD.2.gcn.weight    |   32768    |
|    sptD.2.gcn.bias     |     64     |
| sptD.2.timeConv.weight |     3      |
|  sptD.2.timeConv.bias  |     1      |
|     sptD.4.weight      |    768     |
|      sptD.4.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |   12288    |
|    sptW.0.gcn.bias     |    256     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.sAtt.Vs     |    729     |
|     sptW.2.sAtt.bs     |    729     |
|     sptW.2.sAtt.W1     |    256     |
|     sptW.2.sAtt.W2     |    256     |
|     sptW.2.sAtt.W3     |     1      |
|   sptW.2.gcn.weight    |   32768    |
|    sptW.2.gcn.bias     |     64     |
| sptW.2.timeConv.weight |     3      |
|  sptW.2.timeConv.bias  |     1      |
|     sptW.4.weight      |    768     |
|      sptW.4.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 142794
 Trial: 43 Epoch: 0, train_loss: 157325.998046875, validation_loss = 7478.9716796875
 Trial: 43 Epoch: 1, train_loss: 59591.36669921875, validation_loss = 4635.578369140625
 Trial: 43 Epoch: 2, train_loss: 49158.99365234375, validation_loss = 5622.5833740234375
 Trial: 43 Epoch: 3, train_loss: 38888.52844238281, validation_loss = 7322.869873046875
 Trial: 43 Epoch: 4, train_loss: 40648.04309082031, validation_loss = 5333.65869140625
 Trial: 43 Epoch: 5, train_loss: 39702.69982910156, validation_loss = 4277.05126953125
 Trial: 43 Epoch: 6, train_loss: 34092.97009277344, validation_loss = 5177.962158203125
 Trial: 43 Epoch: 7, train_loss: 36764.419677734375, validation_loss = 3856.3330078125
 Trial: 43 Epoch: 8, train_loss: 33740.21325683594, validation_loss = 5605.9825439453125
 Trial: 43 Epoch: 9, train_loss: 35664.57580566406, validation_loss = 4051.81298828125
 Trial: 43 Epoch: 10, train_loss: 34207.481201171875, validation_loss = 6634.2105712890625
 Trial: 43 Epoch: 11, train_loss: 33661.43603515625, validation_loss = 4331.8809814453125
 Trial: 43 Epoch: 12, train_loss: 32646.859497070312, validation_loss = 4223.3985595703125
 Trial: 43 Epoch: 13, train_loss: 29623.517578125, validation_loss = 4439.4053955078125
 Trial: 43 Epoch: 14, train_loss: 31010.617065429688, validation_loss = 4979.8009033203125
 Trial: 43 Epoch: 15, train_loss: 32242.91357421875, validation_loss = 5089.5810546875
 Trial: 43 Epoch: 16, train_loss: 36883.48791503906, validation_loss = 4557.7388916015625
 Trial: 43 Epoch: 17, train_loss: 29129.21728515625, validation_loss = 3917.6627197265625
 Trial: 43 Epoch: 18, train_loss: 31340.192749023438, validation_loss = 4490.425537109375
 Trial: 43 Epoch: 19, train_loss: 33256.61535644531, validation_loss = 3624.7188720703125
 Trial: 43 Epoch: 20, train_loss: 30224.285888671875, validation_loss = 4199.2989501953125
 Trial: 43 Epoch: 21, train_loss: 34350.1298828125, validation_loss = 5304.381591796875
 Trial: 43 Epoch: 22, train_loss: 39078.789306640625, validation_loss = 6214.447509765625
 Trial: 43 Epoch: 23, train_loss: 30375.80029296875, validation_loss = 3987.3524169921875
 Trial: 43 Epoch: 24, train_loss: 27659.587158203125, validation_loss = 3837.3662109375
 Trial: 43 Epoch: 25, train_loss: 29573.844482421875, validation_loss = 5464.2197265625
 Trial: 43 Epoch: 26, train_loss: 29922.031372070312, validation_loss = 4081.4306640625
 Trial: 43 Epoch: 27, train_loss: 29014.856689453125, validation_loss = 3785.974853515625
 Trial: 43 Epoch: 28, train_loss: 26508.680541992188, validation_loss = 3804.8955078125
 Trial: 43 Epoch: 29, train_loss: 29980.60009765625, validation_loss = 4139.48388671875
 Trial: 43 Epoch: 30, train_loss: 28168.890869140625, validation_loss = 3636.954345703125
 Trial: 43 Epoch: 31, train_loss: 30579.180297851562, validation_loss = 3666.70556640625
 Trial: 43 Epoch: 32, train_loss: 25704.792602539062, validation_loss = 4276.1513671875
 Trial: 43 Epoch: 33, train_loss: 26652.018310546875, validation_loss = 5399.16455078125
 Trial: 43 Epoch: 34, train_loss: 31869.697875976562, validation_loss = 4421.46630859375
 Trial: 43 Epoch: 35, train_loss: 33663.624755859375, validation_loss = 4142.6168212890625
 Trial: 43 Epoch: 36, train_loss: 26283.413330078125, validation_loss = 4939.099853515625
 Trial: 43 Epoch: 37, train_loss: 26678.034790039062, validation_loss = 3280.0882568359375
 Trial: 43 Epoch: 38, train_loss: 27801.393798828125, validation_loss = 4104.353759765625
 Trial: 43 Epoch: 39, train_loss: 28090.666259765625, validation_loss = 3854.3515625
 Trial: 43 Epoch: 40, train_loss: 28530.681274414062, validation_loss = 3674.017578125
 Trial: 43 Epoch: 41, train_loss: 28861.93408203125, validation_loss = 4806.30859375
 Trial: 43 Epoch: 42, train_loss: 27237.375854492188, validation_loss = 3119.9222412109375
 Trial: 43 Epoch: 43, train_loss: 26100.2353515625, validation_loss = 4814.1871337890625
 Trial: 43 Epoch: 44, train_loss: 26600.839599609375, validation_loss = 3345.5352783203125
 Trial: 43 Epoch: 45, train_loss: 25239.019653320312, validation_loss = 3878.7154541015625
 Trial: 43 Epoch: 46, train_loss: 25504.583984375, validation_loss = 3621.869140625
 Trial: 43 Epoch: 47, train_loss: 27051.889526367188, validation_loss = 3335.5457763671875
 Trial: 43 Epoch: 48, train_loss: 28039.864501953125, validation_loss = 4027.7718505859375
 Trial: 43 Epoch: 49, train_loss: 26233.516357421875, validation_loss = 4054.72802734375
 Trial: 43 Epoch: 50, train_loss: 23597.582153320312, validation_loss = 4695.865234375
 Trial: 43 Epoch: 51, train_loss: 24639.770263671875, validation_loss = 3601.6673583984375
 Trial: 43 Epoch: 52, train_loss: 24626.3662109375, validation_loss = 3603.49609375
 Trial: 43 Epoch: 53, train_loss: 24288.534301757812, validation_loss = 3155.649658203125
 Trial: 43 Epoch: 54, train_loss: 24343.394165039062, validation_loss = 3265.8712158203125
 Trial: 43 Epoch: 55, train_loss: 23925.593017578125, validation_loss = 4089.2581787109375
 Trial: 43 Epoch: 56, train_loss: 24549.709106445312, validation_loss = 3209.650634765625
 Trial: 43 Epoch: 57, train_loss: 24200.0107421875, validation_loss = 4193.82177734375
 Trial: 43 Epoch: 58, train_loss: 24836.92822265625, validation_loss = 4211.797607421875
 Trial: 43 Epoch: 59, train_loss: 33165.040283203125, validation_loss = 3727.5679931640625
 Trial: 43 Epoch: 60, train_loss: 26175.519409179688, validation_loss = 3591.7711181640625
 Trial: 43 Epoch: 61, train_loss: 23553.479614257812, validation_loss = 3626.258544921875
 Trial: 43 Epoch: 62, train_loss: 23250.214599609375, validation_loss = 4182.0947265625
 Trial: 43 Epoch: 63, train_loss: 24181.756958007812, validation_loss = 3739.13427734375
 Trial: 43 Epoch: 64, train_loss: 23989.638916015625, validation_loss = 3696.9573974609375
 Trial: 43 Epoch: 65, train_loss: 23925.38311767578, validation_loss = 4067.708251953125
 Trial: 43 Epoch: 66, train_loss: 27024.291625976562, validation_loss = 3873.2930908203125
 Trial: 43 Epoch: 67, train_loss: 28629.581787109375, validation_loss = 4725.8673095703125
 Trial: 43 Epoch: 68, train_loss: 27241.145874023438, validation_loss = 4076.5516357421875
 Trial: 43 Epoch: 69, train_loss: 28323.314331054688, validation_loss = 3629.57763671875
 Trial: 43 Epoch: 70, train_loss: 29837.403564453125, validation_loss = 3808.2320556640625
 Trial: 43 Epoch: 71, train_loss: 25556.275268554688, validation_loss = 4187.64501953125
 Trial: 43 Epoch: 72, train_loss: 23519.17041015625, validation_loss = 3111.4000244140625
 Trial: 43 Epoch: 73, train_loss: 23633.507690429688, validation_loss = 3400.2264404296875
 Trial: 43 Epoch: 74, train_loss: 23244.583129882812, validation_loss = 4861.638427734375
 Trial: 43 Epoch: 75, train_loss: 25859.956909179688, validation_loss = 3228.78564453125
 Trial: 43 Epoch: 76, train_loss: 30374.552734375, validation_loss = 3325.6922607421875
 Trial: 43 Epoch: 77, train_loss: 30293.573364257812, validation_loss = 5679.896484375
 Trial: 43 Epoch: 78, train_loss: 27061.375, validation_loss = 3427.271240234375
 Trial: 43 Epoch: 79, train_loss: 23984.609252929688, validation_loss = 3536.4132080078125
 Trial: 43 Epoch: 80, train_loss: 25289.64532470703, validation_loss = 3317.8868408203125
 Trial: 43 Epoch: 81, train_loss: 24039.058349609375, validation_loss = 2896.9927978515625
 Trial: 43 Epoch: 82, train_loss: 22990.823120117188, validation_loss = 3597.938720703125
 Trial: 43 Epoch: 83, train_loss: 23619.390747070312, validation_loss = 3286.6358642578125
 Trial: 43 Epoch: 84, train_loss: 24338.141479492188, validation_loss = 3426.449951171875
 Trial: 43 Epoch: 85, train_loss: 24179.996826171875, validation_loss = 3607.97802734375
 Trial: 43 Epoch: 86, train_loss: 22157.179565429688, validation_loss = 3595.0628662109375
 Trial: 43 Epoch: 87, train_loss: 25597.2705078125, validation_loss = 3391.0308837890625
 Trial: 43 Epoch: 88, train_loss: 25323.394897460938, validation_loss = 4787.90185546875
 Trial: 43 Epoch: 89, train_loss: 22907.5048828125, validation_loss = 3882.8231201171875
 Trial: 43 Epoch: 90, train_loss: 21566.009216308594, validation_loss = 4458.1917724609375
 Trial: 43 Epoch: 91, train_loss: 27313.856323242188, validation_loss = 3221.189697265625
 Trial: 43 Epoch: 92, train_loss: 25829.834716796875, validation_loss = 3097.32373046875
 Trial: 43 Epoch: 93, train_loss: 24497.643432617188, validation_loss = 4150.697265625
 Trial: 43 Epoch: 94, train_loss: 21742.444580078125, validation_loss = 2958.1220703125
 Trial: 43 Epoch: 95, train_loss: 21528.034545898438, validation_loss = 3915.7900390625
 Trial: 43 Epoch: 96, train_loss: 22152.078247070312, validation_loss = 3071.697509765625
 Trial: 43 Epoch: 97, train_loss: 21111.598022460938, validation_loss = 3183.9615478515625
 Trial: 43 Epoch: 98, train_loss: 21752.354370117188, validation_loss = 3462.9893798828125
 Trial: 43 Epoch: 99, train_loss: 22748.463256835938, validation_loss = 2942.474609375
========Trial 44 params: {'n_layers': 2, 'K': 2, 'Dropout': 0.2, 'Hidden0': 256, 'Hidden1': 64, 'optimizer': 'Adam', 'lr': 0.013355666190829094, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |   12288    |
|    sptH.0.gcn.bias     |    256     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.sAtt.Vs     |    729     |
|     sptH.2.sAtt.bs     |    729     |
|     sptH.2.sAtt.W1     |    256     |
|     sptH.2.sAtt.W2     |    256     |
|     sptH.2.sAtt.W3     |     1      |
|   sptH.2.gcn.weight    |   32768    |
|    sptH.2.gcn.bias     |     64     |
| sptH.2.timeConv.weight |     3      |
|  sptH.2.timeConv.bias  |     1      |
|     sptH.4.weight      |    768     |
|      sptH.4.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    6144    |
|    sptD.0.gcn.bias     |    256     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.sAtt.Vs     |    729     |
|     sptD.2.sAtt.bs     |    729     |
|     sptD.2.sAtt.W1     |    256     |
|     sptD.2.sAtt.W2     |    256     |
|     sptD.2.sAtt.W3     |     1      |
|   sptD.2.gcn.weight    |   32768    |
|    sptD.2.gcn.bias     |     64     |
| sptD.2.timeConv.weight |     3      |
|  sptD.2.timeConv.bias  |     1      |
|     sptD.4.weight      |    768     |
|      sptD.4.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |   12288    |
|    sptW.0.gcn.bias     |    256     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.sAtt.Vs     |    729     |
|     sptW.2.sAtt.bs     |    729     |
|     sptW.2.sAtt.W1     |    256     |
|     sptW.2.sAtt.W2     |    256     |
|     sptW.2.sAtt.W3     |     1      |
|   sptW.2.gcn.weight    |   32768    |
|    sptW.2.gcn.bias     |     64     |
| sptW.2.timeConv.weight |     3      |
|  sptW.2.timeConv.bias  |     1      |
|     sptW.4.weight      |    768     |
|      sptW.4.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 142794
 Trial: 44 Epoch: 0, train_loss: 157819.76513671875, validation_loss = 7434.208251953125
 Trial: 44 Epoch: 1, train_loss: 61871.126708984375, validation_loss = 5142.109619140625
 Trial: 44 Epoch: 2, train_loss: 49559.013671875, validation_loss = 4440.9403076171875
 Trial: 44 Epoch: 3, train_loss: 40397.81799316406, validation_loss = 8691.28271484375
 Trial: 44 Epoch: 4, train_loss: 39807.9482421875, validation_loss = 4557.944580078125
 Trial: 44 Epoch: 5, train_loss: 37521.24853515625, validation_loss = 5465.622802734375
 Trial: 44 Epoch: 6, train_loss: 33536.31335449219, validation_loss = 5041.232421875
 Trial: 44 Epoch: 7, train_loss: 38613.362060546875, validation_loss = 3902.4378662109375
 Trial: 44 Epoch: 8, train_loss: 34263.542236328125, validation_loss = 6584.08203125
 Trial: 44 Epoch: 9, train_loss: 36672.126220703125, validation_loss = 4186.1484375
 Trial: 44 Epoch: 10, train_loss: 33879.93591308594, validation_loss = 6448.353271484375
 Trial: 44 Epoch: 11, train_loss: 34190.97814941406, validation_loss = 4545.413330078125
 Trial: 44 Epoch: 12, train_loss: 32347.23388671875, validation_loss = 4373.232666015625
 Trial: 44 Epoch: 13, train_loss: 31435.6728515625, validation_loss = 4750.816650390625
 Trial: 44 Epoch: 14, train_loss: 32667.131591796875, validation_loss = 5907.9371337890625
 Trial: 44 Epoch: 15, train_loss: 33674.78210449219, validation_loss = 6116.296630859375
 Trial: 44 Epoch: 16, train_loss: 37503.808837890625, validation_loss = 5581.613037109375
 Trial: 44 Epoch: 17, train_loss: 29946.326416015625, validation_loss = 3595.9970703125
 Trial: 44 Epoch: 18, train_loss: 31022.899169921875, validation_loss = 4274.4693603515625
 Trial: 44 Epoch: 19, train_loss: 33536.49853515625, validation_loss = 3610.2503662109375
 Trial: 44 Epoch: 20, train_loss: 31140.157348632812, validation_loss = 3988.794189453125
 Trial: 44 Epoch: 21, train_loss: 36722.82556152344, validation_loss = 5448.87451171875
 Trial: 44 Epoch: 22, train_loss: 40064.66540527344, validation_loss = 6264.99755859375
 Trial: 44 Epoch: 23, train_loss: 30355.368408203125, validation_loss = 4538.0316162109375
 Trial: 44 Epoch: 24, train_loss: 27787.912963867188, validation_loss = 3657.2686767578125
 Trial: 44 Epoch: 25, train_loss: 27138.59521484375, validation_loss = 4241.1466064453125
 Trial: 44 Epoch: 26, train_loss: 27678.10009765625, validation_loss = 6081.0535888671875
 Trial: 44 Epoch: 27, train_loss: 27965.496704101562, validation_loss = 3353.9117431640625
 Trial: 44 Epoch: 28, train_loss: 26152.869506835938, validation_loss = 4571.1763916015625
 Trial: 44 Epoch: 29, train_loss: 30295.739013671875, validation_loss = 4055.383544921875
 Trial: 44 Epoch: 30, train_loss: 27163.412353515625, validation_loss = 3442.8504638671875
 Trial: 44 Epoch: 31, train_loss: 26957.456298828125, validation_loss = 3907.5321044921875
 Trial: 44 Epoch: 32, train_loss: 24939.334838867188, validation_loss = 4191.92529296875
 Trial: 44 Epoch: 33, train_loss: 27125.518432617188, validation_loss = 4767.1953125
 Trial: 44 Epoch: 34, train_loss: 30598.330200195312, validation_loss = 4107.8345947265625
 Trial: 44 Epoch: 35, train_loss: 31620.953857421875, validation_loss = 3903.421630859375
 Trial: 44 Epoch: 36, train_loss: 26439.948852539062, validation_loss = 4962.312255859375
 Trial: 44 Epoch: 37, train_loss: 27479.02734375, validation_loss = 3275.8521728515625
 Trial: 44 Epoch: 38, train_loss: 27094.715942382812, validation_loss = 4200.4481201171875
 Trial: 44 Epoch: 39, train_loss: 26228.724731445312, validation_loss = 3766.4935302734375
 Trial: 44 Epoch: 40, train_loss: 27062.448120117188, validation_loss = 3758.3470458984375
 Trial: 44 Epoch: 41, train_loss: 26900.460693359375, validation_loss = 4321.412841796875
 Trial: 44 Epoch: 42, train_loss: 26852.26416015625, validation_loss = 3183.2943115234375
 Trial: 44 Epoch: 43, train_loss: 25822.670532226562, validation_loss = 4644.5135498046875
 Trial: 44 Epoch: 44, train_loss: 26218.210693359375, validation_loss = 3400.660400390625
 Trial: 44 Epoch: 45, train_loss: 24453.901794433594, validation_loss = 3857.3187255859375
 Trial: 44 Epoch: 46, train_loss: 24925.008850097656, validation_loss = 3521.9703369140625
 Trial: 44 Epoch: 47, train_loss: 28027.073120117188, validation_loss = 3371.9859619140625
 Trial: 44 Epoch: 48, train_loss: 29114.104858398438, validation_loss = 4270.3985595703125
 Trial: 44 Epoch: 49, train_loss: 26425.031982421875, validation_loss = 3784.2484130859375
 Trial: 44 Epoch: 50, train_loss: 24554.299438476562, validation_loss = 3844.53662109375
 Trial: 44 Epoch: 51, train_loss: 24207.456298828125, validation_loss = 3535.71875
 Trial: 44 Epoch: 52, train_loss: 26106.535034179688, validation_loss = 4345.5692138671875
 Trial: 44 Epoch: 53, train_loss: 25370.649658203125, validation_loss = 3017.5703125
 Trial: 44 Epoch: 54, train_loss: 24806.773315429688, validation_loss = 3496.0374755859375
 Trial: 44 Epoch: 55, train_loss: 24115.7529296875, validation_loss = 3807.2530517578125
 Trial: 44 Epoch: 56, train_loss: 24908.970825195312, validation_loss = 3339.822021484375
 Trial: 44 Epoch: 57, train_loss: 23434.0478515625, validation_loss = 3321.1571044921875
 Trial: 44 Epoch: 58, train_loss: 24144.24951171875, validation_loss = 3569.4266357421875
 Trial: 44 Epoch: 59, train_loss: 28334.356689453125, validation_loss = 4532.1722412109375
 Trial: 44 Epoch: 60, train_loss: 24578.008178710938, validation_loss = 2868.97998046875
 Trial: 44 Epoch: 61, train_loss: 26523.027465820312, validation_loss = 4838.65185546875
 Trial: 44 Epoch: 62, train_loss: 23806.86181640625, validation_loss = 3357.685302734375
 Trial: 44 Epoch: 63, train_loss: 22939.854370117188, validation_loss = 4000.493896484375
 Trial: 44 Epoch: 64, train_loss: 26032.724731445312, validation_loss = 3478.15185546875
 Trial: 44 Epoch: 65, train_loss: 25091.436157226562, validation_loss = 3732.3685302734375
 Trial: 44 Epoch: 66, train_loss: 27081.943359375, validation_loss = 3319.9866943359375
 Trial: 44 Epoch: 67, train_loss: 26414.249755859375, validation_loss = 4343.4556884765625
 Trial: 44 Epoch: 68, train_loss: 24702.539428710938, validation_loss = 4007.2894287109375
 Trial: 44 Epoch: 69, train_loss: 24939.418090820312, validation_loss = 3729.313720703125
 Trial: 44 Epoch: 70, train_loss: 26269.298706054688, validation_loss = 2862.564208984375
 Trial: 44 Epoch: 71, train_loss: 23689.083251953125, validation_loss = 5473.04248046875
 Trial: 44 Epoch: 72, train_loss: 24406.912841796875, validation_loss = 3061.1304931640625
 Trial: 44 Epoch: 73, train_loss: 24621.541748046875, validation_loss = 4053.0400390625
 Trial: 44 Epoch: 74, train_loss: 24197.915283203125, validation_loss = 4798.4306640625
 Trial: 44 Epoch: 75, train_loss: 24471.112365722656, validation_loss = 3459.630859375
 Trial: 44 Epoch: 76, train_loss: 32527.534301757812, validation_loss = 3302.768798828125
 Trial: 44 Epoch: 77, train_loss: 31521.055419921875, validation_loss = 6914.594482421875
 Trial: 44 Epoch: 78, train_loss: 29705.354858398438, validation_loss = 3359.2213134765625
 Trial: 44 Epoch: 79, train_loss: 25346.562866210938, validation_loss = 3393.1500244140625
 Trial: 44 Epoch: 80, train_loss: 24707.762084960938, validation_loss = 3373.5885009765625
 Trial: 44 Epoch: 81, train_loss: 24584.915161132812, validation_loss = 3170.0264892578125
 Trial: 44 Epoch: 82, train_loss: 22691.263305664062, validation_loss = 3088.1845703125
 Trial: 44 Epoch: 83, train_loss: 24168.663696289062, validation_loss = 3182.584716796875
 Trial: 44 Epoch: 84, train_loss: 25105.095947265625, validation_loss = 3582.02880859375
 Trial: 44 Epoch: 85, train_loss: 23375.062866210938, validation_loss = 3328.53564453125
 Trial: 44 Epoch: 86, train_loss: 21664.8388671875, validation_loss = 3504.9500732421875
 Trial: 44 Epoch: 87, train_loss: 26912.951904296875, validation_loss = 3603.858642578125
 Trial: 44 Epoch: 88, train_loss: 24333.634887695312, validation_loss = 4143.0548095703125
 Trial: 44 Epoch: 89, train_loss: 23025.022338867188, validation_loss = 3237.7904052734375
 Trial: 44 Epoch: 90, train_loss: 20649.768432617188, validation_loss = 4731.92236328125
 Trial: 44 Epoch: 91, train_loss: 34096.001220703125, validation_loss = 4655.931884765625
 Trial: 44 Epoch: 92, train_loss: 47938.288818359375, validation_loss = 5929.177978515625
 Trial: 44 Epoch: 93, train_loss: 43376.71325683594, validation_loss = 4387.9580078125
 Trial: 44 Epoch: 94, train_loss: 38886.361328125, validation_loss = 3790.057373046875
 Trial: 44 Epoch: 95, train_loss: 32435.85107421875, validation_loss = 5762.77587890625
 Trial: 44 Epoch: 96, train_loss: 34591.54211425781, validation_loss = 7353.360595703125
 Trial: 44 Epoch: 97, train_loss: 31841.58056640625, validation_loss = 4154.4501953125
 Trial: 44 Epoch: 98, train_loss: 30794.01416015625, validation_loss = 3986.864990234375
 Trial: 44 Epoch: 99, train_loss: 29495.01611328125, validation_loss = 3677.8013916015625
========Trial 45 params: {'n_layers': 2, 'K': 2, 'Dropout': 0.5, 'Hidden0': 256, 'Hidden1': 64, 'optimizer': 'Adam', 'lr': 0.04970805526432641, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |   12288    |
|    sptH.0.gcn.bias     |    256     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.sAtt.Vs     |    729     |
|     sptH.2.sAtt.bs     |    729     |
|     sptH.2.sAtt.W1     |    256     |
|     sptH.2.sAtt.W2     |    256     |
|     sptH.2.sAtt.W3     |     1      |
|   sptH.2.gcn.weight    |   32768    |
|    sptH.2.gcn.bias     |     64     |
| sptH.2.timeConv.weight |     3      |
|  sptH.2.timeConv.bias  |     1      |
|     sptH.4.weight      |    768     |
|      sptH.4.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    6144    |
|    sptD.0.gcn.bias     |    256     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.sAtt.Vs     |    729     |
|     sptD.2.sAtt.bs     |    729     |
|     sptD.2.sAtt.W1     |    256     |
|     sptD.2.sAtt.W2     |    256     |
|     sptD.2.sAtt.W3     |     1      |
|   sptD.2.gcn.weight    |   32768    |
|    sptD.2.gcn.bias     |     64     |
| sptD.2.timeConv.weight |     3      |
|  sptD.2.timeConv.bias  |     1      |
|     sptD.4.weight      |    768     |
|      sptD.4.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |   12288    |
|    sptW.0.gcn.bias     |    256     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.sAtt.Vs     |    729     |
|     sptW.2.sAtt.bs     |    729     |
|     sptW.2.sAtt.W1     |    256     |
|     sptW.2.sAtt.W2     |    256     |
|     sptW.2.sAtt.W3     |     1      |
|   sptW.2.gcn.weight    |   32768    |
|    sptW.2.gcn.bias     |     64     |
| sptW.2.timeConv.weight |     3      |
|  sptW.2.timeConv.bias  |     1      |
|     sptW.4.weight      |    768     |
|      sptW.4.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 142794
 Trial: 45 Epoch: 0, train_loss: 865746.2119140625, validation_loss = 11074.2353515625
========Trial 46 params: {'n_layers': 2, 'K': 2, 'Dropout': 0.2, 'Hidden0': 256, 'Hidden1': 64, 'optimizer': 'Adam', 'lr': 0.03160759578026915, 'batch': 16}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |   12288    |
|    sptH.0.gcn.bias     |    256     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.sAtt.Vs     |    729     |
|     sptH.2.sAtt.bs     |    729     |
|     sptH.2.sAtt.W1     |    256     |
|     sptH.2.sAtt.W2     |    256     |
|     sptH.2.sAtt.W3     |     1      |
|   sptH.2.gcn.weight    |   32768    |
|    sptH.2.gcn.bias     |     64     |
| sptH.2.timeConv.weight |     3      |
|  sptH.2.timeConv.bias  |     1      |
|     sptH.4.weight      |    768     |
|      sptH.4.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    6144    |
|    sptD.0.gcn.bias     |    256     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.sAtt.Vs     |    729     |
|     sptD.2.sAtt.bs     |    729     |
|     sptD.2.sAtt.W1     |    256     |
|     sptD.2.sAtt.W2     |    256     |
|     sptD.2.sAtt.W3     |     1      |
|   sptD.2.gcn.weight    |   32768    |
|    sptD.2.gcn.bias     |     64     |
| sptD.2.timeConv.weight |     3      |
|  sptD.2.timeConv.bias  |     1      |
|     sptD.4.weight      |    768     |
|      sptD.4.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |   12288    |
|    sptW.0.gcn.bias     |    256     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.sAtt.Vs     |    729     |
|     sptW.2.sAtt.bs     |    729     |
|     sptW.2.sAtt.W1     |    256     |
|     sptW.2.sAtt.W2     |    256     |
|     sptW.2.sAtt.W3     |     1      |
|   sptW.2.gcn.weight    |   32768    |
|    sptW.2.gcn.bias     |     64     |
| sptW.2.timeConv.weight |     3      |
|  sptW.2.timeConv.bias  |     1      |
|     sptW.4.weight      |    768     |
|      sptW.4.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 142794
 Trial: 46 Epoch: 0, train_loss: 157467.62109375, validation_loss = 45992.90234375
========Trial 47 params: {'n_layers': 1, 'K': 2, 'Dropout': 0.4, 'Hidden0': 256, 'optimizer': 'Adam', 'lr': 0.0163660811716622, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |   12288    |
|    sptH.0.gcn.bias     |    256     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |    3072    |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    6144    |
|    sptD.0.gcn.bias     |    256     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |    3072    |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |   12288    |
|    sptW.0.gcn.bias     |    256     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |    3072    |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 45285
 Trial: 47 Epoch: 0, train_loss: 139241.56494140625, validation_loss = 7072.443603515625
 Trial: 47 Epoch: 1, train_loss: 52504.35693359375, validation_loss = 5199.3441162109375
 Trial: 47 Epoch: 2, train_loss: 39422.515380859375, validation_loss = 4910.351806640625
 Trial: 47 Epoch: 3, train_loss: 43734.72521972656, validation_loss = 5720.74560546875
 Trial: 47 Epoch: 4, train_loss: 59486.16003417969, validation_loss = 4067.6666259765625
 Trial: 47 Epoch: 5, train_loss: 46346.36828613281, validation_loss = 8322.507568359375
 Trial: 47 Epoch: 6, train_loss: 30667.175537109375, validation_loss = 4639.520751953125
 Trial: 47 Epoch: 7, train_loss: 29362.543090820312, validation_loss = 4237.8909912109375
 Trial: 47 Epoch: 8, train_loss: 27629.934448242188, validation_loss = 3535.2576904296875
 Trial: 47 Epoch: 9, train_loss: 26779.469116210938, validation_loss = 3703.2017822265625
 Trial: 47 Epoch: 10, train_loss: 26374.887939453125, validation_loss = 4501.888427734375
 Trial: 47 Epoch: 11, train_loss: 30560.380432128906, validation_loss = 5334.833984375
 Trial: 47 Epoch: 12, train_loss: 30248.369384765625, validation_loss = 3990.70361328125
 Trial: 47 Epoch: 13, train_loss: 29368.47119140625, validation_loss = 5100.47412109375
 Trial: 47 Epoch: 14, train_loss: 25649.385986328125, validation_loss = 3306.236083984375
 Trial: 47 Epoch: 15, train_loss: 25439.964721679688, validation_loss = 3487.2821044921875
 Trial: 47 Epoch: 16, train_loss: 23223.89080810547, validation_loss = 3491.2794189453125
 Trial: 47 Epoch: 17, train_loss: 22767.49591064453, validation_loss = 3951.694580078125
 Trial: 47 Epoch: 18, train_loss: 24589.4345703125, validation_loss = 3458.7010498046875
 Trial: 47 Epoch: 19, train_loss: 24616.198852539062, validation_loss = 3623.955810546875
 Trial: 47 Epoch: 20, train_loss: 25596.853759765625, validation_loss = 3246.443359375
 Trial: 47 Epoch: 21, train_loss: 25748.197631835938, validation_loss = 5199.274169921875
 Trial: 47 Epoch: 22, train_loss: 29610.805114746094, validation_loss = 4095.232666015625
 Trial: 47 Epoch: 23, train_loss: 28682.016235351562, validation_loss = 4902.3206787109375
 Trial: 47 Epoch: 24, train_loss: 31378.608764648438, validation_loss = 4183.9464111328125
 Trial: 47 Epoch: 25, train_loss: 28144.13623046875, validation_loss = 5644.30859375
 Trial: 47 Epoch: 26, train_loss: 27819.228637695312, validation_loss = 3470.2691650390625
 Trial: 47 Epoch: 27, train_loss: 24379.484313964844, validation_loss = 2945.76123046875
 Trial: 47 Epoch: 28, train_loss: 25588.015747070312, validation_loss = 3270.90576171875
 Trial: 47 Epoch: 29, train_loss: 26089.927734375, validation_loss = 3330.227294921875
 Trial: 47 Epoch: 30, train_loss: 24000.599487304688, validation_loss = 4283.390869140625
 Trial: 47 Epoch: 31, train_loss: 26932.17626953125, validation_loss = 4921.354248046875
 Trial: 47 Epoch: 32, train_loss: 28998.684814453125, validation_loss = 3237.7010498046875
 Trial: 47 Epoch: 33, train_loss: 23154.100708007812, validation_loss = 3073.7103271484375
 Trial: 47 Epoch: 34, train_loss: 23818.354370117188, validation_loss = 2866.9361572265625
 Trial: 47 Epoch: 35, train_loss: 22254.82666015625, validation_loss = 3326.1812744140625
 Trial: 47 Epoch: 36, train_loss: 24044.450927734375, validation_loss = 3185.4141845703125
 Trial: 47 Epoch: 37, train_loss: 23935.314208984375, validation_loss = 3361.806884765625
 Trial: 47 Epoch: 38, train_loss: 21744.088745117188, validation_loss = 3027.2391357421875
 Trial: 47 Epoch: 39, train_loss: 20901.209228515625, validation_loss = 2966.909912109375
 Trial: 47 Epoch: 40, train_loss: 21209.763793945312, validation_loss = 2962.0367431640625
 Trial: 47 Epoch: 41, train_loss: 20374.33038330078, validation_loss = 2738.5579833984375
 Trial: 47 Epoch: 42, train_loss: 20317.039611816406, validation_loss = 2658.86669921875
 Trial: 47 Epoch: 43, train_loss: 21677.684204101562, validation_loss = 3258.1934814453125
 Trial: 47 Epoch: 44, train_loss: 20989.2158203125, validation_loss = 3032.404296875
 Trial: 47 Epoch: 45, train_loss: 22764.536560058594, validation_loss = 3233.2154541015625
 Trial: 47 Epoch: 46, train_loss: 21830.569702148438, validation_loss = 4173.84912109375
 Trial: 47 Epoch: 47, train_loss: 20711.033325195312, validation_loss = 2684.8001708984375
 Trial: 47 Epoch: 48, train_loss: 21304.040161132812, validation_loss = 2747.671142578125
 Trial: 47 Epoch: 49, train_loss: 20121.109741210938, validation_loss = 3632.04833984375
 Trial: 47 Epoch: 50, train_loss: 21361.932861328125, validation_loss = 3061.26953125
 Trial: 47 Epoch: 51, train_loss: 19223.350219726562, validation_loss = 3398.5272216796875
 Trial: 47 Epoch: 52, train_loss: 21507.977172851562, validation_loss = 3499.126220703125
 Trial: 47 Epoch: 53, train_loss: 24969.069213867188, validation_loss = 2720.7047119140625
 Trial: 47 Epoch: 54, train_loss: 23224.15771484375, validation_loss = 2807.5364990234375
 Trial: 47 Epoch: 55, train_loss: 21467.20343017578, validation_loss = 3026.2130126953125
 Trial: 47 Epoch: 56, train_loss: 20329.46826171875, validation_loss = 2754.3447265625
 Trial: 47 Epoch: 57, train_loss: 22698.051025390625, validation_loss = 3191.453369140625
 Trial: 47 Epoch: 58, train_loss: 22641.829345703125, validation_loss = 2989.8553466796875
 Trial: 47 Epoch: 59, train_loss: 21048.1455078125, validation_loss = 2790.1563720703125
 Trial: 47 Epoch: 60, train_loss: 21622.117065429688, validation_loss = 2644.5726318359375
 Trial: 47 Epoch: 61, train_loss: 22967.478759765625, validation_loss = 3532.1676025390625
 Trial: 47 Epoch: 62, train_loss: 20960.445434570312, validation_loss = 2963.3936767578125
 Trial: 47 Epoch: 63, train_loss: 19928.9912109375, validation_loss = 3250.969482421875
 Trial: 47 Epoch: 64, train_loss: 19152.401245117188, validation_loss = 3013.4710693359375
 Trial: 47 Epoch: 65, train_loss: 19676.854064941406, validation_loss = 2924.51123046875
 Trial: 47 Epoch: 66, train_loss: 20375.253845214844, validation_loss = 3376.982421875
 Trial: 47 Epoch: 67, train_loss: 24220.61639404297, validation_loss = 3840.271728515625
 Trial: 47 Epoch: 68, train_loss: 22519.485595703125, validation_loss = 2708.2928466796875
 Trial: 47 Epoch: 69, train_loss: 19987.06787109375, validation_loss = 3074.904052734375
 Trial: 47 Epoch: 70, train_loss: 20134.547973632812, validation_loss = 3415.2142333984375
 Trial: 47 Epoch: 71, train_loss: 19428.19512939453, validation_loss = 2783.8353271484375
 Trial: 47 Epoch: 72, train_loss: 19299.10302734375, validation_loss = 2747.8582763671875
 Trial: 47 Epoch: 73, train_loss: 19858.400268554688, validation_loss = 3093.3145751953125
 Trial: 47 Epoch: 74, train_loss: 19152.70733642578, validation_loss = 2980.1826171875
 Trial: 47 Epoch: 75, train_loss: 20254.38330078125, validation_loss = 3608.1988525390625
 Trial: 47 Epoch: 76, train_loss: 22276.038024902344, validation_loss = 3313.460693359375
 Trial: 47 Epoch: 77, train_loss: 20856.757934570312, validation_loss = 3004.8560791015625
 Trial: 47 Epoch: 78, train_loss: 19343.935607910156, validation_loss = 2802.485595703125
 Trial: 47 Epoch: 79, train_loss: 23105.781799316406, validation_loss = 3529.995361328125
 Trial: 47 Epoch: 80, train_loss: 20486.261108398438, validation_loss = 2760.4423828125
 Trial: 47 Epoch: 81, train_loss: 18633.58514404297, validation_loss = 2876.588623046875
 Trial: 47 Epoch: 82, train_loss: 19008.486572265625, validation_loss = 2903.6346435546875
 Trial: 47 Epoch: 83, train_loss: 19320.19842529297, validation_loss = 2919.48388671875
 Trial: 47 Epoch: 84, train_loss: 18610.637817382812, validation_loss = 2783.9979248046875
 Trial: 47 Epoch: 85, train_loss: 18853.450439453125, validation_loss = 3192.3172607421875
 Trial: 47 Epoch: 86, train_loss: 20368.056884765625, validation_loss = 3060.16259765625
 Trial: 47 Epoch: 87, train_loss: 21950.624145507812, validation_loss = 3673.99853515625
 Trial: 47 Epoch: 88, train_loss: 19873.37255859375, validation_loss = 2653.8927001953125
 Trial: 47 Epoch: 89, train_loss: 21291.496032714844, validation_loss = 2767.182373046875
 Trial: 47 Epoch: 90, train_loss: 20199.925537109375, validation_loss = 4212.210205078125
 Trial: 47 Epoch: 91, train_loss: 21677.967346191406, validation_loss = 2804.2698974609375
 Trial: 47 Epoch: 92, train_loss: 21452.88037109375, validation_loss = 3464.09033203125
 Trial: 47 Epoch: 93, train_loss: 19810.03515625, validation_loss = 2641.9200439453125
 Trial: 47 Epoch: 94, train_loss: 21825.3681640625, validation_loss = 3715.15380859375
 Trial: 47 Epoch: 95, train_loss: 21762.683532714844, validation_loss = 3288.769287109375
 Trial: 47 Epoch: 96, train_loss: 23096.26513671875, validation_loss = 3286.33642578125
 Trial: 47 Epoch: 97, train_loss: 20114.218994140625, validation_loss = 2728.2657470703125
 Trial: 47 Epoch: 98, train_loss: 20132.195190429688, validation_loss = 2890.2581787109375
 Trial: 47 Epoch: 99, train_loss: 19295.839477539062, validation_loss = 3050.3187255859375
========Trial 48 params: {'n_layers': 1, 'K': 2, 'Dropout': 0.4, 'Hidden0': 256, 'optimizer': 'Adam', 'lr': 0.02509108288629704, 'batch': 8}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |   12288    |
|    sptH.0.gcn.bias     |    256     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |    3072    |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    6144    |
|    sptD.0.gcn.bias     |    256     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |    3072    |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |   12288    |
|    sptW.0.gcn.bias     |    256     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |    3072    |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 45285
 Trial: 48 Epoch: 0, train_loss: 216715.681640625, validation_loss = 13776.1494140625
========Trial 49 params: {'n_layers': 1, 'K': 2, 'Dropout': 0.4, 'Hidden0': 256, 'optimizer': 'Adam', 'lr': 0.017392281477627048, 'batch': 64}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |   12288    |
|    sptH.0.gcn.bias     |    256     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |    3072    |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    6144    |
|    sptD.0.gcn.bias     |    256     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |    3072    |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |   12288    |
|    sptW.0.gcn.bias     |    256     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |    3072    |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 45285
 Trial: 49 Epoch: 0, train_loss: 67321.109375, validation_loss = 16351.78125
========Trial 50 params: {'n_layers': 1, 'K': 2, 'Dropout': 0.4, 'Hidden0': 256, 'optimizer': 'Adam', 'lr': 0.04344195652459284, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |   12288    |
|    sptH.0.gcn.bias     |    256     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |    3072    |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    6144    |
|    sptD.0.gcn.bias     |    256     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |    3072    |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |   12288    |
|    sptW.0.gcn.bias     |    256     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |    3072    |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 45285
 Trial: 50 Epoch: 0, train_loss: 486834.11181640625, validation_loss = 9079.82568359375
========Trial 51 params: {'n_layers': 1, 'K': 2, 'Dropout': 0.2, 'Hidden0': 256, 'optimizer': 'Adam', 'lr': 0.012980119193243439, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |   12288    |
|    sptH.0.gcn.bias     |    256     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |    3072    |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    6144    |
|    sptD.0.gcn.bias     |    256     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |    3072    |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |   12288    |
|    sptW.0.gcn.bias     |    256     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |    3072    |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 45285
 Trial: 51 Epoch: 0, train_loss: 133628.92431640625, validation_loss = 5472.9747314453125
 Trial: 51 Epoch: 1, train_loss: 39888.311767578125, validation_loss = 4620.5367431640625
 Trial: 51 Epoch: 2, train_loss: 35350.75341796875, validation_loss = 4692.47119140625
 Trial: 51 Epoch: 3, train_loss: 41872.566650390625, validation_loss = 5622.9677734375
 Trial: 51 Epoch: 4, train_loss: 58327.08068847656, validation_loss = 3952.54833984375
 Trial: 51 Epoch: 5, train_loss: 40442.84216308594, validation_loss = 6425.3980712890625
 Trial: 51 Epoch: 6, train_loss: 27612.565551757812, validation_loss = 4667.8028564453125
 Trial: 51 Epoch: 7, train_loss: 27377.54541015625, validation_loss = 3981.3310546875
 Trial: 51 Epoch: 8, train_loss: 25365.478881835938, validation_loss = 3783.039794921875
 Trial: 51 Epoch: 9, train_loss: 24895.468139648438, validation_loss = 3370.2542724609375
 Trial: 51 Epoch: 10, train_loss: 25847.83984375, validation_loss = 4660.8568115234375
 Trial: 51 Epoch: 11, train_loss: 27179.773315429688, validation_loss = 5029.3681640625
 Trial: 51 Epoch: 12, train_loss: 28149.808227539062, validation_loss = 3252.2093505859375
 Trial: 51 Epoch: 13, train_loss: 26217.287963867188, validation_loss = 6036.1533203125
 Trial: 51 Epoch: 14, train_loss: 27391.074340820312, validation_loss = 3811.73193359375
 Trial: 51 Epoch: 15, train_loss: 23146.151977539062, validation_loss = 3534.531494140625
 Trial: 51 Epoch: 16, train_loss: 21943.23223876953, validation_loss = 3332.89599609375
 Trial: 51 Epoch: 17, train_loss: 21496.894287109375, validation_loss = 4060.55810546875
 Trial: 51 Epoch: 18, train_loss: 25206.404663085938, validation_loss = 3137.1748046875
 Trial: 51 Epoch: 19, train_loss: 23940.294555664062, validation_loss = 2928.763427734375
 Trial: 51 Epoch: 20, train_loss: 22022.512084960938, validation_loss = 2958.7003173828125
 Trial: 51 Epoch: 21, train_loss: 22641.34765625, validation_loss = 4011.962646484375
 Trial: 51 Epoch: 22, train_loss: 23684.817993164062, validation_loss = 3290.0341796875
 Trial: 51 Epoch: 23, train_loss: 22452.147033691406, validation_loss = 3899.195556640625
 Trial: 51 Epoch: 24, train_loss: 24831.312927246094, validation_loss = 3690.3802490234375
 Trial: 51 Epoch: 25, train_loss: 22892.255981445312, validation_loss = 3594.3228759765625
 Trial: 51 Epoch: 26, train_loss: 22139.755798339844, validation_loss = 3246.936279296875
 Trial: 51 Epoch: 27, train_loss: 21448.151245117188, validation_loss = 3093.1025390625
 Trial: 51 Epoch: 28, train_loss: 21807.5771484375, validation_loss = 2791.2391357421875
 Trial: 51 Epoch: 29, train_loss: 23412.287231445312, validation_loss = 2767.6016845703125
 Trial: 51 Epoch: 30, train_loss: 22156.416381835938, validation_loss = 4441.447265625
 Trial: 51 Epoch: 31, train_loss: 22192.200927734375, validation_loss = 3169.0023193359375
 Trial: 51 Epoch: 32, train_loss: 23646.43115234375, validation_loss = 2911.1754150390625
 Trial: 51 Epoch: 33, train_loss: 19890.522094726562, validation_loss = 3644.60107421875
 Trial: 51 Epoch: 34, train_loss: 23929.502075195312, validation_loss = 2823.27294921875
 Trial: 51 Epoch: 35, train_loss: 19782.726928710938, validation_loss = 3504.524658203125
 Trial: 51 Epoch: 36, train_loss: 23134.444702148438, validation_loss = 3413.9625244140625
 Trial: 51 Epoch: 37, train_loss: 21590.107177734375, validation_loss = 3492.9481201171875
 Trial: 51 Epoch: 38, train_loss: 20397.084533691406, validation_loss = 3003.5040283203125
 Trial: 51 Epoch: 39, train_loss: 18993.038513183594, validation_loss = 2855.4085693359375
 Trial: 51 Epoch: 40, train_loss: 19101.502868652344, validation_loss = 2748.70361328125
 Trial: 51 Epoch: 41, train_loss: 18472.20068359375, validation_loss = 2633.17578125
 Trial: 51 Epoch: 42, train_loss: 18683.965454101562, validation_loss = 2570.3001708984375
 Trial: 51 Epoch: 43, train_loss: 20204.610717773438, validation_loss = 3073.3599853515625
 Trial: 51 Epoch: 44, train_loss: 20267.261169433594, validation_loss = 3153.08935546875
 Trial: 51 Epoch: 45, train_loss: 20850.49981689453, validation_loss = 2793.99462890625
 Trial: 51 Epoch: 46, train_loss: 20139.911499023438, validation_loss = 4373.6024169921875
 Trial: 51 Epoch: 47, train_loss: 20101.872985839844, validation_loss = 2527.0955810546875
 Trial: 51 Epoch: 48, train_loss: 19779.772827148438, validation_loss = 2777.61962890625
 Trial: 51 Epoch: 49, train_loss: 19007.839904785156, validation_loss = 3629.557373046875
 Trial: 51 Epoch: 50, train_loss: 20779.638793945312, validation_loss = 2782.5848388671875
 Trial: 51 Epoch: 51, train_loss: 19027.527221679688, validation_loss = 3222.3951416015625
 Trial: 51 Epoch: 52, train_loss: 19730.64715576172, validation_loss = 3267.8773193359375
 Trial: 51 Epoch: 53, train_loss: 22686.046997070312, validation_loss = 2529.2491455078125
 Trial: 51 Epoch: 54, train_loss: 23091.66815185547, validation_loss = 2738.984375
 Trial: 51 Epoch: 55, train_loss: 19151.051330566406, validation_loss = 2622.0811767578125
 Trial: 51 Epoch: 56, train_loss: 18655.021362304688, validation_loss = 2520.5672607421875
 Trial: 51 Epoch: 57, train_loss: 19775.161010742188, validation_loss = 3051.828857421875
 Trial: 51 Epoch: 58, train_loss: 21170.514282226562, validation_loss = 2981.6195068359375
 Trial: 51 Epoch: 59, train_loss: 19914.286865234375, validation_loss = 2922.7908935546875
 Trial: 51 Epoch: 60, train_loss: 19412.760314941406, validation_loss = 2670.51171875
 Trial: 51 Epoch: 61, train_loss: 20847.53125, validation_loss = 3178.572998046875
 Trial: 51 Epoch: 62, train_loss: 19071.88916015625, validation_loss = 2796.7061767578125
 Trial: 51 Epoch: 63, train_loss: 18659.514770507812, validation_loss = 2957.4036865234375
 Trial: 51 Epoch: 64, train_loss: 18045.97540283203, validation_loss = 3146.3548583984375
 Trial: 51 Epoch: 65, train_loss: 18351.882873535156, validation_loss = 2669.028564453125
 Trial: 51 Epoch: 66, train_loss: 17448.26171875, validation_loss = 2703.422607421875
 Trial: 51 Epoch: 67, train_loss: 19488.540588378906, validation_loss = 2749.54248046875
 Trial: 51 Epoch: 68, train_loss: 19556.869201660156, validation_loss = 2410.7159423828125
 Trial: 51 Epoch: 69, train_loss: 17961.188415527344, validation_loss = 2715.2938232421875
 Trial: 51 Epoch: 70, train_loss: 18913.29949951172, validation_loss = 3159.618896484375
 Trial: 51 Epoch: 71, train_loss: 17116.05419921875, validation_loss = 2493.8572998046875
 Trial: 51 Epoch: 72, train_loss: 17325.029235839844, validation_loss = 2550.8416748046875
 Trial: 51 Epoch: 73, train_loss: 18182.24346923828, validation_loss = 2940.0072021484375
 Trial: 51 Epoch: 74, train_loss: 17635.501892089844, validation_loss = 2687.9656982421875
 Trial: 51 Epoch: 75, train_loss: 17658.649047851562, validation_loss = 3425.6502685546875
 Trial: 51 Epoch: 76, train_loss: 20880.8935546875, validation_loss = 3668.1165771484375
 Trial: 51 Epoch: 77, train_loss: 20400.941284179688, validation_loss = 2448.3349609375
 Trial: 51 Epoch: 78, train_loss: 16866.76446533203, validation_loss = 2535.9307861328125
 Trial: 51 Epoch: 79, train_loss: 19656.525634765625, validation_loss = 3673.4539794921875
 Trial: 51 Epoch: 80, train_loss: 17888.5224609375, validation_loss = 2653.541015625
 Trial: 51 Epoch: 81, train_loss: 17803.331604003906, validation_loss = 3039.8831787109375
 Trial: 51 Epoch: 82, train_loss: 17659.659729003906, validation_loss = 3545.2811279296875
 Trial: 51 Epoch: 83, train_loss: 17146.17901611328, validation_loss = 2604.9599609375
 Trial: 51 Epoch: 84, train_loss: 17746.91717529297, validation_loss = 2627.488525390625
 Trial: 51 Epoch: 85, train_loss: 17125.165649414062, validation_loss = 2751.0074462890625
 Trial: 51 Epoch: 86, train_loss: 17240.429809570312, validation_loss = 2564.649169921875
 Trial: 51 Epoch: 87, train_loss: 18272.48321533203, validation_loss = 3109.8775634765625
 Trial: 51 Epoch: 88, train_loss: 17470.921508789062, validation_loss = 2477.8697509765625
 Trial: 51 Epoch: 89, train_loss: 18946.5732421875, validation_loss = 2473.6697998046875
 Trial: 51 Epoch: 90, train_loss: 18030.868530273438, validation_loss = 4105.024169921875
 Trial: 51 Epoch: 91, train_loss: 19279.026123046875, validation_loss = 2626.0850830078125
 Trial: 51 Epoch: 92, train_loss: 17901.58758544922, validation_loss = 2899.04296875
 Trial: 51 Epoch: 93, train_loss: 17448.50604248047, validation_loss = 2408.90966796875
 Trial: 51 Epoch: 94, train_loss: 18488.46759033203, validation_loss = 2922.17529296875
 Trial: 51 Epoch: 95, train_loss: 18381.588623046875, validation_loss = 2828.1553955078125
 Trial: 51 Epoch: 96, train_loss: 19690.18896484375, validation_loss = 2765.158203125
 Trial: 51 Epoch: 97, train_loss: 16493.813842773438, validation_loss = 2554.507568359375
 Trial: 51 Epoch: 98, train_loss: 16695.582153320312, validation_loss = 2470.3717041015625
 Trial: 51 Epoch: 99, train_loss: 17298.68701171875, validation_loss = 2949.38671875
========Trial 52 params: {'n_layers': 1, 'K': 2, 'Dropout': 0.4, 'Hidden0': 256, 'optimizer': 'Adam', 'lr': 0.013204956356742521, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |   12288    |
|    sptH.0.gcn.bias     |    256     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |    3072    |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    6144    |
|    sptD.0.gcn.bias     |    256     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |    3072    |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |   12288    |
|    sptW.0.gcn.bias     |    256     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |    3072    |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 45285
 Trial: 52 Epoch: 0, train_loss: 144181.2783203125, validation_loss = 5779.881591796875
 Trial: 52 Epoch: 1, train_loss: 45399.07482910156, validation_loss = 5324.5458984375
 Trial: 52 Epoch: 2, train_loss: 38081.262451171875, validation_loss = 4993.4444580078125
 Trial: 52 Epoch: 3, train_loss: 45064.5439453125, validation_loss = 5643.37255859375
 Trial: 52 Epoch: 4, train_loss: 55254.42004394531, validation_loss = 4261.451904296875
 Trial: 52 Epoch: 5, train_loss: 39341.64306640625, validation_loss = 6214.71484375
 Trial: 52 Epoch: 6, train_loss: 29509.453857421875, validation_loss = 4557.0531005859375
 Trial: 52 Epoch: 7, train_loss: 29006.904296875, validation_loss = 4321.912841796875
 Trial: 52 Epoch: 8, train_loss: 27355.900512695312, validation_loss = 3655.4276123046875
 Trial: 52 Epoch: 9, train_loss: 26201.562377929688, validation_loss = 3463.1033935546875
 Trial: 52 Epoch: 10, train_loss: 27289.341918945312, validation_loss = 4539.0042724609375
 Trial: 52 Epoch: 11, train_loss: 30934.71063232422, validation_loss = 5445.317138671875
 Trial: 52 Epoch: 12, train_loss: 30824.847290039062, validation_loss = 3735.8345947265625
 Trial: 52 Epoch: 13, train_loss: 28888.218627929688, validation_loss = 5451.9093017578125
 Trial: 52 Epoch: 14, train_loss: 27936.93701171875, validation_loss = 3796.5201416015625
 Trial: 52 Epoch: 15, train_loss: 25424.207397460938, validation_loss = 3632.0078125
 Trial: 52 Epoch: 16, train_loss: 23776.875244140625, validation_loss = 3596.472412109375
 Trial: 52 Epoch: 17, train_loss: 23225.293701171875, validation_loss = 4205.5831298828125
 Trial: 52 Epoch: 18, train_loss: 25719.304443359375, validation_loss = 3404.1729736328125
 Trial: 52 Epoch: 19, train_loss: 25081.47265625, validation_loss = 3655.5352783203125
 Trial: 52 Epoch: 20, train_loss: 25848.174560546875, validation_loss = 3504.9097900390625
 Trial: 52 Epoch: 21, train_loss: 27306.777099609375, validation_loss = 5826.96484375
 Trial: 52 Epoch: 22, train_loss: 28480.584350585938, validation_loss = 3310.091796875
 Trial: 52 Epoch: 23, train_loss: 26867.067810058594, validation_loss = 4308.8409423828125
 Trial: 52 Epoch: 24, train_loss: 26163.991821289062, validation_loss = 3388.060302734375
 Trial: 52 Epoch: 25, train_loss: 23171.873291015625, validation_loss = 3469.926025390625
 Trial: 52 Epoch: 26, train_loss: 24039.732971191406, validation_loss = 3178.77490234375
 Trial: 52 Epoch: 27, train_loss: 22391.57080078125, validation_loss = 3036.393310546875
 Trial: 52 Epoch: 28, train_loss: 23927.20263671875, validation_loss = 2891.2078857421875
 Trial: 52 Epoch: 29, train_loss: 24742.20947265625, validation_loss = 3069.9708251953125
 Trial: 52 Epoch: 30, train_loss: 22731.342407226562, validation_loss = 4425.2427978515625
 Trial: 52 Epoch: 31, train_loss: 23790.248901367188, validation_loss = 3586.7689208984375
 Trial: 52 Epoch: 32, train_loss: 24894.914489746094, validation_loss = 3176.8048095703125
 Trial: 52 Epoch: 33, train_loss: 20959.10858154297, validation_loss = 3457.5823974609375
 Trial: 52 Epoch: 34, train_loss: 25698.355224609375, validation_loss = 2832.5994873046875
 Trial: 52 Epoch: 35, train_loss: 21933.254272460938, validation_loss = 3655.1461181640625
 Trial: 52 Epoch: 36, train_loss: 24295.714233398438, validation_loss = 3450.400390625
 Trial: 52 Epoch: 37, train_loss: 23628.934936523438, validation_loss = 3700.8681640625
 Trial: 52 Epoch: 38, train_loss: 22190.719299316406, validation_loss = 3050.013916015625
 Trial: 52 Epoch: 39, train_loss: 20680.092834472656, validation_loss = 3016.5792236328125
 Trial: 52 Epoch: 40, train_loss: 20970.460815429688, validation_loss = 2972.6361083984375
 Trial: 52 Epoch: 41, train_loss: 20397.159423828125, validation_loss = 2672.04443359375
 Trial: 52 Epoch: 42, train_loss: 19940.858459472656, validation_loss = 2696.8487548828125
 Trial: 52 Epoch: 43, train_loss: 21904.45458984375, validation_loss = 3130.706787109375
 Trial: 52 Epoch: 44, train_loss: 20585.978271484375, validation_loss = 3492.5123291015625
 Trial: 52 Epoch: 45, train_loss: 22185.813354492188, validation_loss = 2873.389404296875
 Trial: 52 Epoch: 46, train_loss: 21502.964111328125, validation_loss = 5264.91943359375
 Trial: 52 Epoch: 47, train_loss: 22074.598754882812, validation_loss = 3008.15234375
 Trial: 52 Epoch: 48, train_loss: 22752.758056640625, validation_loss = 2973.082275390625
 Trial: 52 Epoch: 49, train_loss: 19871.805419921875, validation_loss = 3884.2586669921875
 Trial: 52 Epoch: 50, train_loss: 22000.331298828125, validation_loss = 3168.814453125
 Trial: 52 Epoch: 51, train_loss: 20165.650634765625, validation_loss = 3275.4342041015625
 Trial: 52 Epoch: 52, train_loss: 21261.04150390625, validation_loss = 3601.193115234375
 Trial: 52 Epoch: 53, train_loss: 24375.833984375, validation_loss = 2694.525390625
 Trial: 52 Epoch: 54, train_loss: 23531.512084960938, validation_loss = 3065.6724853515625
 Trial: 52 Epoch: 55, train_loss: 21586.926025390625, validation_loss = 2966.4869384765625
 Trial: 52 Epoch: 56, train_loss: 20072.790405273438, validation_loss = 2801.7359619140625
 Trial: 52 Epoch: 57, train_loss: 22454.321411132812, validation_loss = 3102.9197998046875
 Trial: 52 Epoch: 58, train_loss: 22767.305419921875, validation_loss = 2908.28271484375
 Trial: 52 Epoch: 59, train_loss: 20996.646911621094, validation_loss = 2832.3623046875
 Trial: 52 Epoch: 60, train_loss: 21603.614501953125, validation_loss = 2716.60986328125
 Trial: 52 Epoch: 61, train_loss: 23353.041259765625, validation_loss = 3486.4317626953125
 Trial: 52 Epoch: 62, train_loss: 21445.11328125, validation_loss = 2960.04296875
 Trial: 52 Epoch: 63, train_loss: 20103.392456054688, validation_loss = 3017.5352783203125
 Trial: 52 Epoch: 64, train_loss: 19602.4345703125, validation_loss = 3180.662109375
 Trial: 52 Epoch: 65, train_loss: 20755.947998046875, validation_loss = 2848.1551513671875
 Trial: 52 Epoch: 66, train_loss: 20962.74725341797, validation_loss = 3309.156494140625
 Trial: 52 Epoch: 67, train_loss: 20407.127563476562, validation_loss = 2701.339599609375
 Trial: 52 Epoch: 68, train_loss: 20408.370849609375, validation_loss = 2810.8438720703125
 Trial: 52 Epoch: 69, train_loss: 18630.155639648438, validation_loss = 2779.21484375
 Trial: 52 Epoch: 70, train_loss: 20295.25714111328, validation_loss = 3865.7879638671875
 Trial: 52 Epoch: 71, train_loss: 19451.627319335938, validation_loss = 2669.272705078125
 Trial: 52 Epoch: 72, train_loss: 19089.36004638672, validation_loss = 2701.8433837890625
 Trial: 52 Epoch: 73, train_loss: 19822.662475585938, validation_loss = 3181.759033203125
 Trial: 52 Epoch: 74, train_loss: 19258.682250976562, validation_loss = 3016.5821533203125
 Trial: 52 Epoch: 75, train_loss: 19664.65692138672, validation_loss = 3332.077880859375
 Trial: 52 Epoch: 76, train_loss: 21459.73126220703, validation_loss = 3491.5028076171875
 Trial: 52 Epoch: 77, train_loss: 20803.89892578125, validation_loss = 2904.68603515625
 Trial: 52 Epoch: 78, train_loss: 18764.809448242188, validation_loss = 2630.309814453125
 Trial: 52 Epoch: 79, train_loss: 22003.70965576172, validation_loss = 4118.66162109375
 Trial: 52 Epoch: 80, train_loss: 20325.69903564453, validation_loss = 2885.1160888671875
 Trial: 52 Epoch: 81, train_loss: 19219.513793945312, validation_loss = 3108.0401611328125
 Trial: 52 Epoch: 82, train_loss: 19688.430053710938, validation_loss = 3403.071533203125
 Trial: 52 Epoch: 83, train_loss: 19305.44891357422, validation_loss = 2772.196533203125
 Trial: 52 Epoch: 84, train_loss: 19222.060302734375, validation_loss = 2648.4820556640625
 Trial: 52 Epoch: 85, train_loss: 19053.139709472656, validation_loss = 2979.9381103515625
 Trial: 52 Epoch: 86, train_loss: 19988.228576660156, validation_loss = 2734.306396484375
 Trial: 52 Epoch: 87, train_loss: 22151.129760742188, validation_loss = 4126.78369140625
 Trial: 52 Epoch: 88, train_loss: 19567.660766601562, validation_loss = 2570.5555419921875
 Trial: 52 Epoch: 89, train_loss: 21346.629028320312, validation_loss = 3217.969482421875
 Trial: 52 Epoch: 90, train_loss: 19603.973876953125, validation_loss = 3662.81689453125
 Trial: 52 Epoch: 91, train_loss: 20399.278564453125, validation_loss = 2834.038818359375
 Trial: 52 Epoch: 92, train_loss: 19897.691467285156, validation_loss = 2924.8282470703125
 Trial: 52 Epoch: 93, train_loss: 19016.116455078125, validation_loss = 2662.71435546875
 Trial: 52 Epoch: 94, train_loss: 21324.177978515625, validation_loss = 3294.463134765625
 Trial: 52 Epoch: 95, train_loss: 21772.63067626953, validation_loss = 2898.000244140625
 Trial: 52 Epoch: 96, train_loss: 20144.465942382812, validation_loss = 2833.9547119140625
 Trial: 52 Epoch: 97, train_loss: 18846.50323486328, validation_loss = 2585.7066650390625
 Trial: 52 Epoch: 98, train_loss: 19141.52783203125, validation_loss = 2855.6312255859375
 Trial: 52 Epoch: 99, train_loss: 19544.230041503906, validation_loss = 3301.158935546875
========Trial 53 params: {'n_layers': 1, 'K': 2, 'Dropout': 0.2, 'Hidden0': 256, 'optimizer': 'Adam', 'lr': 0.016561190933037765, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |   12288    |
|    sptH.0.gcn.bias     |    256     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |    3072    |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    6144    |
|    sptD.0.gcn.bias     |    256     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |    3072    |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |   12288    |
|    sptW.0.gcn.bias     |    256     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |    3072    |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 45285
 Trial: 53 Epoch: 0, train_loss: 133494.50830078125, validation_loss = 6113.864501953125
 Trial: 53 Epoch: 1, train_loss: 45525.86975097656, validation_loss = 4912.5859375
 Trial: 53 Epoch: 2, train_loss: 38592.1259765625, validation_loss = 5091.74951171875
 Trial: 53 Epoch: 3, train_loss: 41536.146484375, validation_loss = 5586.120849609375
 Trial: 53 Epoch: 4, train_loss: 58117.1572265625, validation_loss = 3922.099609375
 Trial: 53 Epoch: 5, train_loss: 44210.66760253906, validation_loss = 8103.257568359375
 Trial: 53 Epoch: 6, train_loss: 29073.650268554688, validation_loss = 5001.43017578125
 Trial: 53 Epoch: 7, train_loss: 27414.188842773438, validation_loss = 4191.8690185546875
 Trial: 53 Epoch: 8, train_loss: 26290.179809570312, validation_loss = 3575.4854736328125
 Trial: 53 Epoch: 9, train_loss: 25540.906494140625, validation_loss = 3372.80517578125
 Trial: 53 Epoch: 10, train_loss: 26127.28466796875, validation_loss = 4924.2822265625
 Trial: 53 Epoch: 11, train_loss: 27696.164184570312, validation_loss = 5082.802978515625
 Trial: 53 Epoch: 12, train_loss: 29762.631225585938, validation_loss = 3353.079833984375
 Trial: 53 Epoch: 13, train_loss: 26692.197021484375, validation_loss = 6043.2490234375
 Trial: 53 Epoch: 14, train_loss: 26956.135375976562, validation_loss = 3462.0205078125
 Trial: 53 Epoch: 15, train_loss: 23648.9248046875, validation_loss = 3619.8541259765625
 Trial: 53 Epoch: 16, train_loss: 22147.69110107422, validation_loss = 3299.5877685546875
 Trial: 53 Epoch: 17, train_loss: 21556.895263671875, validation_loss = 3978.931640625
 Trial: 53 Epoch: 18, train_loss: 24701.496459960938, validation_loss = 3351.8203125
 Trial: 53 Epoch: 19, train_loss: 24036.038208007812, validation_loss = 3415.54638671875
 Trial: 53 Epoch: 20, train_loss: 23405.83837890625, validation_loss = 3163.422607421875
 Trial: 53 Epoch: 21, train_loss: 23938.576416015625, validation_loss = 4852.2872314453125
 Trial: 53 Epoch: 22, train_loss: 26517.110229492188, validation_loss = 3965.3819580078125
 Trial: 53 Epoch: 23, train_loss: 25071.00665283203, validation_loss = 4332.49072265625
 Trial: 53 Epoch: 24, train_loss: 27284.749267578125, validation_loss = 3975.5562744140625
 Trial: 53 Epoch: 25, train_loss: 24928.4609375, validation_loss = 4155.2608642578125
 Trial: 53 Epoch: 26, train_loss: 24075.250366210938, validation_loss = 3442.4764404296875
 Trial: 53 Epoch: 27, train_loss: 23072.52655029297, validation_loss = 3024.79443359375
 Trial: 53 Epoch: 28, train_loss: 23538.426147460938, validation_loss = 2962.6435546875
 Trial: 53 Epoch: 29, train_loss: 26710.672973632812, validation_loss = 2952.1964111328125
 Trial: 53 Epoch: 30, train_loss: 23717.448852539062, validation_loss = 4450.31640625
 Trial: 53 Epoch: 31, train_loss: 24151.95654296875, validation_loss = 3424.5291748046875
 Trial: 53 Epoch: 32, train_loss: 25423.786987304688, validation_loss = 3112.586669921875
 Trial: 53 Epoch: 33, train_loss: 20970.82159423828, validation_loss = 3660.616455078125
 Trial: 53 Epoch: 34, train_loss: 23407.796264648438, validation_loss = 2762.25537109375
 Trial: 53 Epoch: 35, train_loss: 21051.94384765625, validation_loss = 3610.5335693359375
 Trial: 53 Epoch: 36, train_loss: 24277.609008789062, validation_loss = 3371.2440185546875
 Trial: 53 Epoch: 37, train_loss: 22600.273071289062, validation_loss = 3455.506591796875
 Trial: 53 Epoch: 38, train_loss: 21061.373046875, validation_loss = 3077.168701171875
 Trial: 53 Epoch: 39, train_loss: 19428.922973632812, validation_loss = 2875.20751953125
 Trial: 53 Epoch: 40, train_loss: 19227.97821044922, validation_loss = 2758.7032470703125
 Trial: 53 Epoch: 41, train_loss: 18627.248779296875, validation_loss = 2586.5572509765625
 Trial: 53 Epoch: 42, train_loss: 19761.907348632812, validation_loss = 2581.13134765625
 Trial: 53 Epoch: 43, train_loss: 20816.867919921875, validation_loss = 3327.83740234375
 Trial: 53 Epoch: 44, train_loss: 20838.58154296875, validation_loss = 2920.233642578125
 Trial: 53 Epoch: 45, train_loss: 22643.570068359375, validation_loss = 2860.4395751953125
 Trial: 53 Epoch: 46, train_loss: 21166.049682617188, validation_loss = 4609.888427734375
 Trial: 53 Epoch: 47, train_loss: 20317.969665527344, validation_loss = 2673.8377685546875
 Trial: 53 Epoch: 48, train_loss: 20369.659057617188, validation_loss = 2836.301513671875
 Trial: 53 Epoch: 49, train_loss: 19797.389709472656, validation_loss = 3092.695556640625
 Trial: 53 Epoch: 50, train_loss: 20297.89862060547, validation_loss = 2987.8623046875
 Trial: 53 Epoch: 51, train_loss: 18936.681762695312, validation_loss = 3138.4840087890625
 Trial: 53 Epoch: 52, train_loss: 19809.805725097656, validation_loss = 3222.13232421875
 Trial: 53 Epoch: 53, train_loss: 22663.717834472656, validation_loss = 2547.2149658203125
 Trial: 53 Epoch: 54, train_loss: 22848.140991210938, validation_loss = 2646.9207763671875
 Trial: 53 Epoch: 55, train_loss: 19249.905151367188, validation_loss = 2789.9378662109375
 Trial: 53 Epoch: 56, train_loss: 18824.957641601562, validation_loss = 2608.27490234375
 Trial: 53 Epoch: 57, train_loss: 18980.822998046875, validation_loss = 3116.505859375
 Trial: 53 Epoch: 58, train_loss: 19670.581298828125, validation_loss = 3106.24609375
 Trial: 53 Epoch: 59, train_loss: 19709.607543945312, validation_loss = 2513.5018310546875
 Trial: 53 Epoch: 60, train_loss: 18742.484375, validation_loss = 2633.97802734375
 Trial: 53 Epoch: 61, train_loss: 19977.8720703125, validation_loss = 3173.1976318359375
 Trial: 53 Epoch: 62, train_loss: 18513.021606445312, validation_loss = 2727.1002197265625
 Trial: 53 Epoch: 63, train_loss: 18134.06964111328, validation_loss = 2887.1370849609375
 Trial: 53 Epoch: 64, train_loss: 17577.878295898438, validation_loss = 2866.0892333984375
 Trial: 53 Epoch: 65, train_loss: 17876.59735107422, validation_loss = 2651.8016357421875
 Trial: 53 Epoch: 66, train_loss: 17404.09149169922, validation_loss = 2806.3564453125
 Trial: 53 Epoch: 67, train_loss: 20764.109619140625, validation_loss = 3348.85107421875
 Trial: 53 Epoch: 68, train_loss: 20585.92608642578, validation_loss = 2552.5849609375
 Trial: 53 Epoch: 69, train_loss: 18391.791870117188, validation_loss = 2742.6314697265625
 Trial: 53 Epoch: 70, train_loss: 20135.252075195312, validation_loss = 3338.361083984375
 Trial: 53 Epoch: 71, train_loss: 17753.507080078125, validation_loss = 2557.4652099609375
 Trial: 53 Epoch: 72, train_loss: 18846.523498535156, validation_loss = 2477.9808349609375
 Trial: 53 Epoch: 73, train_loss: 18208.560974121094, validation_loss = 2762.278564453125
 Trial: 53 Epoch: 74, train_loss: 17498.95751953125, validation_loss = 2976.4561767578125
 Trial: 53 Epoch: 75, train_loss: 17470.944274902344, validation_loss = 3261.9422607421875
 Trial: 53 Epoch: 76, train_loss: 19519.61749267578, validation_loss = 3637.7276611328125
 Trial: 53 Epoch: 77, train_loss: 20839.450927734375, validation_loss = 2483.2886962890625
 Trial: 53 Epoch: 78, train_loss: 17404.889892578125, validation_loss = 2632.2935791015625
 Trial: 53 Epoch: 79, train_loss: 21475.253051757812, validation_loss = 3285.7193603515625
 Trial: 53 Epoch: 80, train_loss: 18657.305236816406, validation_loss = 2552.5435791015625
 Trial: 53 Epoch: 81, train_loss: 17873.163208007812, validation_loss = 2823.1859130859375
 Trial: 53 Epoch: 82, train_loss: 18276.28497314453, validation_loss = 3000.499755859375
 Trial: 53 Epoch: 83, train_loss: 18634.754455566406, validation_loss = 2863.5126953125
 Trial: 53 Epoch: 84, train_loss: 17380.700622558594, validation_loss = 2702.8004150390625
 Trial: 53 Epoch: 85, train_loss: 17363.0302734375, validation_loss = 2865.5198974609375
 Trial: 53 Epoch: 86, train_loss: 17163.671020507812, validation_loss = 2646.7845458984375
 Trial: 53 Epoch: 87, train_loss: 17720.86541748047, validation_loss = 2791.1552734375
 Trial: 53 Epoch: 88, train_loss: 18143.95831298828, validation_loss = 2487.3902587890625
 Trial: 53 Epoch: 89, train_loss: 19495.404663085938, validation_loss = 2416.470703125
 Trial: 53 Epoch: 90, train_loss: 17737.73211669922, validation_loss = 4367.82177734375
 Trial: 53 Epoch: 91, train_loss: 18955.983276367188, validation_loss = 2669.4986572265625
 Trial: 53 Epoch: 92, train_loss: 17889.104858398438, validation_loss = 3089.267333984375
 Trial: 53 Epoch: 93, train_loss: 18100.450927734375, validation_loss = 2381.1043701171875
 Trial: 53 Epoch: 94, train_loss: 18434.19842529297, validation_loss = 3066.3267822265625
 Trial: 53 Epoch: 95, train_loss: 18310.53302001953, validation_loss = 3083.4566650390625
 Trial: 53 Epoch: 96, train_loss: 21732.44903564453, validation_loss = 2541.3974609375
 Trial: 53 Epoch: 97, train_loss: 16584.25146484375, validation_loss = 2567.4095458984375
 Trial: 53 Epoch: 98, train_loss: 16771.43328857422, validation_loss = 2458.3221435546875
 Trial: 53 Epoch: 99, train_loss: 17695.11376953125, validation_loss = 2893.1824951171875
========Trial 54 params: {'n_layers': 1, 'K': 2, 'Dropout': 0.2, 'Hidden0': 256, 'optimizer': 'Adam', 'lr': 0.025806401478048694, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |   12288    |
|    sptH.0.gcn.bias     |    256     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |    3072    |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    6144    |
|    sptD.0.gcn.bias     |    256     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |    3072    |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |   12288    |
|    sptW.0.gcn.bias     |    256     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |    3072    |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 45285
 Trial: 54 Epoch: 0, train_loss: 116396.91442871094, validation_loss = 5869.95751953125
 Trial: 54 Epoch: 1, train_loss: 48092.1845703125, validation_loss = 6654.359619140625
 Trial: 54 Epoch: 2, train_loss: 41877.25842285156, validation_loss = 5886.416259765625
========Trial 55 params: {'n_layers': 1, 'K': 2, 'Dropout': 0.5, 'Hidden0': 256, 'optimizer': 'Adam', 'lr': 0.015788546897828683, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |   12288    |
|    sptH.0.gcn.bias     |    256     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |    3072    |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    6144    |
|    sptD.0.gcn.bias     |    256     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |    3072    |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |   12288    |
|    sptW.0.gcn.bias     |    256     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |    3072    |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 45285
 Trial: 55 Epoch: 0, train_loss: 151010.0166015625, validation_loss = 6371.6005859375
 Trial: 55 Epoch: 1, train_loss: 53777.75939941406, validation_loss = 5360.3232421875
 Trial: 55 Epoch: 2, train_loss: 42177.2333984375, validation_loss = 5331.2916259765625
 Trial: 55 Epoch: 3, train_loss: 46612.09765625, validation_loss = 5627.00732421875
 Trial: 55 Epoch: 4, train_loss: 58036.711181640625, validation_loss = 4280.681640625
 Trial: 55 Epoch: 5, train_loss: 43670.82922363281, validation_loss = 8092.176513671875
 Trial: 55 Epoch: 6, train_loss: 31706.664306640625, validation_loss = 4913.22021484375
 Trial: 55 Epoch: 7, train_loss: 30435.896728515625, validation_loss = 4707.300048828125
 Trial: 55 Epoch: 8, train_loss: 28918.11767578125, validation_loss = 4103.8031005859375
 Trial: 55 Epoch: 9, train_loss: 28065.714599609375, validation_loss = 3580.874267578125
 Trial: 55 Epoch: 10, train_loss: 28267.069702148438, validation_loss = 4926.4732666015625
 Trial: 55 Epoch: 11, train_loss: 29956.437255859375, validation_loss = 5019.343994140625
 Trial: 55 Epoch: 12, train_loss: 31624.033813476562, validation_loss = 3918.0484619140625
 Trial: 55 Epoch: 13, train_loss: 29150.589599609375, validation_loss = 5438.11767578125
 Trial: 55 Epoch: 14, train_loss: 27971.288818359375, validation_loss = 3419.5123291015625
 Trial: 55 Epoch: 15, train_loss: 26698.075805664062, validation_loss = 3846.75341796875
 Trial: 55 Epoch: 16, train_loss: 24641.443969726562, validation_loss = 3655.9710693359375
 Trial: 55 Epoch: 17, train_loss: 24051.925354003906, validation_loss = 4137.638427734375
 Trial: 55 Epoch: 18, train_loss: 26107.980712890625, validation_loss = 3433.85498046875
 Trial: 55 Epoch: 19, train_loss: 26623.697387695312, validation_loss = 3761.8883056640625
 Trial: 55 Epoch: 20, train_loss: 27011.739624023438, validation_loss = 3508.4091796875
 Trial: 55 Epoch: 21, train_loss: 27430.959228515625, validation_loss = 6097.5419921875
 Trial: 55 Epoch: 22, train_loss: 31220.640014648438, validation_loss = 4037.9874267578125
 Trial: 55 Epoch: 23, train_loss: 30613.50311279297, validation_loss = 5492.849853515625
 Trial: 55 Epoch: 24, train_loss: 31174.243530273438, validation_loss = 4179.629638671875
 Trial: 55 Epoch: 25, train_loss: 27418.039184570312, validation_loss = 4690.872802734375
 Trial: 55 Epoch: 26, train_loss: 27648.643920898438, validation_loss = 3380.413818359375
 Trial: 55 Epoch: 27, train_loss: 24381.509216308594, validation_loss = 2928.08251953125
 Trial: 55 Epoch: 28, train_loss: 24272.062866210938, validation_loss = 3429.9796142578125
 Trial: 55 Epoch: 29, train_loss: 25437.888427734375, validation_loss = 3228.16943359375
 Trial: 55 Epoch: 30, train_loss: 25584.978881835938, validation_loss = 4126.4761962890625
 Trial: 55 Epoch: 31, train_loss: 28018.787963867188, validation_loss = 4758.4368896484375
 Trial: 55 Epoch: 32, train_loss: 29409.75830078125, validation_loss = 3261.8077392578125
 Trial: 55 Epoch: 33, train_loss: 23705.969360351562, validation_loss = 3186.5816650390625
 Trial: 55 Epoch: 34, train_loss: 24270.419189453125, validation_loss = 3001.4200439453125
 Trial: 55 Epoch: 35, train_loss: 23565.448486328125, validation_loss = 3427.9375
 Trial: 55 Epoch: 36, train_loss: 24648.907470703125, validation_loss = 3706.6981201171875
 Trial: 55 Epoch: 37, train_loss: 25042.613647460938, validation_loss = 3801.5321044921875
 Trial: 55 Epoch: 38, train_loss: 24234.308349609375, validation_loss = 3161.5233154296875
 Trial: 55 Epoch: 39, train_loss: 22463.266540527344, validation_loss = 2966.8228759765625
 Trial: 55 Epoch: 40, train_loss: 21620.859741210938, validation_loss = 3120.309326171875
 Trial: 55 Epoch: 41, train_loss: 21011.48370361328, validation_loss = 2843.903076171875
 Trial: 55 Epoch: 42, train_loss: 20968.684326171875, validation_loss = 2833.8172607421875
 Trial: 55 Epoch: 43, train_loss: 23630.737670898438, validation_loss = 3279.4759521484375
 Trial: 55 Epoch: 44, train_loss: 23237.759643554688, validation_loss = 3227.968017578125
 Trial: 55 Epoch: 45, train_loss: 23703.09100341797, validation_loss = 3295.03515625
 Trial: 55 Epoch: 46, train_loss: 24189.206420898438, validation_loss = 3733.3104248046875
 Trial: 55 Epoch: 47, train_loss: 22340.938232421875, validation_loss = 2829.33740234375
 Trial: 55 Epoch: 48, train_loss: 22739.315063476562, validation_loss = 2829.326171875
 Trial: 55 Epoch: 49, train_loss: 21227.130004882812, validation_loss = 3655.225341796875
 Trial: 55 Epoch: 50, train_loss: 22052.10009765625, validation_loss = 3047.9622802734375
 Trial: 55 Epoch: 51, train_loss: 21236.021850585938, validation_loss = 2896.0687255859375
 Trial: 55 Epoch: 52, train_loss: 24665.951904296875, validation_loss = 3801.884521484375
 Trial: 55 Epoch: 53, train_loss: 27546.712646484375, validation_loss = 2840.7230224609375
 Trial: 55 Epoch: 54, train_loss: 23131.005737304688, validation_loss = 3058.465087890625
 Trial: 55 Epoch: 55, train_loss: 22920.483642578125, validation_loss = 2980.0906982421875
 Trial: 55 Epoch: 56, train_loss: 23790.29296875, validation_loss = 2642.705078125
 Trial: 55 Epoch: 57, train_loss: 25358.688598632812, validation_loss = 3062.61669921875
 Trial: 55 Epoch: 58, train_loss: 22980.672241210938, validation_loss = 2967.2392578125
 Trial: 55 Epoch: 59, train_loss: 21803.37969970703, validation_loss = 3014.448974609375
 Trial: 55 Epoch: 60, train_loss: 24339.31884765625, validation_loss = 2910.5101318359375
 Trial: 55 Epoch: 61, train_loss: 22278.9072265625, validation_loss = 2997.3846435546875
 Trial: 55 Epoch: 62, train_loss: 21256.231567382812, validation_loss = 2948.9622802734375
 Trial: 55 Epoch: 63, train_loss: 20875.052124023438, validation_loss = 3283.990478515625
 Trial: 55 Epoch: 64, train_loss: 20973.92059326172, validation_loss = 3027.971435546875
 Trial: 55 Epoch: 65, train_loss: 20705.110961914062, validation_loss = 2757.02880859375
 Trial: 55 Epoch: 66, train_loss: 21139.392517089844, validation_loss = 3391.479248046875
 Trial: 55 Epoch: 67, train_loss: 24073.785034179688, validation_loss = 2735.4168701171875
 Trial: 55 Epoch: 68, train_loss: 22663.117309570312, validation_loss = 2863.7574462890625
 Trial: 55 Epoch: 69, train_loss: 20633.146850585938, validation_loss = 3258.503662109375
 Trial: 55 Epoch: 70, train_loss: 21278.85382080078, validation_loss = 3609.406982421875
 Trial: 55 Epoch: 71, train_loss: 20804.320678710938, validation_loss = 2784.671142578125
 Trial: 55 Epoch: 72, train_loss: 20197.955932617188, validation_loss = 2831.869384765625
 Trial: 55 Epoch: 73, train_loss: 20647.478149414062, validation_loss = 3159.6849365234375
 Trial: 55 Epoch: 74, train_loss: 20816.524353027344, validation_loss = 3405.2698974609375
 Trial: 55 Epoch: 75, train_loss: 22097.995239257812, validation_loss = 3784.6363525390625
 Trial: 55 Epoch: 76, train_loss: 22168.961059570312, validation_loss = 3617.9808349609375
 Trial: 55 Epoch: 77, train_loss: 22129.577270507812, validation_loss = 2936.9332275390625
 Trial: 55 Epoch: 78, train_loss: 20151.560119628906, validation_loss = 2807.4693603515625
 Trial: 55 Epoch: 79, train_loss: 22416.16375732422, validation_loss = 4037.7242431640625
 Trial: 55 Epoch: 80, train_loss: 21029.665893554688, validation_loss = 2815.517578125
 Trial: 55 Epoch: 81, train_loss: 19540.714111328125, validation_loss = 2872.515869140625
 Trial: 55 Epoch: 82, train_loss: 19879.906982421875, validation_loss = 3404.8626708984375
 Trial: 55 Epoch: 83, train_loss: 19689.77801513672, validation_loss = 2959.358154296875
 Trial: 55 Epoch: 84, train_loss: 20164.016174316406, validation_loss = 2749.19287109375
 Trial: 55 Epoch: 85, train_loss: 21477.1845703125, validation_loss = 3620.1356201171875
 Trial: 55 Epoch: 86, train_loss: 21997.939453125, validation_loss = 3271.419677734375
 Trial: 55 Epoch: 87, train_loss: 24909.836547851562, validation_loss = 4287.9385986328125
 Trial: 55 Epoch: 88, train_loss: 20893.8154296875, validation_loss = 2710.677978515625
 Trial: 55 Epoch: 89, train_loss: 23215.867309570312, validation_loss = 3352.5718994140625
 Trial: 55 Epoch: 90, train_loss: 21157.259155273438, validation_loss = 3536.1209716796875
 Trial: 55 Epoch: 91, train_loss: 21548.07696533203, validation_loss = 2911.0504150390625
 Trial: 55 Epoch: 92, train_loss: 20513.803833007812, validation_loss = 2803.353271484375
 Trial: 55 Epoch: 93, train_loss: 21299.342041015625, validation_loss = 2869.889404296875
 Trial: 55 Epoch: 94, train_loss: 23165.9619140625, validation_loss = 3537.304931640625
 Trial: 55 Epoch: 95, train_loss: 23457.859497070312, validation_loss = 3134.67236328125
 Trial: 55 Epoch: 96, train_loss: 23506.252197265625, validation_loss = 3080.6728515625
 Trial: 55 Epoch: 97, train_loss: 20897.20654296875, validation_loss = 2969.0582275390625
 Trial: 55 Epoch: 98, train_loss: 19899.691284179688, validation_loss = 2828.861572265625
 Trial: 55 Epoch: 99, train_loss: 20084.936279296875, validation_loss = 3194.69873046875
========Trial 56 params: {'n_layers': 1, 'K': 2, 'Dropout': 0.4, 'Hidden0': 256, 'optimizer': 'Adam', 'lr': 0.012400175633158698, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |   12288    |
|    sptH.0.gcn.bias     |    256     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |    3072    |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    6144    |
|    sptD.0.gcn.bias     |    256     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |    3072    |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |   12288    |
|    sptW.0.gcn.bias     |    256     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |    3072    |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 45285
 Trial: 56 Epoch: 0, train_loss: 133711.4306640625, validation_loss = 5858.283203125
 Trial: 56 Epoch: 1, train_loss: 43875.02001953125, validation_loss = 5269.236572265625
 Trial: 56 Epoch: 2, train_loss: 37100.23779296875, validation_loss = 4798.447265625
 Trial: 56 Epoch: 3, train_loss: 43587.800537109375, validation_loss = 5743.45458984375
 Trial: 56 Epoch: 4, train_loss: 57999.91247558594, validation_loss = 4226.6015625
 Trial: 56 Epoch: 5, train_loss: 42775.87255859375, validation_loss = 6641.0177001953125
 Trial: 56 Epoch: 6, train_loss: 30547.921630859375, validation_loss = 4743.4761962890625
 Trial: 56 Epoch: 7, train_loss: 29748.533813476562, validation_loss = 4206.620849609375
 Trial: 56 Epoch: 8, train_loss: 27760.737548828125, validation_loss = 3760.6566162109375
 Trial: 56 Epoch: 9, train_loss: 25821.844360351562, validation_loss = 3540.0458984375
 Trial: 56 Epoch: 10, train_loss: 26957.493774414062, validation_loss = 4990.589599609375
 Trial: 56 Epoch: 11, train_loss: 30111.401245117188, validation_loss = 5498.3046875
 Trial: 56 Epoch: 12, train_loss: 30398.627319335938, validation_loss = 3571.8514404296875
 Trial: 56 Epoch: 13, train_loss: 28679.695434570312, validation_loss = 5558.750244140625
 Trial: 56 Epoch: 14, train_loss: 27970.703979492188, validation_loss = 3824.5537109375
 Trial: 56 Epoch: 15, train_loss: 24986.075073242188, validation_loss = 3722.8760986328125
 Trial: 56 Epoch: 16, train_loss: 23737.298889160156, validation_loss = 3560.18359375
 Trial: 56 Epoch: 17, train_loss: 22734.10076904297, validation_loss = 4099.7357177734375
 Trial: 56 Epoch: 18, train_loss: 25512.167236328125, validation_loss = 3361.7144775390625
 Trial: 56 Epoch: 19, train_loss: 25208.317504882812, validation_loss = 3680.579833984375
 Trial: 56 Epoch: 20, train_loss: 25222.7314453125, validation_loss = 3373.7333984375
 Trial: 56 Epoch: 21, train_loss: 26332.548095703125, validation_loss = 5635.161376953125
 Trial: 56 Epoch: 22, train_loss: 27547.63702392578, validation_loss = 3554.19482421875
 Trial: 56 Epoch: 23, train_loss: 26418.07012939453, validation_loss = 4454.398193359375
 Trial: 56 Epoch: 24, train_loss: 26174.33203125, validation_loss = 3461.5660400390625
 Trial: 56 Epoch: 25, train_loss: 23182.410034179688, validation_loss = 3586.6080322265625
 Trial: 56 Epoch: 26, train_loss: 24009.268005371094, validation_loss = 3198.404052734375
 Trial: 56 Epoch: 27, train_loss: 22206.776123046875, validation_loss = 2959.4647216796875
 Trial: 56 Epoch: 28, train_loss: 23805.014404296875, validation_loss = 2948.9554443359375
 Trial: 56 Epoch: 29, train_loss: 25188.58428955078, validation_loss = 3208.0830078125
 Trial: 56 Epoch: 30, train_loss: 22495.535705566406, validation_loss = 4720.9920654296875
 Trial: 56 Epoch: 31, train_loss: 24560.824584960938, validation_loss = 4033.921875
 Trial: 56 Epoch: 32, train_loss: 25999.911987304688, validation_loss = 3220.5025634765625
 Trial: 56 Epoch: 33, train_loss: 21736.02947998047, validation_loss = 3278.4425048828125
 Trial: 56 Epoch: 34, train_loss: 23735.638549804688, validation_loss = 2918.800537109375
 Trial: 56 Epoch: 35, train_loss: 21661.163330078125, validation_loss = 3456.0548095703125
 Trial: 56 Epoch: 36, train_loss: 23753.214477539062, validation_loss = 3256.465087890625
 Trial: 56 Epoch: 37, train_loss: 23202.124389648438, validation_loss = 3389.6063232421875
 Trial: 56 Epoch: 38, train_loss: 21883.72442626953, validation_loss = 3024.3372802734375
 Trial: 56 Epoch: 39, train_loss: 20483.20428466797, validation_loss = 3039.0439453125
 Trial: 56 Epoch: 40, train_loss: 21223.84930419922, validation_loss = 2952.4564208984375
 Trial: 56 Epoch: 41, train_loss: 20001.86395263672, validation_loss = 2781.6396484375
 Trial: 56 Epoch: 42, train_loss: 19601.205444335938, validation_loss = 2703.5980224609375
 Trial: 56 Epoch: 43, train_loss: 21841.560485839844, validation_loss = 3152.6951904296875
 Trial: 56 Epoch: 44, train_loss: 20668.8447265625, validation_loss = 3461.396240234375
 Trial: 56 Epoch: 45, train_loss: 21000.07110595703, validation_loss = 3320.4637451171875
 Trial: 56 Epoch: 46, train_loss: 21432.089599609375, validation_loss = 3924.7598876953125
 Trial: 56 Epoch: 47, train_loss: 20567.232543945312, validation_loss = 2818.177734375
 Trial: 56 Epoch: 48, train_loss: 21312.123168945312, validation_loss = 2975.9013671875
 Trial: 56 Epoch: 49, train_loss: 19675.743041992188, validation_loss = 3860.6146240234375
 Trial: 56 Epoch: 50, train_loss: 21420.682006835938, validation_loss = 3193.2381591796875
 Trial: 56 Epoch: 51, train_loss: 20215.895568847656, validation_loss = 3454.9486083984375
 Trial: 56 Epoch: 52, train_loss: 21724.126708984375, validation_loss = 3743.236083984375
 Trial: 56 Epoch: 53, train_loss: 24105.833129882812, validation_loss = 2669.43212890625
 Trial: 56 Epoch: 54, train_loss: 23121.101196289062, validation_loss = 3066.7781982421875
 Trial: 56 Epoch: 55, train_loss: 21422.35174560547, validation_loss = 2903.3175048828125
 Trial: 56 Epoch: 56, train_loss: 20360.11572265625, validation_loss = 2830.011962890625
 Trial: 56 Epoch: 57, train_loss: 22512.072875976562, validation_loss = 3016.2587890625
 Trial: 56 Epoch: 58, train_loss: 21952.438110351562, validation_loss = 2828.874755859375
 Trial: 56 Epoch: 59, train_loss: 20269.121948242188, validation_loss = 2644.20458984375
 Trial: 56 Epoch: 60, train_loss: 20751.68017578125, validation_loss = 2688.7471923828125
 Trial: 56 Epoch: 61, train_loss: 21526.47265625, validation_loss = 3296.877197265625
 Trial: 56 Epoch: 62, train_loss: 20111.61279296875, validation_loss = 2939.0863037109375
 Trial: 56 Epoch: 63, train_loss: 19074.430297851562, validation_loss = 3033.93408203125
 Trial: 56 Epoch: 64, train_loss: 20176.800659179688, validation_loss = 3240.178466796875
 Trial: 56 Epoch: 65, train_loss: 20396.616455078125, validation_loss = 2945.080078125
 Trial: 56 Epoch: 66, train_loss: 20604.572814941406, validation_loss = 3334.698486328125
 Trial: 56 Epoch: 67, train_loss: 20316.45819091797, validation_loss = 2681.2098388671875
 Trial: 56 Epoch: 68, train_loss: 19591.868286132812, validation_loss = 2868.3837890625
 Trial: 56 Epoch: 69, train_loss: 18206.07550048828, validation_loss = 2813.9490966796875
 Trial: 56 Epoch: 70, train_loss: 19938.470947265625, validation_loss = 3721.099853515625
 Trial: 56 Epoch: 71, train_loss: 18236.184509277344, validation_loss = 2696.5985107421875
 Trial: 56 Epoch: 72, train_loss: 18286.443481445312, validation_loss = 2760.9117431640625
 Trial: 56 Epoch: 73, train_loss: 19522.280395507812, validation_loss = 3226.585205078125
 Trial: 56 Epoch: 74, train_loss: 18450.272521972656, validation_loss = 3067.54150390625
 Trial: 56 Epoch: 75, train_loss: 18875.864379882812, validation_loss = 3109.202880859375
 Trial: 56 Epoch: 76, train_loss: 20817.43914794922, validation_loss = 3597.9453125
 Trial: 56 Epoch: 77, train_loss: 20065.397216796875, validation_loss = 2955.8367919921875
 Trial: 56 Epoch: 78, train_loss: 18504.29248046875, validation_loss = 2710.7037353515625
 Trial: 56 Epoch: 79, train_loss: 22269.76934814453, validation_loss = 3989.2554931640625
 Trial: 56 Epoch: 80, train_loss: 19682.909545898438, validation_loss = 2946.6400146484375
 Trial: 56 Epoch: 81, train_loss: 18837.647033691406, validation_loss = 3153.1685791015625
 Trial: 56 Epoch: 82, train_loss: 19116.56866455078, validation_loss = 3387.30224609375
 Trial: 56 Epoch: 83, train_loss: 19601.550170898438, validation_loss = 2875.7962646484375
 Trial: 56 Epoch: 84, train_loss: 18343.645446777344, validation_loss = 2723.04541015625
 Trial: 56 Epoch: 85, train_loss: 18135.66632080078, validation_loss = 2922.195556640625
 Trial: 56 Epoch: 86, train_loss: 19499.797973632812, validation_loss = 2844.4027099609375
 Trial: 56 Epoch: 87, train_loss: 21807.38299560547, validation_loss = 3943.0523681640625
 Trial: 56 Epoch: 88, train_loss: 19161.732543945312, validation_loss = 2669.311767578125
 Trial: 56 Epoch: 89, train_loss: 20463.755493164062, validation_loss = 3011.505126953125
 Trial: 56 Epoch: 90, train_loss: 18917.939697265625, validation_loss = 3710.489501953125
 Trial: 56 Epoch: 91, train_loss: 19439.215270996094, validation_loss = 2926.2618408203125
 Trial: 56 Epoch: 92, train_loss: 19000.527587890625, validation_loss = 2972.4854736328125
 Trial: 56 Epoch: 93, train_loss: 18917.039306640625, validation_loss = 2655.5743408203125
 Trial: 56 Epoch: 94, train_loss: 21468.727294921875, validation_loss = 3406.1658935546875
 Trial: 56 Epoch: 95, train_loss: 21394.065307617188, validation_loss = 2891.1275634765625
 Trial: 56 Epoch: 96, train_loss: 19895.102478027344, validation_loss = 3055.5853271484375
 Trial: 56 Epoch: 97, train_loss: 18967.111938476562, validation_loss = 2745.4268798828125
 Trial: 56 Epoch: 98, train_loss: 18656.534545898438, validation_loss = 2736.6207275390625
 Trial: 56 Epoch: 99, train_loss: 18583.117797851562, validation_loss = 3165.9268798828125
========Trial 57 params: {'n_layers': 1, 'K': 2, 'Dropout': 0.1, 'Hidden0': 4, 'optimizer': 'Adam', 'lr': 0.01771728142883808, 'batch': 8}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |    192     |
|    sptH.0.gcn.bias     |     4      |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |     48     |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |     96     |
|    sptD.0.gcn.bias     |     4      |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |     48     |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |    192     |
|    sptW.0.gcn.bias     |     4      |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |     48     |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 5217
 Trial: 57 Epoch: 0, train_loss: 340411.1953125, validation_loss = 23816.546875
========Trial 58 params: {'n_layers': 1, 'K': 2, 'Dropout': 0.2, 'Hidden0': 256, 'optimizer': 'Adam', 'lr': 0.015115038711461795, 'batch': 16}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |   12288    |
|    sptH.0.gcn.bias     |    256     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |    3072    |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    6144    |
|    sptD.0.gcn.bias     |    256     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |    3072    |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |   12288    |
|    sptW.0.gcn.bias     |    256     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |    3072    |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 45285
 Trial: 58 Epoch: 0, train_loss: 158368.326171875, validation_loss = 12883.548828125
========Trial 59 params: {'n_layers': 1, 'K': 4, 'Dropout': 0.2, 'Hidden0': 256, 'optimizer': 'Adam', 'lr': 0.026806023586566682, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |   24576    |
|    sptH.0.gcn.bias     |    256     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |    3072    |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |   12288    |
|    sptD.0.gcn.bias     |    256     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |    3072    |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |   24576    |
|    sptW.0.gcn.bias     |    256     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |    3072    |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 76005
 Trial: 59 Epoch: 0, train_loss: 358713.48974609375, validation_loss = 14984.760498046875
========Trial 60 params: {'n_layers': 1, 'K': 2, 'Dropout': 0.2, 'Hidden0': 256, 'optimizer': 'Adam', 'lr': 0.019599245712406788, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |   12288    |
|    sptH.0.gcn.bias     |    256     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |    3072    |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    6144    |
|    sptD.0.gcn.bias     |    256     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |    3072    |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |   12288    |
|    sptW.0.gcn.bias     |    256     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |    3072    |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 45285
 Trial: 60 Epoch: 0, train_loss: 90342.65197753906, validation_loss = 5287.430419921875
 Trial: 60 Epoch: 1, train_loss: 53113.57080078125, validation_loss = 5342.283447265625
 Trial: 60 Epoch: 2, train_loss: 35995.4541015625, validation_loss = 5398.769775390625
 Trial: 60 Epoch: 3, train_loss: 48159.017639160156, validation_loss = 4738.583984375
 Trial: 60 Epoch: 4, train_loss: 71957.81494140625, validation_loss = 4375.65283203125
 Trial: 60 Epoch: 5, train_loss: 58084.68798828125, validation_loss = 11527.958251953125
 Trial: 60 Epoch: 6, train_loss: 32849.07861328125, validation_loss = 5863.0943603515625
 Trial: 60 Epoch: 7, train_loss: 28917.1953125, validation_loss = 4783.83251953125
 Trial: 60 Epoch: 8, train_loss: 28033.52685546875, validation_loss = 3902.5001220703125
 Trial: 60 Epoch: 9, train_loss: 26764.34149169922, validation_loss = 3479.768310546875
 Trial: 60 Epoch: 10, train_loss: 27197.682373046875, validation_loss = 4730.0609130859375
 Trial: 60 Epoch: 11, train_loss: 26592.511962890625, validation_loss = 4331.3636474609375
 Trial: 60 Epoch: 12, train_loss: 27511.849975585938, validation_loss = 3237.005859375
 Trial: 60 Epoch: 13, train_loss: 26321.598266601562, validation_loss = 5691.3316650390625
 Trial: 60 Epoch: 14, train_loss: 26700.846435546875, validation_loss = 3285.3612060546875
 Trial: 60 Epoch: 15, train_loss: 23912.876098632812, validation_loss = 3737.4459228515625
 Trial: 60 Epoch: 16, train_loss: 22326.920166015625, validation_loss = 3308.23876953125
 Trial: 60 Epoch: 17, train_loss: 22103.370727539062, validation_loss = 3833.8673095703125
 Trial: 60 Epoch: 18, train_loss: 24319.74755859375, validation_loss = 3320.318115234375
 Trial: 60 Epoch: 19, train_loss: 24092.608154296875, validation_loss = 3391.16650390625
 Trial: 60 Epoch: 20, train_loss: 24731.09747314453, validation_loss = 3384.10791015625
 Trial: 60 Epoch: 21, train_loss: 24142.212158203125, validation_loss = 4977.151123046875
 Trial: 60 Epoch: 22, train_loss: 27621.603454589844, validation_loss = 3878.39892578125
 Trial: 60 Epoch: 23, train_loss: 25834.795837402344, validation_loss = 4749.4266357421875
 Trial: 60 Epoch: 24, train_loss: 30078.32373046875, validation_loss = 4348.299072265625
 Trial: 60 Epoch: 25, train_loss: 26454.234375, validation_loss = 4645.0242919921875
 Trial: 60 Epoch: 26, train_loss: 25672.693237304688, validation_loss = 3380.1923828125
 Trial: 60 Epoch: 27, train_loss: 24076.786560058594, validation_loss = 3148.752197265625
 Trial: 60 Epoch: 28, train_loss: 25622.08642578125, validation_loss = 3202.2486572265625
 Trial: 60 Epoch: 29, train_loss: 28591.201171875, validation_loss = 3394.4365234375
 Trial: 60 Epoch: 30, train_loss: 26035.766967773438, validation_loss = 4062.681640625
 Trial: 60 Epoch: 31, train_loss: 25960.9560546875, validation_loss = 4740.5244140625
 Trial: 60 Epoch: 32, train_loss: 27984.98858642578, validation_loss = 3267.92529296875
 Trial: 60 Epoch: 33, train_loss: 22047.937255859375, validation_loss = 3295.2008056640625
 Trial: 60 Epoch: 34, train_loss: 22286.367065429688, validation_loss = 2927.15869140625
 Trial: 60 Epoch: 35, train_loss: 21487.461303710938, validation_loss = 3279.816162109375
 Trial: 60 Epoch: 36, train_loss: 23463.55712890625, validation_loss = 3127.595703125
 Trial: 60 Epoch: 37, train_loss: 22567.623657226562, validation_loss = 3580.518798828125
 Trial: 60 Epoch: 38, train_loss: 22456.124389648438, validation_loss = 3124.0115966796875
 Trial: 60 Epoch: 39, train_loss: 20194.98809814453, validation_loss = 2896.2633056640625
 Trial: 60 Epoch: 40, train_loss: 19604.626098632812, validation_loss = 3028.124267578125
 Trial: 60 Epoch: 41, train_loss: 19265.292541503906, validation_loss = 2559.049072265625
 Trial: 60 Epoch: 42, train_loss: 20299.910400390625, validation_loss = 2662.369140625
 Trial: 60 Epoch: 43, train_loss: 21220.0283203125, validation_loss = 3140.135009765625
 Trial: 60 Epoch: 44, train_loss: 20169.83221435547, validation_loss = 3150.4710693359375
 Trial: 60 Epoch: 45, train_loss: 21199.562438964844, validation_loss = 3163.1099853515625
 Trial: 60 Epoch: 46, train_loss: 20664.307739257812, validation_loss = 4027.260009765625
 Trial: 60 Epoch: 47, train_loss: 20314.668029785156, validation_loss = 2507.9185791015625
 Trial: 60 Epoch: 48, train_loss: 20543.457092285156, validation_loss = 2720.54052734375
 Trial: 60 Epoch: 49, train_loss: 18973.584106445312, validation_loss = 3804.8408203125
 Trial: 60 Epoch: 50, train_loss: 21390.443115234375, validation_loss = 3169.84716796875
 Trial: 60 Epoch: 51, train_loss: 20002.081298828125, validation_loss = 3146.7059326171875
 Trial: 60 Epoch: 52, train_loss: 20972.039916992188, validation_loss = 3528.0916748046875
 Trial: 60 Epoch: 53, train_loss: 23392.271850585938, validation_loss = 2617.359130859375
 Trial: 60 Epoch: 54, train_loss: 21339.362854003906, validation_loss = 2773.9398193359375
 Trial: 60 Epoch: 55, train_loss: 20099.147216796875, validation_loss = 2883.794921875
 Trial: 60 Epoch: 56, train_loss: 19368.34356689453, validation_loss = 2659.8782958984375
 Trial: 60 Epoch: 57, train_loss: 20033.494018554688, validation_loss = 3220.0087890625
 Trial: 60 Epoch: 58, train_loss: 22058.024536132812, validation_loss = 3420.086669921875
 Trial: 60 Epoch: 59, train_loss: 21060.730346679688, validation_loss = 2817.7926025390625
 Trial: 60 Epoch: 60, train_loss: 18994.721252441406, validation_loss = 2595.341064453125
 Trial: 60 Epoch: 61, train_loss: 19530.861694335938, validation_loss = 3363.533203125
 Trial: 60 Epoch: 62, train_loss: 19020.937072753906, validation_loss = 2923.9649658203125
 Trial: 60 Epoch: 63, train_loss: 18645.82928466797, validation_loss = 3129.2391357421875
 Trial: 60 Epoch: 64, train_loss: 18352.448181152344, validation_loss = 3081.113037109375
 Trial: 60 Epoch: 65, train_loss: 18388.868713378906, validation_loss = 2670.527099609375
 Trial: 60 Epoch: 66, train_loss: 17814.91796875, validation_loss = 2874.1258544921875
 Trial: 60 Epoch: 67, train_loss: 21209.822021484375, validation_loss = 4040.8150634765625
 Trial: 60 Epoch: 68, train_loss: 22861.46142578125, validation_loss = 2757.3798828125
 Trial: 60 Epoch: 69, train_loss: 19701.503662109375, validation_loss = 3048.504150390625
 Trial: 60 Epoch: 70, train_loss: 21320.40838623047, validation_loss = 3551.9189453125
 Trial: 60 Epoch: 71, train_loss: 18720.63507080078, validation_loss = 2613.9224853515625
 Trial: 60 Epoch: 72, train_loss: 18245.074279785156, validation_loss = 2600.1575927734375
 Trial: 60 Epoch: 73, train_loss: 18828.94219970703, validation_loss = 2948.46337890625
 Trial: 60 Epoch: 74, train_loss: 17891.92645263672, validation_loss = 2748.2386474609375
 Trial: 60 Epoch: 75, train_loss: 18921.25848388672, validation_loss = 3257.9730224609375
 Trial: 60 Epoch: 76, train_loss: 20610.860412597656, validation_loss = 4918.78857421875
 Trial: 60 Epoch: 77, train_loss: 22041.88427734375, validation_loss = 2655.8206787109375
 Trial: 60 Epoch: 78, train_loss: 18414.168579101562, validation_loss = 2735.8328857421875
 Trial: 60 Epoch: 79, train_loss: 23862.694396972656, validation_loss = 4294.080078125
 Trial: 60 Epoch: 80, train_loss: 19147.544921875, validation_loss = 2671.2451171875
 Trial: 60 Epoch: 81, train_loss: 18637.12127685547, validation_loss = 3168.121826171875
 Trial: 60 Epoch: 82, train_loss: 19718.63409423828, validation_loss = 3930.4415283203125
 Trial: 60 Epoch: 83, train_loss: 17899.92401123047, validation_loss = 2790.4404296875
 Trial: 60 Epoch: 84, train_loss: 19093.62841796875, validation_loss = 3042.5927734375
 Trial: 60 Epoch: 85, train_loss: 18325.542053222656, validation_loss = 3094.6728515625
 Trial: 60 Epoch: 86, train_loss: 18653.28338623047, validation_loss = 2809.35400390625
 Trial: 60 Epoch: 87, train_loss: 21443.760192871094, validation_loss = 4595.7823486328125
 Trial: 60 Epoch: 88, train_loss: 20053.656677246094, validation_loss = 2577.3043212890625
 Trial: 60 Epoch: 89, train_loss: 21509.156982421875, validation_loss = 2850.265869140625
 Trial: 60 Epoch: 90, train_loss: 20165.75616455078, validation_loss = 4546.124755859375
 Trial: 60 Epoch: 91, train_loss: 21168.93682861328, validation_loss = 2657.1632080078125
 Trial: 60 Epoch: 92, train_loss: 18831.499145507812, validation_loss = 2973.463134765625
 Trial: 60 Epoch: 93, train_loss: 18881.17010498047, validation_loss = 2567.315185546875
 Trial: 60 Epoch: 94, train_loss: 19148.188354492188, validation_loss = 3361.9471435546875
 Trial: 60 Epoch: 95, train_loss: 20347.683654785156, validation_loss = 3364.6116943359375
 Trial: 60 Epoch: 96, train_loss: 24221.880920410156, validation_loss = 2573.6273193359375
 Trial: 60 Epoch: 97, train_loss: 17587.803344726562, validation_loss = 2611.29150390625
 Trial: 60 Epoch: 98, train_loss: 18127.751953125, validation_loss = 2518.926513671875
 Trial: 60 Epoch: 99, train_loss: 19692.132690429688, validation_loss = 3039.89990234375
========Trial 61 params: {'n_layers': 1, 'K': 2, 'Dropout': 0.2, 'Hidden0': 256, 'optimizer': 'Adam', 'lr': 0.03753357984784123, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |   12288    |
|    sptH.0.gcn.bias     |    256     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |    3072    |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    6144    |
|    sptD.0.gcn.bias     |    256     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |    3072    |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |   12288    |
|    sptW.0.gcn.bias     |    256     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |    3072    |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 45285
 Trial: 61 Epoch: 0, train_loss: 278648.69580078125, validation_loss = 5940.67578125
 Trial: 61 Epoch: 1, train_loss: 53827.1796875, validation_loss = 6941.61279296875
 Trial: 61 Epoch: 2, train_loss: 51451.0927734375, validation_loss = 5049.114501953125
 Trial: 61 Epoch: 3, train_loss: 64353.939208984375, validation_loss = 6530.16259765625
 Trial: 61 Epoch: 4, train_loss: 68179.87329101562, validation_loss = 4482.188720703125
 Trial: 61 Epoch: 5, train_loss: 48852.18212890625, validation_loss = 9255.072021484375
 Trial: 61 Epoch: 6, train_loss: 38776.537109375, validation_loss = 6679.3212890625
 Trial: 61 Epoch: 7, train_loss: 34754.6826171875, validation_loss = 4899.578369140625
 Trial: 61 Epoch: 8, train_loss: 32083.546508789062, validation_loss = 4168.52587890625
 Trial: 61 Epoch: 9, train_loss: 30898.800903320312, validation_loss = 3763.9964599609375
 Trial: 61 Epoch: 10, train_loss: 31647.740112304688, validation_loss = 5917.1474609375
 Trial: 61 Epoch: 11, train_loss: 32820.62854003906, validation_loss = 5123.7216796875
 Trial: 61 Epoch: 12, train_loss: 32670.652709960938, validation_loss = 5074.317138671875
 Trial: 61 Epoch: 13, train_loss: 32038.651245117188, validation_loss = 4224.600830078125
 Trial: 61 Epoch: 14, train_loss: 28624.662841796875, validation_loss = 3703.8729248046875
 Trial: 61 Epoch: 15, train_loss: 30632.82861328125, validation_loss = 4437.247802734375
 Trial: 61 Epoch: 16, train_loss: 28413.320922851562, validation_loss = 4068.537109375
 Trial: 61 Epoch: 17, train_loss: 27526.01806640625, validation_loss = 4225.1944580078125
 Trial: 61 Epoch: 18, train_loss: 26856.418701171875, validation_loss = 3215.0218505859375
 Trial: 61 Epoch: 19, train_loss: 26368.08056640625, validation_loss = 4299.479736328125
 Trial: 61 Epoch: 20, train_loss: 26655.385375976562, validation_loss = 3675.8470458984375
 Trial: 61 Epoch: 21, train_loss: 26842.927124023438, validation_loss = 3388.8541259765625
 Trial: 61 Epoch: 22, train_loss: 26452.9833984375, validation_loss = 3756.2657470703125
 Trial: 61 Epoch: 23, train_loss: 27942.022094726562, validation_loss = 3722.017333984375
 Trial: 61 Epoch: 24, train_loss: 30404.214965820312, validation_loss = 3415.0010986328125
 Trial: 61 Epoch: 25, train_loss: 30605.6689453125, validation_loss = 3283.34716796875
 Trial: 61 Epoch: 26, train_loss: 26992.82763671875, validation_loss = 3330.8433837890625
 Trial: 61 Epoch: 27, train_loss: 26756.330078125, validation_loss = 3140.6961669921875
 Trial: 61 Epoch: 28, train_loss: 30024.7255859375, validation_loss = 3479.872314453125
 Trial: 61 Epoch: 29, train_loss: 26824.21484375, validation_loss = 3540.055419921875
 Trial: 61 Epoch: 30, train_loss: 29132.652954101562, validation_loss = 6278.45751953125
 Trial: 61 Epoch: 31, train_loss: 27643.629272460938, validation_loss = 3259.8436279296875
 Trial: 61 Epoch: 32, train_loss: 26364.458740234375, validation_loss = 3685.585205078125
 Trial: 61 Epoch: 33, train_loss: 25822.8828125, validation_loss = 3848.6182861328125
 Trial: 61 Epoch: 34, train_loss: 26580.678344726562, validation_loss = 6211.7666015625
 Trial: 61 Epoch: 35, train_loss: 30048.435424804688, validation_loss = 3988.08203125
 Trial: 61 Epoch: 36, train_loss: 31603.203735351562, validation_loss = 3260.80322265625
 Trial: 61 Epoch: 37, train_loss: 29252.675903320312, validation_loss = 3304.2896728515625
 Trial: 61 Epoch: 38, train_loss: 25974.048950195312, validation_loss = 3116.732421875
 Trial: 61 Epoch: 39, train_loss: 25298.064880371094, validation_loss = 3626.203857421875
 Trial: 61 Epoch: 40, train_loss: 25197.392211914062, validation_loss = 4510.0751953125
 Trial: 61 Epoch: 41, train_loss: 25178.882080078125, validation_loss = 3365.4630126953125
 Trial: 61 Epoch: 42, train_loss: 24606.975463867188, validation_loss = 3347.793212890625
 Trial: 61 Epoch: 43, train_loss: 25151.21875, validation_loss = 2866.8380126953125
 Trial: 61 Epoch: 44, train_loss: 25292.473876953125, validation_loss = 4627.14404296875
 Trial: 61 Epoch: 45, train_loss: 24003.44891357422, validation_loss = 3266.5589599609375
 Trial: 61 Epoch: 46, train_loss: 22617.02197265625, validation_loss = 4910.450439453125
 Trial: 61 Epoch: 47, train_loss: 25350.237182617188, validation_loss = 3622.8292236328125
 Trial: 61 Epoch: 48, train_loss: 23070.681762695312, validation_loss = 3311.7398681640625
 Trial: 61 Epoch: 49, train_loss: 23258.880981445312, validation_loss = 5179.25634765625
 Trial: 61 Epoch: 50, train_loss: 26081.75537109375, validation_loss = 4501.3043212890625
 Trial: 61 Epoch: 51, train_loss: 24117.861694335938, validation_loss = 3993.126953125
 Trial: 61 Epoch: 52, train_loss: 24645.895751953125, validation_loss = 3172.6842041015625
 Trial: 61 Epoch: 53, train_loss: 25672.233032226562, validation_loss = 3911.9052734375
 Trial: 61 Epoch: 54, train_loss: 27224.03076171875, validation_loss = 3349.9891357421875
 Trial: 61 Epoch: 55, train_loss: 25209.791870117188, validation_loss = 3453.3955078125
 Trial: 61 Epoch: 56, train_loss: 24341.718627929688, validation_loss = 3693.90185546875
 Trial: 61 Epoch: 57, train_loss: 26860.58642578125, validation_loss = 3766.4588623046875
 Trial: 61 Epoch: 58, train_loss: 24079.071166992188, validation_loss = 3768.8182373046875
 Trial: 61 Epoch: 59, train_loss: 23655.454956054688, validation_loss = 3516.402587890625
 Trial: 61 Epoch: 60, train_loss: 25942.331420898438, validation_loss = 4399.1937255859375
 Trial: 61 Epoch: 61, train_loss: 26420.8486328125, validation_loss = 3111.3734130859375
 Trial: 61 Epoch: 62, train_loss: 25058.463745117188, validation_loss = 3901.5247802734375
 Trial: 61 Epoch: 63, train_loss: 24354.93359375, validation_loss = 5695.869873046875
 Trial: 61 Epoch: 64, train_loss: 26029.462036132812, validation_loss = 4409.95703125
 Trial: 61 Epoch: 65, train_loss: 24184.032348632812, validation_loss = 3831.482177734375
 Trial: 61 Epoch: 66, train_loss: 23551.800903320312, validation_loss = 3804.928955078125
 Trial: 61 Epoch: 67, train_loss: 26045.217529296875, validation_loss = 5455.0870361328125
 Trial: 61 Epoch: 68, train_loss: 25273.101318359375, validation_loss = 3573.9683837890625
 Trial: 61 Epoch: 69, train_loss: 23117.620056152344, validation_loss = 3028.9281005859375
 Trial: 61 Epoch: 70, train_loss: 27783.265686035156, validation_loss = 3833.5614013671875
 Trial: 61 Epoch: 71, train_loss: 24991.960693359375, validation_loss = 4730.7362060546875
 Trial: 61 Epoch: 72, train_loss: 24144.471801757812, validation_loss = 3593.24462890625
 Trial: 61 Epoch: 73, train_loss: 26050.04150390625, validation_loss = 3178.89501953125
 Trial: 61 Epoch: 74, train_loss: 24728.539184570312, validation_loss = 3545.54248046875
 Trial: 61 Epoch: 75, train_loss: 25489.327270507812, validation_loss = 3636.3251953125
 Trial: 61 Epoch: 76, train_loss: 25674.99658203125, validation_loss = 5249.08935546875
 Trial: 61 Epoch: 77, train_loss: 28748.319580078125, validation_loss = 6142.350341796875
 Trial: 61 Epoch: 78, train_loss: 25095.697875976562, validation_loss = 3059.43310546875
 Trial: 61 Epoch: 79, train_loss: 26946.164672851562, validation_loss = 3763.22900390625
 Trial: 61 Epoch: 80, train_loss: 28068.336669921875, validation_loss = 3491.85009765625
 Trial: 61 Epoch: 81, train_loss: 26300.658569335938, validation_loss = 4282.62060546875
 Trial: 61 Epoch: 82, train_loss: 24379.742797851562, validation_loss = 4086.0052490234375
 Trial: 61 Epoch: 83, train_loss: 25625.25244140625, validation_loss = 4574.2550048828125
 Trial: 61 Epoch: 84, train_loss: 23252.343017578125, validation_loss = 4145.8016357421875
 Trial: 61 Epoch: 85, train_loss: 22664.1748046875, validation_loss = 3760.5404052734375
 Trial: 61 Epoch: 86, train_loss: 23780.114990234375, validation_loss = 3256.5067138671875
 Trial: 61 Epoch: 87, train_loss: 23638.990478515625, validation_loss = 3366.663818359375
 Trial: 61 Epoch: 88, train_loss: 23094.260498046875, validation_loss = 3522.2342529296875
 Trial: 61 Epoch: 89, train_loss: 24958.1806640625, validation_loss = 3778.7010498046875
 Trial: 61 Epoch: 90, train_loss: 24855.102661132812, validation_loss = 3941.075927734375
 Trial: 61 Epoch: 91, train_loss: 25174.136352539062, validation_loss = 3663.382080078125
 Trial: 61 Epoch: 92, train_loss: 23016.910400390625, validation_loss = 3156.179931640625
 Trial: 61 Epoch: 93, train_loss: 23946.184936523438, validation_loss = 2914.027587890625
 Trial: 61 Epoch: 94, train_loss: 22893.105834960938, validation_loss = 3330.49755859375
 Trial: 61 Epoch: 95, train_loss: 22665.780029296875, validation_loss = 3186.6053466796875
 Trial: 61 Epoch: 96, train_loss: 27357.507446289062, validation_loss = 3064.641357421875
 Trial: 61 Epoch: 97, train_loss: 24437.485900878906, validation_loss = 3243.7821044921875
 Trial: 61 Epoch: 98, train_loss: 23201.961181640625, validation_loss = 3115.8492431640625
 Trial: 61 Epoch: 99, train_loss: 24934.067749023438, validation_loss = 3400.858642578125
========Trial 62 params: {'n_layers': 1, 'K': 2, 'Dropout': 0.2, 'Hidden0': 256, 'optimizer': 'Adam', 'lr': 0.021137696262613467, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |   12288    |
|    sptH.0.gcn.bias     |    256     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |    3072    |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    6144    |
|    sptD.0.gcn.bias     |    256     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |    3072    |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |   12288    |
|    sptW.0.gcn.bias     |    256     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |    3072    |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 45285
 Trial: 62 Epoch: 0, train_loss: 91792.17504882812, validation_loss = 5409.782470703125
 Trial: 62 Epoch: 1, train_loss: 55540.49011230469, validation_loss = 5230.66943359375
 Trial: 62 Epoch: 2, train_loss: 36099.340087890625, validation_loss = 5763.185302734375
 Trial: 62 Epoch: 3, train_loss: 49708.947204589844, validation_loss = 5057.739501953125
 Trial: 62 Epoch: 4, train_loss: 75280.7607421875, validation_loss = 4317.814697265625
 Trial: 62 Epoch: 5, train_loss: 55105.4013671875, validation_loss = 8497.3447265625
 Trial: 62 Epoch: 6, train_loss: 31417.418823242188, validation_loss = 5421.481201171875
 Trial: 62 Epoch: 7, train_loss: 29546.829711914062, validation_loss = 4633.8697509765625
 Trial: 62 Epoch: 8, train_loss: 27565.846313476562, validation_loss = 4299.8067626953125
 Trial: 62 Epoch: 9, train_loss: 26537.027587890625, validation_loss = 3423.446533203125
 Trial: 62 Epoch: 10, train_loss: 26833.916625976562, validation_loss = 4627.132080078125
 Trial: 62 Epoch: 11, train_loss: 26723.007690429688, validation_loss = 4317.393798828125
 Trial: 62 Epoch: 12, train_loss: 28439.135375976562, validation_loss = 3274.942626953125
 Trial: 62 Epoch: 13, train_loss: 26872.0078125, validation_loss = 5711.984375
 Trial: 62 Epoch: 14, train_loss: 25753.81884765625, validation_loss = 3240.071044921875
 Trial: 62 Epoch: 15, train_loss: 24452.155151367188, validation_loss = 3741.6414794921875
 Trial: 62 Epoch: 16, train_loss: 22911.364318847656, validation_loss = 3348.0362548828125
 Trial: 62 Epoch: 17, train_loss: 22225.48504638672, validation_loss = 4113.5792236328125
 Trial: 62 Epoch: 18, train_loss: 24496.68975830078, validation_loss = 3457.540771484375
 Trial: 62 Epoch: 19, train_loss: 24614.893432617188, validation_loss = 3674.43603515625
 Trial: 62 Epoch: 20, train_loss: 25366.03533935547, validation_loss = 3425.8302001953125
 Trial: 62 Epoch: 21, train_loss: 24504.314453125, validation_loss = 5080.0909423828125
 Trial: 62 Epoch: 22, train_loss: 28392.044189453125, validation_loss = 4183.5882568359375
 Trial: 62 Epoch: 23, train_loss: 27306.421875, validation_loss = 5462.35693359375
 Trial: 62 Epoch: 24, train_loss: 29365.343139648438, validation_loss = 4420.463134765625
 Trial: 62 Epoch: 25, train_loss: 26101.878662109375, validation_loss = 5247.5587158203125
 Trial: 62 Epoch: 26, train_loss: 27183.821166992188, validation_loss = 3593.3426513671875
 Trial: 62 Epoch: 27, train_loss: 24703.721313476562, validation_loss = 3000.2569580078125
 Trial: 62 Epoch: 28, train_loss: 25601.169921875, validation_loss = 3600.746826171875
 Trial: 62 Epoch: 29, train_loss: 27211.544555664062, validation_loss = 3496.6533203125
 Trial: 62 Epoch: 30, train_loss: 26701.690185546875, validation_loss = 4146.804443359375
 Trial: 62 Epoch: 31, train_loss: 26222.775756835938, validation_loss = 4851.361083984375
 Trial: 62 Epoch: 32, train_loss: 27709.290405273438, validation_loss = 3522.1839599609375
 Trial: 62 Epoch: 33, train_loss: 22228.160400390625, validation_loss = 3399.2413330078125
 Trial: 62 Epoch: 34, train_loss: 22042.939575195312, validation_loss = 3035.9354248046875
 Trial: 62 Epoch: 35, train_loss: 22379.536010742188, validation_loss = 3532.5045166015625
 Trial: 62 Epoch: 36, train_loss: 24049.075561523438, validation_loss = 3423.6602783203125
 Trial: 62 Epoch: 37, train_loss: 23883.90673828125, validation_loss = 4038.42724609375
 Trial: 62 Epoch: 38, train_loss: 22821.3876953125, validation_loss = 3245.6448974609375
 Trial: 62 Epoch: 39, train_loss: 20160.380859375, validation_loss = 2972.486328125
 Trial: 62 Epoch: 40, train_loss: 20712.16571044922, validation_loss = 2915.5142822265625
 Trial: 62 Epoch: 41, train_loss: 19064.710388183594, validation_loss = 2806.1748046875
 Trial: 62 Epoch: 42, train_loss: 19869.452026367188, validation_loss = 2744.9219970703125
 Trial: 62 Epoch: 43, train_loss: 20788.746826171875, validation_loss = 3082.9732666015625
 Trial: 62 Epoch: 44, train_loss: 21149.293334960938, validation_loss = 2960.3587646484375
 Trial: 62 Epoch: 45, train_loss: 21292.068725585938, validation_loss = 3052.538818359375
 Trial: 62 Epoch: 46, train_loss: 20265.010864257812, validation_loss = 4992.959716796875
 Trial: 62 Epoch: 47, train_loss: 21175.894897460938, validation_loss = 2757.819091796875
 Trial: 62 Epoch: 48, train_loss: 20770.086791992188, validation_loss = 2934.4442138671875
 Trial: 62 Epoch: 49, train_loss: 20253.849609375, validation_loss = 3216.836181640625
 Trial: 62 Epoch: 50, train_loss: 21400.089721679688, validation_loss = 3192.3763427734375
 Trial: 62 Epoch: 51, train_loss: 20459.961853027344, validation_loss = 3211.9578857421875
 Trial: 62 Epoch: 52, train_loss: 20613.2685546875, validation_loss = 3190.802001953125
 Trial: 62 Epoch: 53, train_loss: 22529.882690429688, validation_loss = 2733.439697265625
 Trial: 62 Epoch: 54, train_loss: 21830.181640625, validation_loss = 3024.009765625
 Trial: 62 Epoch: 55, train_loss: 19658.71075439453, validation_loss = 3195.504638671875
 Trial: 62 Epoch: 56, train_loss: 19285.981811523438, validation_loss = 2767.30322265625
 Trial: 62 Epoch: 57, train_loss: 19451.189331054688, validation_loss = 3595.056396484375
 Trial: 62 Epoch: 58, train_loss: 19348.456787109375, validation_loss = 2989.617919921875
 Trial: 62 Epoch: 59, train_loss: 19028.7490234375, validation_loss = 2887.677734375
 Trial: 62 Epoch: 60, train_loss: 20777.731689453125, validation_loss = 2861.857177734375
 Trial: 62 Epoch: 61, train_loss: 21905.356811523438, validation_loss = 4067.686279296875
 Trial: 62 Epoch: 62, train_loss: 19860.618896484375, validation_loss = 3016.71728515625
 Trial: 62 Epoch: 63, train_loss: 18156.365966796875, validation_loss = 2972.8480224609375
 Trial: 62 Epoch: 64, train_loss: 18905.98162841797, validation_loss = 3259.3936767578125
 Trial: 62 Epoch: 65, train_loss: 18608.08770751953, validation_loss = 2962.058837890625
 Trial: 62 Epoch: 66, train_loss: 18146.237060546875, validation_loss = 3237.6937255859375
 Trial: 62 Epoch: 67, train_loss: 21921.820861816406, validation_loss = 3647.804443359375
 Trial: 62 Epoch: 68, train_loss: 22319.4375, validation_loss = 2798.6236572265625
 Trial: 62 Epoch: 69, train_loss: 19517.214294433594, validation_loss = 3158.9136962890625
 Trial: 62 Epoch: 70, train_loss: 21385.67071533203, validation_loss = 3325.95166015625
 Trial: 62 Epoch: 71, train_loss: 18092.572692871094, validation_loss = 2686.2752685546875
 Trial: 62 Epoch: 72, train_loss: 19012.903747558594, validation_loss = 2794.5133056640625
 Trial: 62 Epoch: 73, train_loss: 19307.35626220703, validation_loss = 3126.0860595703125
 Trial: 62 Epoch: 74, train_loss: 18221.179321289062, validation_loss = 3119.6104736328125
 Trial: 62 Epoch: 75, train_loss: 18249.736938476562, validation_loss = 3480.24462890625
 Trial: 62 Epoch: 76, train_loss: 19885.695739746094, validation_loss = 4431.158447265625
 Trial: 62 Epoch: 77, train_loss: 22006.771240234375, validation_loss = 2768.682373046875
 Trial: 62 Epoch: 78, train_loss: 18522.910583496094, validation_loss = 2568.2528076171875
 Trial: 62 Epoch: 79, train_loss: 22724.509887695312, validation_loss = 3617.0257568359375
 Trial: 62 Epoch: 80, train_loss: 19095.989013671875, validation_loss = 2812.6597900390625
 Trial: 62 Epoch: 81, train_loss: 18714.515258789062, validation_loss = 2873.9268798828125
 Trial: 62 Epoch: 82, train_loss: 18356.063598632812, validation_loss = 3668.99365234375
 Trial: 62 Epoch: 83, train_loss: 17945.659729003906, validation_loss = 2947.7327880859375
 Trial: 62 Epoch: 84, train_loss: 18454.807495117188, validation_loss = 3107.229248046875
 Trial: 62 Epoch: 85, train_loss: 18240.476928710938, validation_loss = 3116.378173828125
 Trial: 62 Epoch: 86, train_loss: 18321.432739257812, validation_loss = 3001.876220703125
 Trial: 62 Epoch: 87, train_loss: 19513.183471679688, validation_loss = 3557.946044921875
 Trial: 62 Epoch: 88, train_loss: 19064.49755859375, validation_loss = 2642.001220703125
 Trial: 62 Epoch: 89, train_loss: 22185.506713867188, validation_loss = 2794.0633544921875
 Trial: 62 Epoch: 90, train_loss: 20178.243041992188, validation_loss = 6106.357666015625
 Trial: 62 Epoch: 91, train_loss: 22459.968383789062, validation_loss = 2821.42919921875
 Trial: 62 Epoch: 92, train_loss: 19960.452514648438, validation_loss = 3492.5081787109375
 Trial: 62 Epoch: 93, train_loss: 19533.239501953125, validation_loss = 2549.027587890625
 Trial: 62 Epoch: 94, train_loss: 19354.056396484375, validation_loss = 3698.6551513671875
 Trial: 62 Epoch: 95, train_loss: 18908.009521484375, validation_loss = 3381.353759765625
 Trial: 62 Epoch: 96, train_loss: 22827.482666015625, validation_loss = 2820.71044921875
 Trial: 62 Epoch: 97, train_loss: 17232.555908203125, validation_loss = 2767.36865234375
 Trial: 62 Epoch: 98, train_loss: 17517.38623046875, validation_loss = 2778.3399658203125
 Trial: 62 Epoch: 99, train_loss: 18281.94287109375, validation_loss = 2879.4371337890625
========Trial 63 params: {'n_layers': 1, 'K': 2, 'Dropout': 0.2, 'Hidden0': 256, 'optimizer': 'Adam', 'lr': 0.021956251430006218, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |   12288    |
|    sptH.0.gcn.bias     |    256     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |    3072    |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    6144    |
|    sptD.0.gcn.bias     |    256     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |    3072    |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |   12288    |
|    sptW.0.gcn.bias     |    256     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |    3072    |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 45285
 Trial: 63 Epoch: 0, train_loss: 91671.38159179688, validation_loss = 6317.989501953125
 Trial: 63 Epoch: 1, train_loss: 56650.390869140625, validation_loss = 5134.756591796875
 Trial: 63 Epoch: 2, train_loss: 36065.12194824219, validation_loss = 5494.338623046875
 Trial: 63 Epoch: 3, train_loss: 50084.872802734375, validation_loss = 5284.604248046875
 Trial: 63 Epoch: 4, train_loss: 74310.33471679688, validation_loss = 4207.1904296875
 Trial: 63 Epoch: 5, train_loss: 57183.73095703125, validation_loss = 9531.171875
 Trial: 63 Epoch: 6, train_loss: 31908.036010742188, validation_loss = 5815.0352783203125
 Trial: 63 Epoch: 7, train_loss: 29285.394409179688, validation_loss = 4617.5711669921875
 Trial: 63 Epoch: 8, train_loss: 27790.98828125, validation_loss = 4137.966552734375
 Trial: 63 Epoch: 9, train_loss: 26735.706420898438, validation_loss = 3535.3389892578125
 Trial: 63 Epoch: 10, train_loss: 27116.631591796875, validation_loss = 4878.4652099609375
 Trial: 63 Epoch: 11, train_loss: 27628.19091796875, validation_loss = 4544.6796875
 Trial: 63 Epoch: 12, train_loss: 29357.917358398438, validation_loss = 3408.7110595703125
 Trial: 63 Epoch: 13, train_loss: 27360.953491210938, validation_loss = 5976.172119140625
 Trial: 63 Epoch: 14, train_loss: 25290.24951171875, validation_loss = 3316.6646728515625
 Trial: 63 Epoch: 15, train_loss: 24211.57666015625, validation_loss = 3847.535400390625
 Trial: 63 Epoch: 16, train_loss: 22886.227600097656, validation_loss = 3165.697509765625
 Trial: 63 Epoch: 17, train_loss: 23021.42901611328, validation_loss = 4438.2872314453125
 Trial: 63 Epoch: 18, train_loss: 25035.745361328125, validation_loss = 3550.2060546875
 Trial: 63 Epoch: 19, train_loss: 24404.917114257812, validation_loss = 3617.6436767578125
 Trial: 63 Epoch: 20, train_loss: 24672.512451171875, validation_loss = 3203.916748046875
 Trial: 63 Epoch: 21, train_loss: 23819.604125976562, validation_loss = 4134.233154296875
 Trial: 63 Epoch: 22, train_loss: 27193.58514404297, validation_loss = 3930.7445068359375
 Trial: 63 Epoch: 23, train_loss: 26625.321838378906, validation_loss = 4863.6363525390625
 Trial: 63 Epoch: 24, train_loss: 32060.426879882812, validation_loss = 4557.136474609375
 Trial: 63 Epoch: 25, train_loss: 28658.1611328125, validation_loss = 5832.132080078125
 Trial: 63 Epoch: 26, train_loss: 28436.491333007812, validation_loss = 3540.4415283203125
 Trial: 63 Epoch: 27, train_loss: 25110.53692626953, validation_loss = 2913.0714111328125
 Trial: 63 Epoch: 28, train_loss: 27158.65478515625, validation_loss = 3720.6246337890625
 Trial: 63 Epoch: 29, train_loss: 28426.49853515625, validation_loss = 3764.660888671875
 Trial: 63 Epoch: 30, train_loss: 27890.2080078125, validation_loss = 4082.2886962890625
 Trial: 63 Epoch: 31, train_loss: 27315.930419921875, validation_loss = 5148.0185546875
 Trial: 63 Epoch: 32, train_loss: 28090.320190429688, validation_loss = 3089.384765625
 Trial: 63 Epoch: 33, train_loss: 22361.675903320312, validation_loss = 3240.4638671875
 Trial: 63 Epoch: 34, train_loss: 21597.316650390625, validation_loss = 3021.784912109375
 Trial: 63 Epoch: 35, train_loss: 22958.311401367188, validation_loss = 3616.0797119140625
 Trial: 63 Epoch: 36, train_loss: 24619.473754882812, validation_loss = 3149.2349853515625
 Trial: 63 Epoch: 37, train_loss: 24029.291381835938, validation_loss = 3781.248779296875
 Trial: 63 Epoch: 38, train_loss: 24316.251831054688, validation_loss = 3290.8372802734375
 Trial: 63 Epoch: 39, train_loss: 20950.0029296875, validation_loss = 2886.5745849609375
 Trial: 63 Epoch: 40, train_loss: 20128.021057128906, validation_loss = 2944.3486328125
 Trial: 63 Epoch: 41, train_loss: 19642.075561523438, validation_loss = 2645.2613525390625
 Trial: 63 Epoch: 42, train_loss: 20501.00018310547, validation_loss = 2694.9764404296875
 Trial: 63 Epoch: 43, train_loss: 20602.563232421875, validation_loss = 2988.02978515625
 Trial: 63 Epoch: 44, train_loss: 20705.741638183594, validation_loss = 3281.59423828125
 Trial: 63 Epoch: 45, train_loss: 21827.93975830078, validation_loss = 2834.0645751953125
 Trial: 63 Epoch: 46, train_loss: 20177.015625, validation_loss = 5685.50634765625
 Trial: 63 Epoch: 47, train_loss: 22317.5830078125, validation_loss = 2739.5762939453125
 Trial: 63 Epoch: 48, train_loss: 20726.20672607422, validation_loss = 2671.34912109375
 Trial: 63 Epoch: 49, train_loss: 19251.37255859375, validation_loss = 3688.6458740234375
 Trial: 63 Epoch: 50, train_loss: 22156.192749023438, validation_loss = 3354.3380126953125
 Trial: 63 Epoch: 51, train_loss: 20824.50897216797, validation_loss = 3207.9090576171875
 Trial: 63 Epoch: 52, train_loss: 20501.963500976562, validation_loss = 3189.1468505859375
 Trial: 63 Epoch: 53, train_loss: 23524.817749023438, validation_loss = 2581.18603515625
 Trial: 63 Epoch: 54, train_loss: 23986.86248779297, validation_loss = 2761.106689453125
 Trial: 63 Epoch: 55, train_loss: 20945.792358398438, validation_loss = 3048.520263671875
 Trial: 63 Epoch: 56, train_loss: 18855.801147460938, validation_loss = 2556.7662353515625
 Trial: 63 Epoch: 57, train_loss: 19727.999877929688, validation_loss = 3312.5430908203125
 Trial: 63 Epoch: 58, train_loss: 21947.072143554688, validation_loss = 3384.3094482421875
 Trial: 63 Epoch: 59, train_loss: 21074.853515625, validation_loss = 2633.949951171875
 Trial: 63 Epoch: 60, train_loss: 19013.821533203125, validation_loss = 2776.554931640625
 Trial: 63 Epoch: 61, train_loss: 20181.880859375, validation_loss = 3733.1680908203125
 Trial: 63 Epoch: 62, train_loss: 19108.225708007812, validation_loss = 2837.765625
 Trial: 63 Epoch: 63, train_loss: 18545.868530273438, validation_loss = 2970.660888671875
 Trial: 63 Epoch: 64, train_loss: 18289.236267089844, validation_loss = 2832.9884033203125
 Trial: 63 Epoch: 65, train_loss: 18713.89794921875, validation_loss = 2836.2100830078125
 Trial: 63 Epoch: 66, train_loss: 17605.798706054688, validation_loss = 3032.9527587890625
 Trial: 63 Epoch: 67, train_loss: 21068.275329589844, validation_loss = 4251.804931640625
 Trial: 63 Epoch: 68, train_loss: 22559.17120361328, validation_loss = 2855.05712890625
 Trial: 63 Epoch: 69, train_loss: 18912.18975830078, validation_loss = 2702.789794921875
 Trial: 63 Epoch: 70, train_loss: 23366.91925048828, validation_loss = 3791.436767578125
 Trial: 63 Epoch: 71, train_loss: 19502.375, validation_loss = 2905.9210205078125
 Trial: 63 Epoch: 72, train_loss: 20149.767639160156, validation_loss = 2647.904296875
 Trial: 63 Epoch: 73, train_loss: 19332.681091308594, validation_loss = 2531.6529541015625
 Trial: 63 Epoch: 74, train_loss: 18522.811889648438, validation_loss = 2823.6104736328125
 Trial: 63 Epoch: 75, train_loss: 19631.00225830078, validation_loss = 3855.154296875
 Trial: 63 Epoch: 76, train_loss: 22116.65625, validation_loss = 4041.2677001953125
 Trial: 63 Epoch: 77, train_loss: 22904.373779296875, validation_loss = 2649.820556640625
 Trial: 63 Epoch: 78, train_loss: 18736.74969482422, validation_loss = 2756.1776123046875
 Trial: 63 Epoch: 79, train_loss: 25226.270568847656, validation_loss = 4167.42822265625
 Trial: 63 Epoch: 80, train_loss: 19684.310974121094, validation_loss = 2755.1004638671875
 Trial: 63 Epoch: 81, train_loss: 18489.16143798828, validation_loss = 3000.208740234375
 Trial: 63 Epoch: 82, train_loss: 19174.73193359375, validation_loss = 3699.1322021484375
 Trial: 63 Epoch: 83, train_loss: 18341.626098632812, validation_loss = 2990.5762939453125
 Trial: 63 Epoch: 84, train_loss: 18967.708862304688, validation_loss = 3094.0892333984375
 Trial: 63 Epoch: 85, train_loss: 19072.449951171875, validation_loss = 3552.772216796875
 Trial: 63 Epoch: 86, train_loss: 18327.953857421875, validation_loss = 2947.3138427734375
 Trial: 63 Epoch: 87, train_loss: 20269.39520263672, validation_loss = 3457.8956298828125
 Trial: 63 Epoch: 88, train_loss: 18359.667419433594, validation_loss = 2583.534912109375
 Trial: 63 Epoch: 89, train_loss: 21336.469360351562, validation_loss = 2615.6190185546875
 Trial: 63 Epoch: 90, train_loss: 19365.502868652344, validation_loss = 4994.127685546875
 Trial: 63 Epoch: 91, train_loss: 20174.25, validation_loss = 2627.250244140625
 Trial: 63 Epoch: 92, train_loss: 18457.71710205078, validation_loss = 3000.4354248046875
 Trial: 63 Epoch: 93, train_loss: 19123.63720703125, validation_loss = 2523.5426025390625
 Trial: 63 Epoch: 94, train_loss: 19069.555053710938, validation_loss = 3474.08837890625
 Trial: 63 Epoch: 95, train_loss: 18354.43017578125, validation_loss = 2843.4591064453125
 Trial: 63 Epoch: 96, train_loss: 22952.34600830078, validation_loss = 2668.2310791015625
 Trial: 63 Epoch: 97, train_loss: 18444.00177001953, validation_loss = 2649.8641357421875
 Trial: 63 Epoch: 98, train_loss: 17911.503540039062, validation_loss = 2495.1142578125
 Trial: 63 Epoch: 99, train_loss: 18453.275512695312, validation_loss = 3066.470703125
========Trial 64 params: {'n_layers': 1, 'K': 2, 'Dropout': 0.2, 'Hidden0': 128, 'optimizer': 'Adam', 'lr': 0.011769400867406899, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |    6144    |
|    sptH.0.gcn.bias     |    128     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |    1536    |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    3072    |
|    sptD.0.gcn.bias     |    128     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |    1536    |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |    6144    |
|    sptW.0.gcn.bias     |    128     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |    1536    |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 24933
 Trial: 64 Epoch: 0, train_loss: 112027.49169921875, validation_loss = 7879.59033203125
========Trial 65 params: {'n_layers': 1, 'K': 2, 'Dropout': 0.2, 'Hidden0': 256, 'optimizer': 'Adam', 'lr': 0.008945894382274175, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |   12288    |
|    sptH.0.gcn.bias     |    256     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |    3072    |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    6144    |
|    sptD.0.gcn.bias     |    256     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |    3072    |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |   12288    |
|    sptW.0.gcn.bias     |    256     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |    3072    |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 45285
 Trial: 65 Epoch: 0, train_loss: 111060.32666015625, validation_loss = 5705.967041015625
 Trial: 65 Epoch: 1, train_loss: 36326.34619140625, validation_loss = 4865.9793701171875
 Trial: 65 Epoch: 2, train_loss: 35714.6591796875, validation_loss = 4228.6134033203125
 Trial: 65 Epoch: 3, train_loss: 41354.667236328125, validation_loss = 5515.017578125
 Trial: 65 Epoch: 4, train_loss: 53617.694580078125, validation_loss = 3726.799560546875
 Trial: 65 Epoch: 5, train_loss: 35752.79553222656, validation_loss = 5560.45751953125
 Trial: 65 Epoch: 6, train_loss: 26466.159423828125, validation_loss = 4304.4139404296875
 Trial: 65 Epoch: 7, train_loss: 26957.30908203125, validation_loss = 4152.500244140625
 Trial: 65 Epoch: 8, train_loss: 25152.366455078125, validation_loss = 3960.9130859375
 Trial: 65 Epoch: 9, train_loss: 24168.218811035156, validation_loss = 3370.684326171875
 Trial: 65 Epoch: 10, train_loss: 24463.796997070312, validation_loss = 4333.7906494140625
 Trial: 65 Epoch: 11, train_loss: 25277.971740722656, validation_loss = 4546.85302734375
 Trial: 65 Epoch: 12, train_loss: 28091.91455078125, validation_loss = 3287.5927734375
 Trial: 65 Epoch: 13, train_loss: 26427.758422851562, validation_loss = 5169.51904296875
 Trial: 65 Epoch: 14, train_loss: 27843.564208984375, validation_loss = 5044.0616455078125
 Trial: 65 Epoch: 15, train_loss: 25928.188720703125, validation_loss = 4438.245849609375
 Trial: 65 Epoch: 16, train_loss: 24109.381103515625, validation_loss = 3393.8441162109375
 Trial: 65 Epoch: 17, train_loss: 21224.924865722656, validation_loss = 3546.9969482421875
 Trial: 65 Epoch: 18, train_loss: 23845.3759765625, validation_loss = 3072.332275390625
 Trial: 65 Epoch: 19, train_loss: 21264.775268554688, validation_loss = 2772.8614501953125
 Trial: 65 Epoch: 20, train_loss: 20261.242553710938, validation_loss = 2947.9176025390625
 Trial: 65 Epoch: 21, train_loss: 21472.60577392578, validation_loss = 3641.204345703125
 Trial: 65 Epoch: 22, train_loss: 22337.56591796875, validation_loss = 3186.333984375
 Trial: 65 Epoch: 23, train_loss: 21738.644470214844, validation_loss = 3584.31689453125
 Trial: 65 Epoch: 24, train_loss: 23531.18505859375, validation_loss = 3415.443115234375
 Trial: 65 Epoch: 25, train_loss: 21872.695068359375, validation_loss = 3546.885009765625
 Trial: 65 Epoch: 26, train_loss: 22007.876525878906, validation_loss = 3053.7197265625
 Trial: 65 Epoch: 27, train_loss: 20650.635009765625, validation_loss = 2887.9974365234375
 Trial: 65 Epoch: 28, train_loss: 21943.511108398438, validation_loss = 2928.7034912109375
 Trial: 65 Epoch: 29, train_loss: 24140.894287109375, validation_loss = 3056.6070556640625
 Trial: 65 Epoch: 30, train_loss: 22417.83935546875, validation_loss = 4265.292724609375
 Trial: 65 Epoch: 31, train_loss: 23865.892700195312, validation_loss = 4640.3150634765625
 Trial: 65 Epoch: 32, train_loss: 25324.20770263672, validation_loss = 3002.8240966796875
 Trial: 65 Epoch: 33, train_loss: 20895.215942382812, validation_loss = 2998.9093017578125
 Trial: 65 Epoch: 34, train_loss: 20550.931884765625, validation_loss = 3098.7320556640625
 Trial: 65 Epoch: 35, train_loss: 19612.385192871094, validation_loss = 3060.2208251953125
 Trial: 65 Epoch: 36, train_loss: 21348.126037597656, validation_loss = 3042.7515869140625
 Trial: 65 Epoch: 37, train_loss: 20851.155029296875, validation_loss = 2992.3919677734375
 Trial: 65 Epoch: 38, train_loss: 19798.579345703125, validation_loss = 2866.010986328125
 Trial: 65 Epoch: 39, train_loss: 18154.44024658203, validation_loss = 2955.288330078125
 Trial: 65 Epoch: 40, train_loss: 19955.921264648438, validation_loss = 2788.09423828125
 Trial: 65 Epoch: 41, train_loss: 17954.512634277344, validation_loss = 3153.355712890625
 Trial: 65 Epoch: 42, train_loss: 18597.89794921875, validation_loss = 2672.6016845703125
 Trial: 65 Epoch: 43, train_loss: 19494.615295410156, validation_loss = 2910.677490234375
 Trial: 65 Epoch: 44, train_loss: 18997.357971191406, validation_loss = 3107.3726806640625
 Trial: 65 Epoch: 45, train_loss: 19084.762939453125, validation_loss = 3490.8076171875
 Trial: 65 Epoch: 46, train_loss: 19268.447692871094, validation_loss = 3257.1962890625
 Trial: 65 Epoch: 47, train_loss: 19154.297729492188, validation_loss = 2736.189697265625
 Trial: 65 Epoch: 48, train_loss: 18917.200561523438, validation_loss = 2614.64111328125
 Trial: 65 Epoch: 49, train_loss: 17906.541564941406, validation_loss = 3436.84130859375
 Trial: 65 Epoch: 50, train_loss: 19114.81671142578, validation_loss = 2856.025146484375
 Trial: 65 Epoch: 51, train_loss: 18451.32977294922, validation_loss = 3583.873291015625
 Trial: 65 Epoch: 52, train_loss: 18882.5263671875, validation_loss = 3028.2852783203125
 Trial: 65 Epoch: 53, train_loss: 21239.723266601562, validation_loss = 2640.4462890625
 Trial: 65 Epoch: 54, train_loss: 20142.412048339844, validation_loss = 2982.0999755859375
 Trial: 65 Epoch: 55, train_loss: 18130.601684570312, validation_loss = 2535.0301513671875
 Trial: 65 Epoch: 56, train_loss: 17777.36279296875, validation_loss = 2653.5496826171875
 Trial: 65 Epoch: 57, train_loss: 18881.710083007812, validation_loss = 3024.22509765625
 Trial: 65 Epoch: 58, train_loss: 19421.18194580078, validation_loss = 2809.6260986328125
 Trial: 65 Epoch: 59, train_loss: 18073.513244628906, validation_loss = 2622.1253662109375
 Trial: 65 Epoch: 60, train_loss: 18557.307678222656, validation_loss = 2652.9271240234375
 Trial: 65 Epoch: 61, train_loss: 20456.13427734375, validation_loss = 2809.0853271484375
 Trial: 65 Epoch: 62, train_loss: 19547.566162109375, validation_loss = 2904.97900390625
 Trial: 65 Epoch: 63, train_loss: 18209.120727539062, validation_loss = 2916.28515625
 Trial: 65 Epoch: 64, train_loss: 20368.4296875, validation_loss = 3648.934814453125
 Trial: 65 Epoch: 65, train_loss: 20372.843688964844, validation_loss = 2675.1185302734375
 Trial: 65 Epoch: 66, train_loss: 19068.391174316406, validation_loss = 2867.271240234375
 Trial: 65 Epoch: 67, train_loss: 17312.751892089844, validation_loss = 2729.29052734375
 Trial: 65 Epoch: 68, train_loss: 17438.765197753906, validation_loss = 2667.5748291015625
 Trial: 65 Epoch: 69, train_loss: 16482.986755371094, validation_loss = 2623.9700927734375
 Trial: 65 Epoch: 70, train_loss: 17669.75311279297, validation_loss = 3132.5550537109375
 Trial: 65 Epoch: 71, train_loss: 16233.745361328125, validation_loss = 2539.6531982421875
 Trial: 65 Epoch: 72, train_loss: 16701.447021484375, validation_loss = 2758.6605224609375
 Trial: 65 Epoch: 73, train_loss: 17247.484924316406, validation_loss = 2963.5191650390625
 Trial: 65 Epoch: 74, train_loss: 16651.586303710938, validation_loss = 2799.8359375
 Trial: 65 Epoch: 75, train_loss: 17405.744506835938, validation_loss = 3577.222412109375
 Trial: 65 Epoch: 76, train_loss: 18755.305114746094, validation_loss = 2829.2222900390625
 Trial: 65 Epoch: 77, train_loss: 16803.13671875, validation_loss = 2850.5228271484375
 Trial: 65 Epoch: 78, train_loss: 16654.518676757812, validation_loss = 2798.13671875
 Trial: 65 Epoch: 79, train_loss: 19908.984802246094, validation_loss = 3917.9862060546875
 Trial: 65 Epoch: 80, train_loss: 17192.20684814453, validation_loss = 2966.284912109375
 Trial: 65 Epoch: 81, train_loss: 17752.93084716797, validation_loss = 2889.460693359375
 Trial: 65 Epoch: 82, train_loss: 16746.13555908203, validation_loss = 3847.095703125
 Trial: 65 Epoch: 83, train_loss: 16551.17889404297, validation_loss = 2618.5885009765625
 Trial: 65 Epoch: 84, train_loss: 16878.543395996094, validation_loss = 2709.1834716796875
 Trial: 65 Epoch: 85, train_loss: 16233.474243164062, validation_loss = 2757.0716552734375
 Trial: 65 Epoch: 86, train_loss: 17304.603637695312, validation_loss = 2629.56591796875
 Trial: 65 Epoch: 87, train_loss: 18970.62432861328, validation_loss = 3418.3924560546875
 Trial: 65 Epoch: 88, train_loss: 16797.088500976562, validation_loss = 2564.96435546875
 Trial: 65 Epoch: 89, train_loss: 17482.453674316406, validation_loss = 2785.88037109375
 Trial: 65 Epoch: 90, train_loss: 16238.831909179688, validation_loss = 2900.2791748046875
 Trial: 65 Epoch: 91, train_loss: 16820.09051513672, validation_loss = 2698.9891357421875
 Trial: 65 Epoch: 92, train_loss: 17214.304321289062, validation_loss = 2625.90625
 Trial: 65 Epoch: 93, train_loss: 17752.563110351562, validation_loss = 2522.1343994140625
 Trial: 65 Epoch: 94, train_loss: 18573.910095214844, validation_loss = 2839.5960693359375
 Trial: 65 Epoch: 95, train_loss: 18434.4228515625, validation_loss = 2619.2952880859375
 Trial: 65 Epoch: 96, train_loss: 16239.401794433594, validation_loss = 2636.42724609375
 Trial: 65 Epoch: 97, train_loss: 15954.769958496094, validation_loss = 2585.1451416015625
 Trial: 65 Epoch: 98, train_loss: 16013.532470703125, validation_loss = 2567.5345458984375
 Trial: 65 Epoch: 99, train_loss: 16035.41552734375, validation_loss = 2815.3394775390625
========Trial 66 params: {'n_layers': 1, 'K': 2, 'Dropout': 0.2, 'Hidden0': 16, 'optimizer': 'Adam', 'lr': 0.008482121573687504, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |    768     |
|    sptH.0.gcn.bias     |     16     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |    192     |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    384     |
|    sptD.0.gcn.bias     |     16     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |    192     |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |    768     |
|    sptW.0.gcn.bias     |     16     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |    192     |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 7125
 Trial: 66 Epoch: 0, train_loss: 173361.416015625, validation_loss = 14217.8720703125
========Trial 67 params: {'n_layers': 1, 'K': 2, 'Dropout': 0.2, 'Hidden0': 256, 'optimizer': 'Adam', 'lr': 0.008998620028225041, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |   12288    |
|    sptH.0.gcn.bias     |    256     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |    3072    |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    6144    |
|    sptD.0.gcn.bias     |    256     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |    3072    |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |   12288    |
|    sptW.0.gcn.bias     |    256     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |    3072    |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 45285
 Trial: 67 Epoch: 0, train_loss: 111297.9453125, validation_loss = 5604.1329345703125
 Trial: 67 Epoch: 1, train_loss: 36326.87121582031, validation_loss = 4961.0206298828125
 Trial: 67 Epoch: 2, train_loss: 36384.909423828125, validation_loss = 4172.638916015625
 Trial: 67 Epoch: 3, train_loss: 41334.69714355469, validation_loss = 5397.944580078125
 Trial: 67 Epoch: 4, train_loss: 52720.5205078125, validation_loss = 3754.045166015625
 Trial: 67 Epoch: 5, train_loss: 34944.45556640625, validation_loss = 5290.1231689453125
 Trial: 67 Epoch: 6, train_loss: 26389.691650390625, validation_loss = 4145.498291015625
 Trial: 67 Epoch: 7, train_loss: 27226.237548828125, validation_loss = 4125.93310546875
 Trial: 67 Epoch: 8, train_loss: 25384.902221679688, validation_loss = 3969.9093017578125
 Trial: 67 Epoch: 9, train_loss: 24309.84454345703, validation_loss = 3346.4986572265625
 Trial: 67 Epoch: 10, train_loss: 24664.350952148438, validation_loss = 4151.6826171875
 Trial: 67 Epoch: 11, train_loss: 25095.56805419922, validation_loss = 4234.902587890625
 Trial: 67 Epoch: 12, train_loss: 27553.758422851562, validation_loss = 3304.4056396484375
 Trial: 67 Epoch: 13, train_loss: 26399.257690429688, validation_loss = 5017.49462890625
 Trial: 67 Epoch: 14, train_loss: 27780.638427734375, validation_loss = 5094.8800048828125
 Trial: 67 Epoch: 15, train_loss: 25936.911743164062, validation_loss = 4517.2366943359375
 Trial: 67 Epoch: 16, train_loss: 23976.939819335938, validation_loss = 3440.49853515625
 Trial: 67 Epoch: 17, train_loss: 21084.960021972656, validation_loss = 3590.6497802734375
 Trial: 67 Epoch: 18, train_loss: 23828.0576171875, validation_loss = 3095.46240234375
 Trial: 67 Epoch: 19, train_loss: 21255.891845703125, validation_loss = 2771.516357421875
 Trial: 67 Epoch: 20, train_loss: 20175.045959472656, validation_loss = 2944.7376708984375
 Trial: 67 Epoch: 21, train_loss: 21004.418090820312, validation_loss = 3712.4212646484375
 Trial: 67 Epoch: 22, train_loss: 22545.8544921875, validation_loss = 3168.6990966796875
 Trial: 67 Epoch: 23, train_loss: 22104.237731933594, validation_loss = 3762.65234375
 Trial: 67 Epoch: 24, train_loss: 23593.224975585938, validation_loss = 3392.7679443359375
 Trial: 67 Epoch: 25, train_loss: 21859.75341796875, validation_loss = 3698.487548828125
 Trial: 67 Epoch: 26, train_loss: 21920.22772216797, validation_loss = 3115.4775390625
 Trial: 67 Epoch: 27, train_loss: 20965.96405029297, validation_loss = 2915.705078125
 Trial: 67 Epoch: 28, train_loss: 21744.931518554688, validation_loss = 2943.623291015625
 Trial: 67 Epoch: 29, train_loss: 23717.319885253906, validation_loss = 3031.357666015625
 Trial: 67 Epoch: 30, train_loss: 22306.487854003906, validation_loss = 4181.1805419921875
 Trial: 67 Epoch: 31, train_loss: 24271.761108398438, validation_loss = 4599.1102294921875
 Trial: 67 Epoch: 32, train_loss: 25993.77801513672, validation_loss = 2953.5831298828125
 Trial: 67 Epoch: 33, train_loss: 21237.87860107422, validation_loss = 3000.9024658203125
 Trial: 67 Epoch: 34, train_loss: 20316.289306640625, validation_loss = 3181.6649169921875
 Trial: 67 Epoch: 35, train_loss: 19806.064086914062, validation_loss = 3039.065185546875
 Trial: 67 Epoch: 36, train_loss: 21614.83514404297, validation_loss = 2978.0108642578125
 Trial: 67 Epoch: 37, train_loss: 20795.92529296875, validation_loss = 3040.4637451171875
 Trial: 67 Epoch: 38, train_loss: 19730.43231201172, validation_loss = 2896.7828369140625
 Trial: 67 Epoch: 39, train_loss: 18292.706604003906, validation_loss = 3012.3304443359375
 Trial: 67 Epoch: 40, train_loss: 19497.96258544922, validation_loss = 2764.831298828125
 Trial: 67 Epoch: 41, train_loss: 17951.051391601562, validation_loss = 2907.476318359375
 Trial: 67 Epoch: 42, train_loss: 18865.375122070312, validation_loss = 2686.595703125
 Trial: 67 Epoch: 43, train_loss: 19328.532348632812, validation_loss = 3006.3388671875
 Trial: 67 Epoch: 44, train_loss: 19301.842651367188, validation_loss = 3053.728759765625
 Trial: 67 Epoch: 45, train_loss: 19477.837524414062, validation_loss = 3290.3607177734375
 Trial: 67 Epoch: 46, train_loss: 18734.763549804688, validation_loss = 3391.26611328125
 Trial: 67 Epoch: 47, train_loss: 19003.5869140625, validation_loss = 2735.3116455078125
 Trial: 67 Epoch: 48, train_loss: 18887.881286621094, validation_loss = 2703.849365234375
 Trial: 67 Epoch: 49, train_loss: 18034.432067871094, validation_loss = 3345.80517578125
 Trial: 67 Epoch: 50, train_loss: 19035.837890625, validation_loss = 2833.68701171875
 Trial: 67 Epoch: 51, train_loss: 18400.157287597656, validation_loss = 3505.7996826171875
 Trial: 67 Epoch: 52, train_loss: 18848.310180664062, validation_loss = 3010.91796875
 Trial: 67 Epoch: 53, train_loss: 21318.752563476562, validation_loss = 2642.1043701171875
 Trial: 67 Epoch: 54, train_loss: 20695.477966308594, validation_loss = 2973.418701171875
 Trial: 67 Epoch: 55, train_loss: 18329.452514648438, validation_loss = 2521.921142578125
 Trial: 67 Epoch: 56, train_loss: 17968.521545410156, validation_loss = 2615.0545654296875
 Trial: 67 Epoch: 57, train_loss: 18747.595703125, validation_loss = 2916.9427490234375
 Trial: 67 Epoch: 58, train_loss: 19509.74462890625, validation_loss = 2783.2742919921875
 Trial: 67 Epoch: 59, train_loss: 18161.523681640625, validation_loss = 2597.602783203125
 Trial: 67 Epoch: 60, train_loss: 19046.198181152344, validation_loss = 2615.9249267578125
 Trial: 67 Epoch: 61, train_loss: 20484.04296875, validation_loss = 2772.080322265625
 Trial: 67 Epoch: 62, train_loss: 20061.269287109375, validation_loss = 2976.7874755859375
 Trial: 67 Epoch: 63, train_loss: 18630.821838378906, validation_loss = 2987.2657470703125
 Trial: 67 Epoch: 64, train_loss: 21498.943237304688, validation_loss = 3648.0135498046875
 Trial: 67 Epoch: 65, train_loss: 20505.437744140625, validation_loss = 2671.800537109375
 Trial: 67 Epoch: 66, train_loss: 18070.539001464844, validation_loss = 2847.4613037109375
 Trial: 67 Epoch: 67, train_loss: 17847.225952148438, validation_loss = 2673.76513671875
 Trial: 67 Epoch: 68, train_loss: 17346.537719726562, validation_loss = 2645.9951171875
 Trial: 67 Epoch: 69, train_loss: 16634.9130859375, validation_loss = 2699.57861328125
 Trial: 67 Epoch: 70, train_loss: 18008.37274169922, validation_loss = 3250.46533203125
 Trial: 67 Epoch: 71, train_loss: 16484.833068847656, validation_loss = 2529.2745361328125
 Trial: 67 Epoch: 72, train_loss: 16633.90399169922, validation_loss = 2750.2037353515625
 Trial: 67 Epoch: 73, train_loss: 17086.19110107422, validation_loss = 2937.8223876953125
 Trial: 67 Epoch: 74, train_loss: 16536.71484375, validation_loss = 2803.3521728515625
 Trial: 67 Epoch: 75, train_loss: 17334.277893066406, validation_loss = 3465.016845703125
 Trial: 67 Epoch: 76, train_loss: 18588.442932128906, validation_loss = 2884.6981201171875
 Trial: 67 Epoch: 77, train_loss: 16970.13330078125, validation_loss = 2816.8385009765625
 Trial: 67 Epoch: 78, train_loss: 16547.940368652344, validation_loss = 2648.8980712890625
 Trial: 67 Epoch: 79, train_loss: 19600.66033935547, validation_loss = 3706.152099609375
 Trial: 67 Epoch: 80, train_loss: 17166.553771972656, validation_loss = 2858.0810546875
 Trial: 67 Epoch: 81, train_loss: 18051.072998046875, validation_loss = 2884.323486328125
 Trial: 67 Epoch: 82, train_loss: 17098.754028320312, validation_loss = 3726.489990234375
 Trial: 67 Epoch: 83, train_loss: 16682.005493164062, validation_loss = 2719.7720947265625
 Trial: 67 Epoch: 84, train_loss: 17547.95867919922, validation_loss = 2623.66943359375
 Trial: 67 Epoch: 85, train_loss: 16649.879272460938, validation_loss = 2800.203857421875
 Trial: 67 Epoch: 86, train_loss: 17460.987365722656, validation_loss = 2640.7379150390625
 Trial: 67 Epoch: 87, train_loss: 18711.058166503906, validation_loss = 3300.7884521484375
 Trial: 67 Epoch: 88, train_loss: 16639.359313964844, validation_loss = 2524.8499755859375
 Trial: 67 Epoch: 89, train_loss: 17387.99591064453, validation_loss = 2775.696044921875
 Trial: 67 Epoch: 90, train_loss: 16578.55096435547, validation_loss = 2898.646728515625
 Trial: 67 Epoch: 91, train_loss: 17336.185974121094, validation_loss = 2726.3592529296875
 Trial: 67 Epoch: 92, train_loss: 17351.944580078125, validation_loss = 2544.7958984375
 Trial: 67 Epoch: 93, train_loss: 17747.577392578125, validation_loss = 2466.306884765625
 Trial: 67 Epoch: 94, train_loss: 18427.414611816406, validation_loss = 2877.31396484375
 Trial: 67 Epoch: 95, train_loss: 18616.493408203125, validation_loss = 2637.1788330078125
 Trial: 67 Epoch: 96, train_loss: 17161.83819580078, validation_loss = 2825.4517822265625
 Trial: 67 Epoch: 97, train_loss: 16259.65478515625, validation_loss = 2558.75439453125
 Trial: 67 Epoch: 98, train_loss: 15970.205261230469, validation_loss = 2560.303466796875
 Trial: 67 Epoch: 99, train_loss: 16272.216003417969, validation_loss = 2756.5850830078125
========Trial 68 params: {'n_layers': 1, 'K': 2, 'Dropout': 0.2, 'Hidden0': 4, 'optimizer': 'Adam', 'lr': 0.00868935539004044, 'batch': 64}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |    192     |
|    sptH.0.gcn.bias     |     4      |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |     48     |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |     96     |
|    sptD.0.gcn.bias     |     4      |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |     48     |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |    192     |
|    sptW.0.gcn.bias     |     4      |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |     48     |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 5217
 Trial: 68 Epoch: 0, train_loss: 71342.9453125, validation_loss = 47742.64453125
========Trial 69 params: {'n_layers': 1, 'K': 2, 'Dropout': 0.2, 'Hidden0': 256, 'optimizer': 'Adam', 'lr': 0.013889927279573616, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |   12288    |
|    sptH.0.gcn.bias     |    256     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |    3072    |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    6144    |
|    sptD.0.gcn.bias     |    256     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |    3072    |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |   12288    |
|    sptW.0.gcn.bias     |    256     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |    3072    |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 45285
 Trial: 69 Epoch: 0, train_loss: 143526.4443359375, validation_loss = 5858.111083984375
 Trial: 69 Epoch: 1, train_loss: 40047.7275390625, validation_loss = 4599.720947265625
 Trial: 69 Epoch: 2, train_loss: 36585.839599609375, validation_loss = 4674.6339111328125
 Trial: 69 Epoch: 3, train_loss: 42700.990478515625, validation_loss = 5489.096435546875
 Trial: 69 Epoch: 4, train_loss: 54951.18713378906, validation_loss = 3747.7413330078125
 Trial: 69 Epoch: 5, train_loss: 38754.814453125, validation_loss = 6343.4212646484375
 Trial: 69 Epoch: 6, train_loss: 27622.039428710938, validation_loss = 4795.7396240234375
 Trial: 69 Epoch: 7, train_loss: 27192.254028320312, validation_loss = 4103.55322265625
 Trial: 69 Epoch: 8, train_loss: 25577.891357421875, validation_loss = 3594.6414794921875
 Trial: 69 Epoch: 9, train_loss: 25055.150756835938, validation_loss = 3313.7974853515625
 Trial: 69 Epoch: 10, train_loss: 25466.804565429688, validation_loss = 4513.560546875
 Trial: 69 Epoch: 11, train_loss: 27277.579956054688, validation_loss = 5073.5390625
 Trial: 69 Epoch: 12, train_loss: 27874.9296875, validation_loss = 3475.9359130859375
 Trial: 69 Epoch: 13, train_loss: 25956.578491210938, validation_loss = 5865.220703125
 Trial: 69 Epoch: 14, train_loss: 26407.919921875, validation_loss = 3268.7564697265625
 Trial: 69 Epoch: 15, train_loss: 23239.3173828125, validation_loss = 3360.6785888671875
 Trial: 69 Epoch: 16, train_loss: 21705.324157714844, validation_loss = 3245.4732666015625
 Trial: 69 Epoch: 17, train_loss: 21372.192749023438, validation_loss = 3827.90478515625
 Trial: 69 Epoch: 18, train_loss: 24169.99609375, validation_loss = 3214.5426025390625
 Trial: 69 Epoch: 19, train_loss: 23491.968994140625, validation_loss = 3353.54345703125
 Trial: 69 Epoch: 20, train_loss: 23250.9453125, validation_loss = 3237.8111572265625
 Trial: 69 Epoch: 21, train_loss: 24407.072998046875, validation_loss = 5056.9248046875
 Trial: 69 Epoch: 22, train_loss: 26246.36474609375, validation_loss = 3648.1209716796875
 Trial: 69 Epoch: 23, train_loss: 24186.455078125, validation_loss = 3861.088623046875
 Trial: 69 Epoch: 24, train_loss: 24828.86572265625, validation_loss = 3684.0240478515625
 Trial: 69 Epoch: 25, train_loss: 22454.229248046875, validation_loss = 3249.988525390625
 Trial: 69 Epoch: 26, train_loss: 22297.230895996094, validation_loss = 3184.9703369140625
 Trial: 69 Epoch: 27, train_loss: 21301.891723632812, validation_loss = 2962.34375
 Trial: 69 Epoch: 28, train_loss: 22316.55419921875, validation_loss = 2782.8660888671875
 Trial: 69 Epoch: 29, train_loss: 23637.84930419922, validation_loss = 2775.5997314453125
 Trial: 69 Epoch: 30, train_loss: 22376.991760253906, validation_loss = 4218.12548828125
 Trial: 69 Epoch: 31, train_loss: 22423.89483642578, validation_loss = 3193.52685546875
 Trial: 69 Epoch: 32, train_loss: 24211.512939453125, validation_loss = 2959.2265625
 Trial: 69 Epoch: 33, train_loss: 20474.557983398438, validation_loss = 3620.8272705078125
 Trial: 69 Epoch: 34, train_loss: 24639.04736328125, validation_loss = 2687.162353515625
 Trial: 69 Epoch: 35, train_loss: 20605.737060546875, validation_loss = 3648.5272216796875
 Trial: 69 Epoch: 36, train_loss: 23673.645629882812, validation_loss = 3553.6961669921875
 Trial: 69 Epoch: 37, train_loss: 23170.312377929688, validation_loss = 3621.46826171875
 Trial: 69 Epoch: 38, train_loss: 22065.4140625, validation_loss = 3005.8477783203125
 Trial: 69 Epoch: 39, train_loss: 19381.59716796875, validation_loss = 2741.6927490234375
 Trial: 69 Epoch: 40, train_loss: 19069.254455566406, validation_loss = 2710.8597412109375
 Trial: 69 Epoch: 41, train_loss: 18922.788940429688, validation_loss = 2550.588134765625
 Trial: 69 Epoch: 42, train_loss: 19259.816345214844, validation_loss = 2565.9547119140625
 Trial: 69 Epoch: 43, train_loss: 20645.535766601562, validation_loss = 2991.606689453125
 Trial: 69 Epoch: 44, train_loss: 20667.96826171875, validation_loss = 2954.1126708984375
 Trial: 69 Epoch: 45, train_loss: 21596.54913330078, validation_loss = 2853.03173828125
 Trial: 69 Epoch: 46, train_loss: 20448.761352539062, validation_loss = 4136.6248779296875
 Trial: 69 Epoch: 47, train_loss: 19933.651489257812, validation_loss = 2512.9769287109375
 Trial: 69 Epoch: 48, train_loss: 20030.015014648438, validation_loss = 2653.8150634765625
 Trial: 69 Epoch: 49, train_loss: 19365.249938964844, validation_loss = 3708.22998046875
 Trial: 69 Epoch: 50, train_loss: 21628.732543945312, validation_loss = 3079.8177490234375
 Trial: 69 Epoch: 51, train_loss: 20104.627990722656, validation_loss = 3194.44921875
 Trial: 69 Epoch: 52, train_loss: 19969.43475341797, validation_loss = 3045.2554931640625
 Trial: 69 Epoch: 53, train_loss: 22426.872924804688, validation_loss = 2443.6556396484375
 Trial: 69 Epoch: 54, train_loss: 23230.046508789062, validation_loss = 2610.6334228515625
 Trial: 69 Epoch: 55, train_loss: 19744.080627441406, validation_loss = 2718.4063720703125
 Trial: 69 Epoch: 56, train_loss: 18837.67401123047, validation_loss = 2477.7264404296875
 Trial: 69 Epoch: 57, train_loss: 20391.373291015625, validation_loss = 3012.2769775390625
 Trial: 69 Epoch: 58, train_loss: 21205.77978515625, validation_loss = 3016.7969970703125
 Trial: 69 Epoch: 59, train_loss: 19947.144165039062, validation_loss = 2807.1407470703125
 Trial: 69 Epoch: 60, train_loss: 19453.42205810547, validation_loss = 2738.7178955078125
 Trial: 69 Epoch: 61, train_loss: 21686.401123046875, validation_loss = 3226.3939208984375
 Trial: 69 Epoch: 62, train_loss: 19321.53485107422, validation_loss = 2803.444091796875
 Trial: 69 Epoch: 63, train_loss: 18901.60076904297, validation_loss = 2929.7391357421875
 Trial: 69 Epoch: 64, train_loss: 18606.058959960938, validation_loss = 3150.2591552734375
 Trial: 69 Epoch: 65, train_loss: 19199.014099121094, validation_loss = 2745.06396484375
 Trial: 69 Epoch: 66, train_loss: 18185.80535888672, validation_loss = 2857.0377197265625
 Trial: 69 Epoch: 67, train_loss: 20099.039428710938, validation_loss = 2527.9351806640625
 Trial: 69 Epoch: 68, train_loss: 19615.41571044922, validation_loss = 2494.787353515625
 Trial: 69 Epoch: 69, train_loss: 17640.020141601562, validation_loss = 2695.153564453125
 Trial: 69 Epoch: 70, train_loss: 18378.67449951172, validation_loss = 3261.3931884765625
 Trial: 69 Epoch: 71, train_loss: 17487.946166992188, validation_loss = 2488.06982421875
 Trial: 69 Epoch: 72, train_loss: 17814.992797851562, validation_loss = 2572.58056640625
 Trial: 69 Epoch: 73, train_loss: 18455.121704101562, validation_loss = 2877.2161865234375
 Trial: 69 Epoch: 74, train_loss: 17300.755310058594, validation_loss = 2767.4471435546875
 Trial: 69 Epoch: 75, train_loss: 17893.540405273438, validation_loss = 3468.116943359375
 Trial: 69 Epoch: 76, train_loss: 20263.68621826172, validation_loss = 3371.2857666015625
 Trial: 69 Epoch: 77, train_loss: 19801.496520996094, validation_loss = 2571.4515380859375
 Trial: 69 Epoch: 78, train_loss: 17140.411193847656, validation_loss = 2545.42724609375
 Trial: 69 Epoch: 79, train_loss: 20445.076049804688, validation_loss = 3588.4296875
 Trial: 69 Epoch: 80, train_loss: 18439.122680664062, validation_loss = 2604.9649658203125
 Trial: 69 Epoch: 81, train_loss: 18401.35711669922, validation_loss = 3010.2999267578125
 Trial: 69 Epoch: 82, train_loss: 18044.84991455078, validation_loss = 3551.880126953125
 Trial: 69 Epoch: 83, train_loss: 17414.32354736328, validation_loss = 2717.742919921875
 Trial: 69 Epoch: 84, train_loss: 18073.48712158203, validation_loss = 2885.1029052734375
 Trial: 69 Epoch: 85, train_loss: 17930.027282714844, validation_loss = 2819.7607421875
 Trial: 69 Epoch: 86, train_loss: 17915.93438720703, validation_loss = 2710.26953125
 Trial: 69 Epoch: 87, train_loss: 18837.014282226562, validation_loss = 3020.396728515625
 Trial: 69 Epoch: 88, train_loss: 17935.48779296875, validation_loss = 2462.7723388671875
 Trial: 69 Epoch: 89, train_loss: 20192.777587890625, validation_loss = 2524.3594970703125
 Trial: 69 Epoch: 90, train_loss: 18386.128356933594, validation_loss = 4077.3472900390625
 Trial: 69 Epoch: 91, train_loss: 19781.282470703125, validation_loss = 2558.3853759765625
 Trial: 69 Epoch: 92, train_loss: 18383.479736328125, validation_loss = 3127.7685546875
 Trial: 69 Epoch: 93, train_loss: 18268.98876953125, validation_loss = 2382.4228515625
 Trial: 69 Epoch: 94, train_loss: 18625.96484375, validation_loss = 3037.6282958984375
 Trial: 69 Epoch: 95, train_loss: 18380.067321777344, validation_loss = 2896.563720703125
 Trial: 69 Epoch: 96, train_loss: 20534.114135742188, validation_loss = 2698.598876953125
 Trial: 69 Epoch: 97, train_loss: 17238.95733642578, validation_loss = 2534.7164306640625
 Trial: 69 Epoch: 98, train_loss: 17185.888732910156, validation_loss = 2457.15283203125
 Trial: 69 Epoch: 99, train_loss: 17824.85528564453, validation_loss = 2955.14599609375
========Trial 70 params: {'n_layers': 1, 'K': 2, 'Dropout': 0.1, 'Hidden0': 256, 'optimizer': 'Adam', 'lr': 0.009370215222960028, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |   12288    |
|    sptH.0.gcn.bias     |    256     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |    3072    |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    6144    |
|    sptD.0.gcn.bias     |    256     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |    3072    |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |   12288    |
|    sptW.0.gcn.bias     |    256     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |    3072    |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 45285
 Trial: 70 Epoch: 0, train_loss: 109418.90283203125, validation_loss = 5436.7828369140625
 Trial: 70 Epoch: 1, train_loss: 35995.43176269531, validation_loss = 4794.087890625
 Trial: 70 Epoch: 2, train_loss: 36209.219482421875, validation_loss = 4211.6400146484375
 Trial: 70 Epoch: 3, train_loss: 41285.057861328125, validation_loss = 5352.8212890625
 Trial: 70 Epoch: 4, train_loss: 51431.76940917969, validation_loss = 3686.866943359375
 Trial: 70 Epoch: 5, train_loss: 33937.30456542969, validation_loss = 5370.1903076171875
 Trial: 70 Epoch: 6, train_loss: 25990.414794921875, validation_loss = 4259.1038818359375
 Trial: 70 Epoch: 7, train_loss: 25870.105102539062, validation_loss = 4215.5980224609375
 Trial: 70 Epoch: 8, train_loss: 24910.30029296875, validation_loss = 3677.594970703125
 Trial: 70 Epoch: 9, train_loss: 23839.473693847656, validation_loss = 3245.23291015625
 Trial: 70 Epoch: 10, train_loss: 23857.317260742188, validation_loss = 4203.90087890625
 Trial: 70 Epoch: 11, train_loss: 24442.71759033203, validation_loss = 3984.497314453125
 Trial: 70 Epoch: 12, train_loss: 26306.53759765625, validation_loss = 3190.0513916015625
 Trial: 70 Epoch: 13, train_loss: 24183.74560546875, validation_loss = 4940.7091064453125
 Trial: 70 Epoch: 14, train_loss: 26727.688720703125, validation_loss = 4109.339111328125
 Trial: 70 Epoch: 15, train_loss: 22888.279663085938, validation_loss = 3913.333251953125
 Trial: 70 Epoch: 16, train_loss: 22288.074340820312, validation_loss = 3234.583984375
 Trial: 70 Epoch: 17, train_loss: 20197.239013671875, validation_loss = 3702.384033203125
 Trial: 70 Epoch: 18, train_loss: 24179.28076171875, validation_loss = 2922.0172119140625
 Trial: 70 Epoch: 19, train_loss: 21480.974243164062, validation_loss = 2751.915771484375
 Trial: 70 Epoch: 20, train_loss: 20074.909423828125, validation_loss = 2943.80322265625
 Trial: 70 Epoch: 21, train_loss: 20171.519958496094, validation_loss = 3852.312744140625
 Trial: 70 Epoch: 22, train_loss: 21940.377807617188, validation_loss = 3174.0653076171875
 Trial: 70 Epoch: 23, train_loss: 21113.662170410156, validation_loss = 3793.1002197265625
 Trial: 70 Epoch: 24, train_loss: 22723.7333984375, validation_loss = 3283.8350830078125
 Trial: 70 Epoch: 25, train_loss: 20925.820678710938, validation_loss = 3538.4578857421875
 Trial: 70 Epoch: 26, train_loss: 21361.17852783203, validation_loss = 3148.1417236328125
 Trial: 70 Epoch: 27, train_loss: 20214.396545410156, validation_loss = 2805.9285888671875
 Trial: 70 Epoch: 28, train_loss: 21076.40478515625, validation_loss = 2793.022705078125
 Trial: 70 Epoch: 29, train_loss: 23033.959228515625, validation_loss = 2948.0816650390625
 Trial: 70 Epoch: 30, train_loss: 21553.591186523438, validation_loss = 3969.249755859375
 Trial: 70 Epoch: 31, train_loss: 22478.889282226562, validation_loss = 4661.230712890625
 Trial: 70 Epoch: 32, train_loss: 24892.990173339844, validation_loss = 2981.3614501953125
 Trial: 70 Epoch: 33, train_loss: 20569.69403076172, validation_loss = 3030.18017578125
 Trial: 70 Epoch: 34, train_loss: 20069.783813476562, validation_loss = 3053.5794677734375
 Trial: 70 Epoch: 35, train_loss: 19073.362731933594, validation_loss = 2892.4490966796875
 Trial: 70 Epoch: 36, train_loss: 20562.3466796875, validation_loss = 2936.9517822265625
 Trial: 70 Epoch: 37, train_loss: 20465.638916015625, validation_loss = 3123.5615234375
 Trial: 70 Epoch: 38, train_loss: 19777.763671875, validation_loss = 2923.0023193359375
 Trial: 70 Epoch: 39, train_loss: 17964.010498046875, validation_loss = 2838.4217529296875
 Trial: 70 Epoch: 40, train_loss: 19761.474975585938, validation_loss = 2563.74560546875
 Trial: 70 Epoch: 41, train_loss: 17613.706909179688, validation_loss = 2784.0465087890625
 Trial: 70 Epoch: 42, train_loss: 17350.35235595703, validation_loss = 2550.4796142578125
 Trial: 70 Epoch: 43, train_loss: 18861.639404296875, validation_loss = 3044.52294921875
 Trial: 70 Epoch: 44, train_loss: 18711.35809326172, validation_loss = 3005.6439208984375
 Trial: 70 Epoch: 45, train_loss: 19483.42840576172, validation_loss = 3452.8968505859375
 Trial: 70 Epoch: 46, train_loss: 18853.903259277344, validation_loss = 3258.467041015625
 Trial: 70 Epoch: 47, train_loss: 18723.679931640625, validation_loss = 2540.3443603515625
 Trial: 70 Epoch: 48, train_loss: 18862.83819580078, validation_loss = 2610.0435791015625
 Trial: 70 Epoch: 49, train_loss: 17438.07061767578, validation_loss = 3271.933837890625
 Trial: 70 Epoch: 50, train_loss: 19053.584899902344, validation_loss = 2928.28857421875
 Trial: 70 Epoch: 51, train_loss: 18233.50323486328, validation_loss = 3279.89599609375
 Trial: 70 Epoch: 52, train_loss: 17928.53582763672, validation_loss = 2977.3199462890625
 Trial: 70 Epoch: 53, train_loss: 20531.87176513672, validation_loss = 2526.0999755859375
 Trial: 70 Epoch: 54, train_loss: 21143.470458984375, validation_loss = 2827.16259765625
 Trial: 70 Epoch: 55, train_loss: 17586.73797607422, validation_loss = 2475.2471923828125
 Trial: 70 Epoch: 56, train_loss: 18324.60107421875, validation_loss = 2586.63134765625
 Trial: 70 Epoch: 57, train_loss: 19316.539672851562, validation_loss = 2624.757568359375
 Trial: 70 Epoch: 58, train_loss: 18617.228149414062, validation_loss = 2627.349853515625
 Trial: 70 Epoch: 59, train_loss: 17407.40899658203, validation_loss = 2584.0050048828125
 Trial: 70 Epoch: 60, train_loss: 18165.46405029297, validation_loss = 2591.0670166015625
 Trial: 70 Epoch: 61, train_loss: 20031.286010742188, validation_loss = 2715.6429443359375
 Trial: 70 Epoch: 62, train_loss: 19110.695922851562, validation_loss = 2960.3643798828125
 Trial: 70 Epoch: 63, train_loss: 17942.21630859375, validation_loss = 2893.34033203125
 Trial: 70 Epoch: 64, train_loss: 19972.828491210938, validation_loss = 3493.8387451171875
 Trial: 70 Epoch: 65, train_loss: 19400.835327148438, validation_loss = 2661.9688720703125
 Trial: 70 Epoch: 66, train_loss: 18052.222717285156, validation_loss = 2802.42529296875
 Trial: 70 Epoch: 67, train_loss: 16727.395568847656, validation_loss = 2563.865478515625
 Trial: 70 Epoch: 68, train_loss: 16664.528442382812, validation_loss = 2635.5174560546875
 Trial: 70 Epoch: 69, train_loss: 16025.368286132812, validation_loss = 2534.282958984375
 Trial: 70 Epoch: 70, train_loss: 17793.43096923828, validation_loss = 3189.19580078125
 Trial: 70 Epoch: 71, train_loss: 16516.229125976562, validation_loss = 2498.0614013671875
 Trial: 70 Epoch: 72, train_loss: 16086.355590820312, validation_loss = 2579.984619140625
 Trial: 70 Epoch: 73, train_loss: 16983.473266601562, validation_loss = 2703.0665283203125
 Trial: 70 Epoch: 74, train_loss: 16536.12139892578, validation_loss = 2655.1558837890625
 Trial: 70 Epoch: 75, train_loss: 17427.67657470703, validation_loss = 3552.5709228515625
 Trial: 70 Epoch: 76, train_loss: 18231.573974609375, validation_loss = 2737.132568359375
 Trial: 70 Epoch: 77, train_loss: 16720.971435546875, validation_loss = 2705.0146484375
 Trial: 70 Epoch: 78, train_loss: 16275.317321777344, validation_loss = 2695.8875732421875
 Trial: 70 Epoch: 79, train_loss: 19617.613647460938, validation_loss = 3800.798828125
 Trial: 70 Epoch: 80, train_loss: 16578.101806640625, validation_loss = 2878.89501953125
 Trial: 70 Epoch: 81, train_loss: 18415.734497070312, validation_loss = 2849.5731201171875
 Trial: 70 Epoch: 82, train_loss: 17110.020385742188, validation_loss = 3532.5096435546875
 Trial: 70 Epoch: 83, train_loss: 16328.299377441406, validation_loss = 2564.78076171875
 Trial: 70 Epoch: 84, train_loss: 16451.683959960938, validation_loss = 2593.1768798828125
 Trial: 70 Epoch: 85, train_loss: 16205.405090332031, validation_loss = 2561.97900390625
 Trial: 70 Epoch: 86, train_loss: 17687.01104736328, validation_loss = 2380.532470703125
 Trial: 70 Epoch: 87, train_loss: 16554.070373535156, validation_loss = 2917.978271484375
 Trial: 70 Epoch: 88, train_loss: 16129.069030761719, validation_loss = 2410.3665771484375
 Trial: 70 Epoch: 89, train_loss: 17446.049682617188, validation_loss = 2654.249267578125
 Trial: 70 Epoch: 90, train_loss: 15961.140197753906, validation_loss = 2923.9757080078125
 Trial: 70 Epoch: 91, train_loss: 17693.871154785156, validation_loss = 2561.543212890625
 Trial: 70 Epoch: 92, train_loss: 17050.39794921875, validation_loss = 2468.4881591796875
 Trial: 70 Epoch: 93, train_loss: 16987.778686523438, validation_loss = 2469.55859375
 Trial: 70 Epoch: 94, train_loss: 17482.052490234375, validation_loss = 2698.175048828125
 Trial: 70 Epoch: 95, train_loss: 17160.54705810547, validation_loss = 2646.5987548828125
 Trial: 70 Epoch: 96, train_loss: 16281.552368164062, validation_loss = 2562.1444091796875
 Trial: 70 Epoch: 97, train_loss: 15626.860412597656, validation_loss = 2446.1412353515625
 Trial: 70 Epoch: 98, train_loss: 15034.286254882812, validation_loss = 2500.901611328125
 Trial: 70 Epoch: 99, train_loss: 15851.533813476562, validation_loss = 2849.256591796875
========Trial 71 params: {'n_layers': 1, 'K': 2, 'Dropout': 0.1, 'Hidden0': 256, 'optimizer': 'Adam', 'lr': 0.009730063866741405, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |   12288    |
|    sptH.0.gcn.bias     |    256     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |    3072    |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    6144    |
|    sptD.0.gcn.bias     |    256     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |    3072    |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |   12288    |
|    sptW.0.gcn.bias     |    256     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |    3072    |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 45285
 Trial: 71 Epoch: 0, train_loss: 109697.16162109375, validation_loss = 5200.6370849609375
 Trial: 71 Epoch: 1, train_loss: 36451.22521972656, validation_loss = 4790.223388671875
 Trial: 71 Epoch: 2, train_loss: 36566.15979003906, validation_loss = 4262.615234375
 Trial: 71 Epoch: 3, train_loss: 41590.7724609375, validation_loss = 5417.2197265625
 Trial: 71 Epoch: 4, train_loss: 50824.38427734375, validation_loss = 3584.6322021484375
 Trial: 71 Epoch: 5, train_loss: 33069.40283203125, validation_loss = 5538.6214599609375
 Trial: 71 Epoch: 6, train_loss: 25687.973999023438, validation_loss = 4323.199951171875
 Trial: 71 Epoch: 7, train_loss: 25419.522705078125, validation_loss = 3991.51025390625
 Trial: 71 Epoch: 8, train_loss: 24640.529541015625, validation_loss = 3572.9825439453125
 Trial: 71 Epoch: 9, train_loss: 23618.00958251953, validation_loss = 3214.1402587890625
 Trial: 71 Epoch: 10, train_loss: 23864.547241210938, validation_loss = 4558.0968017578125
 Trial: 71 Epoch: 11, train_loss: 25295.410705566406, validation_loss = 4519.66357421875
 Trial: 71 Epoch: 12, train_loss: 27447.605590820312, validation_loss = 3135.414794921875
 Trial: 71 Epoch: 13, train_loss: 24495.134887695312, validation_loss = 5154.1376953125
 Trial: 71 Epoch: 14, train_loss: 26797.134521484375, validation_loss = 4055.5203857421875
 Trial: 71 Epoch: 15, train_loss: 23057.904724121094, validation_loss = 3884.6968994140625
 Trial: 71 Epoch: 16, train_loss: 22419.172119140625, validation_loss = 3261.1092529296875
 Trial: 71 Epoch: 17, train_loss: 20183.814819335938, validation_loss = 3824.9217529296875
 Trial: 71 Epoch: 18, train_loss: 24191.986083984375, validation_loss = 2924.4022216796875
 Trial: 71 Epoch: 19, train_loss: 21758.322692871094, validation_loss = 2775.148193359375
 Trial: 71 Epoch: 20, train_loss: 20302.109008789062, validation_loss = 2910.4105224609375
 Trial: 71 Epoch: 21, train_loss: 20294.10577392578, validation_loss = 3850.0247802734375
 Trial: 71 Epoch: 22, train_loss: 22105.547607421875, validation_loss = 3126.5848388671875
 Trial: 71 Epoch: 23, train_loss: 21244.777709960938, validation_loss = 3781.0635986328125
 Trial: 71 Epoch: 24, train_loss: 23007.790771484375, validation_loss = 3286.6251220703125
 Trial: 71 Epoch: 25, train_loss: 21305.592529296875, validation_loss = 3511.1201171875
 Trial: 71 Epoch: 26, train_loss: 21217.916259765625, validation_loss = 3117.699951171875
 Trial: 71 Epoch: 27, train_loss: 20444.610961914062, validation_loss = 2730.7958984375
 Trial: 71 Epoch: 28, train_loss: 21155.756469726562, validation_loss = 2722.2393798828125
 Trial: 71 Epoch: 29, train_loss: 22915.55126953125, validation_loss = 2791.257080078125
 Trial: 71 Epoch: 30, train_loss: 21804.45184326172, validation_loss = 4113.026123046875
 Trial: 71 Epoch: 31, train_loss: 23356.000244140625, validation_loss = 4671.506103515625
 Trial: 71 Epoch: 32, train_loss: 25645.13800048828, validation_loss = 3007.43212890625
 Trial: 71 Epoch: 33, train_loss: 20633.149841308594, validation_loss = 3189.2501220703125
 Trial: 71 Epoch: 34, train_loss: 20291.146362304688, validation_loss = 2950.8367919921875
 Trial: 71 Epoch: 35, train_loss: 19287.518615722656, validation_loss = 2934.9398193359375
 Trial: 71 Epoch: 36, train_loss: 21044.615112304688, validation_loss = 2922.3900146484375
 Trial: 71 Epoch: 37, train_loss: 20605.654541015625, validation_loss = 3110.0457763671875
 Trial: 71 Epoch: 38, train_loss: 19896.630798339844, validation_loss = 2860.27587890625
 Trial: 71 Epoch: 39, train_loss: 18200.85302734375, validation_loss = 2834.99169921875
 Trial: 71 Epoch: 40, train_loss: 19822.2822265625, validation_loss = 2568.9122314453125
 Trial: 71 Epoch: 41, train_loss: 17869.270385742188, validation_loss = 2953.154052734375
 Trial: 71 Epoch: 42, train_loss: 17799.605346679688, validation_loss = 2526.937255859375
 Trial: 71 Epoch: 43, train_loss: 19167.27020263672, validation_loss = 3012.360107421875
 Trial: 71 Epoch: 44, train_loss: 19012.809814453125, validation_loss = 2897.380615234375
 Trial: 71 Epoch: 45, train_loss: 20581.871337890625, validation_loss = 3912.886474609375
 Trial: 71 Epoch: 46, train_loss: 20163.500244140625, validation_loss = 3153.9150390625
 Trial: 71 Epoch: 47, train_loss: 18877.68701171875, validation_loss = 2627.316650390625
 Trial: 71 Epoch: 48, train_loss: 18742.969604492188, validation_loss = 2630.239501953125
 Trial: 71 Epoch: 49, train_loss: 17739.716430664062, validation_loss = 3233.8114013671875
 Trial: 71 Epoch: 50, train_loss: 18678.9033203125, validation_loss = 2796.7862548828125
 Trial: 71 Epoch: 51, train_loss: 18189.474853515625, validation_loss = 3259.9139404296875
 Trial: 71 Epoch: 52, train_loss: 18382.54766845703, validation_loss = 2952.8275146484375
 Trial: 71 Epoch: 53, train_loss: 20864.98974609375, validation_loss = 2494.203857421875
 Trial: 71 Epoch: 54, train_loss: 20918.623474121094, validation_loss = 2793.6678466796875
 Trial: 71 Epoch: 55, train_loss: 17950.519653320312, validation_loss = 2492.0010986328125
 Trial: 71 Epoch: 56, train_loss: 18298.462524414062, validation_loss = 2572.2764892578125
 Trial: 71 Epoch: 57, train_loss: 19271.671875, validation_loss = 2646.6075439453125
 Trial: 71 Epoch: 58, train_loss: 19202.865844726562, validation_loss = 2601.43408203125
 Trial: 71 Epoch: 59, train_loss: 17931.152099609375, validation_loss = 2604.60400390625
 Trial: 71 Epoch: 60, train_loss: 18519.44891357422, validation_loss = 2546.853515625
 Trial: 71 Epoch: 61, train_loss: 19672.943603515625, validation_loss = 2676.067626953125
 Trial: 71 Epoch: 62, train_loss: 18863.992797851562, validation_loss = 2836.8934326171875
 Trial: 71 Epoch: 63, train_loss: 17896.345092773438, validation_loss = 2803.0982666015625
 Trial: 71 Epoch: 64, train_loss: 19273.342346191406, validation_loss = 3353.42529296875
 Trial: 71 Epoch: 65, train_loss: 18946.307373046875, validation_loss = 2675.9105224609375
 Trial: 71 Epoch: 66, train_loss: 17636.817443847656, validation_loss = 2778.9346923828125
 Trial: 71 Epoch: 67, train_loss: 17243.491821289062, validation_loss = 2546.01318359375
 Trial: 71 Epoch: 68, train_loss: 16895.82373046875, validation_loss = 2595.4700927734375
 Trial: 71 Epoch: 69, train_loss: 16408.29803466797, validation_loss = 2567.92431640625
 Trial: 71 Epoch: 70, train_loss: 17775.247619628906, validation_loss = 3079.9071044921875
 Trial: 71 Epoch: 71, train_loss: 16512.702087402344, validation_loss = 2475.4525146484375
 Trial: 71 Epoch: 72, train_loss: 16378.305297851562, validation_loss = 2532.6102294921875
 Trial: 71 Epoch: 73, train_loss: 16881.650451660156, validation_loss = 2850.8873291015625
 Trial: 71 Epoch: 74, train_loss: 16366.79345703125, validation_loss = 2600.3427734375
 Trial: 71 Epoch: 75, train_loss: 17198.93035888672, validation_loss = 3609.75146484375
 Trial: 71 Epoch: 76, train_loss: 19229.236389160156, validation_loss = 2838.91796875
 Trial: 71 Epoch: 77, train_loss: 17276.139709472656, validation_loss = 2617.996337890625
 Trial: 71 Epoch: 78, train_loss: 16048.779052734375, validation_loss = 2546.639892578125
 Trial: 71 Epoch: 79, train_loss: 19520.55487060547, validation_loss = 3555.2364501953125
 Trial: 71 Epoch: 80, train_loss: 16868.439453125, validation_loss = 2723.3502197265625
 Trial: 71 Epoch: 81, train_loss: 17052.768188476562, validation_loss = 2927.8365478515625
 Trial: 71 Epoch: 82, train_loss: 16578.91473388672, validation_loss = 3348.8370361328125
 Trial: 71 Epoch: 83, train_loss: 16760.327026367188, validation_loss = 2689.181396484375
 Trial: 71 Epoch: 84, train_loss: 16417.76593017578, validation_loss = 2547.210205078125
 Trial: 71 Epoch: 85, train_loss: 16172.116333007812, validation_loss = 2692.7568359375
 Trial: 71 Epoch: 86, train_loss: 17167.299377441406, validation_loss = 2608.99267578125
 Trial: 71 Epoch: 87, train_loss: 19167.965209960938, validation_loss = 3763.540771484375
 Trial: 71 Epoch: 88, train_loss: 16786.23516845703, validation_loss = 2452.1614990234375
 Trial: 71 Epoch: 89, train_loss: 18343.825622558594, validation_loss = 2824.95068359375
 Trial: 71 Epoch: 90, train_loss: 16847.48614501953, validation_loss = 3007.9088134765625
 Trial: 71 Epoch: 91, train_loss: 17705.332946777344, validation_loss = 2561.3089599609375
 Trial: 71 Epoch: 92, train_loss: 17051.348999023438, validation_loss = 2521.935791015625
 Trial: 71 Epoch: 93, train_loss: 16787.759399414062, validation_loss = 2502.991943359375
 Trial: 71 Epoch: 94, train_loss: 17664.234375, validation_loss = 2863.5758056640625
 Trial: 71 Epoch: 95, train_loss: 17882.39093017578, validation_loss = 2643.4693603515625
 Trial: 71 Epoch: 96, train_loss: 17513.71649169922, validation_loss = 2686.1982421875
 Trial: 71 Epoch: 97, train_loss: 15775.281860351562, validation_loss = 2474.3173828125
 Trial: 71 Epoch: 98, train_loss: 15767.9140625, validation_loss = 2378.81494140625
 Trial: 71 Epoch: 99, train_loss: 15509.572937011719, validation_loss = 2649.2261962890625
========Trial 72 params: {'n_layers': 1, 'K': 2, 'Dropout': 0.1, 'Hidden0': 256, 'optimizer': 'Adam', 'lr': 0.009037640040550106, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |   12288    |
|    sptH.0.gcn.bias     |    256     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |    3072    |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    6144    |
|    sptD.0.gcn.bias     |    256     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |    3072    |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |   12288    |
|    sptW.0.gcn.bias     |    256     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |    3072    |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 45285
 Trial: 72 Epoch: 0, train_loss: 110483.97607421875, validation_loss = 5643.699462890625
 Trial: 72 Epoch: 1, train_loss: 35821.68933105469, validation_loss = 4672.0535888671875
 Trial: 72 Epoch: 2, train_loss: 35149.900390625, validation_loss = 4242.306396484375
 Trial: 72 Epoch: 3, train_loss: 40855.101806640625, validation_loss = 5238.20263671875
 Trial: 72 Epoch: 4, train_loss: 50538.82946777344, validation_loss = 3622.9189453125
 Trial: 72 Epoch: 5, train_loss: 33240.270263671875, validation_loss = 5587.485107421875
 Trial: 72 Epoch: 6, train_loss: 25424.830444335938, validation_loss = 4294.729736328125
 Trial: 72 Epoch: 7, train_loss: 25103.759155273438, validation_loss = 4156.6722412109375
 Trial: 72 Epoch: 8, train_loss: 24398.001831054688, validation_loss = 3504.020263671875
 Trial: 72 Epoch: 9, train_loss: 23395.37286376953, validation_loss = 3214.67578125
 Trial: 72 Epoch: 10, train_loss: 23549.230224609375, validation_loss = 4759.5186767578125
 Trial: 72 Epoch: 11, train_loss: 25427.281311035156, validation_loss = 4612.4033203125
 Trial: 72 Epoch: 12, train_loss: 28088.674926757812, validation_loss = 3161.0618896484375
 Trial: 72 Epoch: 13, train_loss: 24733.061157226562, validation_loss = 4799.6357421875
 Trial: 72 Epoch: 14, train_loss: 26770.223388671875, validation_loss = 4593.87109375
 Trial: 72 Epoch: 15, train_loss: 23897.462829589844, validation_loss = 4190.405517578125
 Trial: 72 Epoch: 16, train_loss: 22925.126831054688, validation_loss = 3358.48876953125
 Trial: 72 Epoch: 17, train_loss: 20304.758911132812, validation_loss = 3509.2232666015625
 Trial: 72 Epoch: 18, train_loss: 23729.484619140625, validation_loss = 2919.4840087890625
 Trial: 72 Epoch: 19, train_loss: 21200.67547607422, validation_loss = 2722.81689453125
 Trial: 72 Epoch: 20, train_loss: 19741.685302734375, validation_loss = 2859.2880859375
 Trial: 72 Epoch: 21, train_loss: 20116.649475097656, validation_loss = 3728.4237060546875
 Trial: 72 Epoch: 22, train_loss: 21784.673828125, validation_loss = 3037.6834716796875
 Trial: 72 Epoch: 23, train_loss: 20927.610107421875, validation_loss = 3496.228515625
 Trial: 72 Epoch: 24, train_loss: 22181.63134765625, validation_loss = 3126.01318359375
 Trial: 72 Epoch: 25, train_loss: 20598.091918945312, validation_loss = 3316.947998046875
 Trial: 72 Epoch: 26, train_loss: 21353.875244140625, validation_loss = 2963.3602294921875
 Trial: 72 Epoch: 27, train_loss: 20240.772338867188, validation_loss = 2690.526123046875
 Trial: 72 Epoch: 28, train_loss: 21304.456298828125, validation_loss = 2788.38037109375
 Trial: 72 Epoch: 29, train_loss: 22927.343505859375, validation_loss = 2867.26806640625
 Trial: 72 Epoch: 30, train_loss: 21553.22735595703, validation_loss = 4167.6392822265625
 Trial: 72 Epoch: 31, train_loss: 23293.619506835938, validation_loss = 5307.652099609375
 Trial: 72 Epoch: 32, train_loss: 25875.90447998047, validation_loss = 3018.7723388671875
 Trial: 72 Epoch: 33, train_loss: 20944.310974121094, validation_loss = 2946.8184814453125
 Trial: 72 Epoch: 34, train_loss: 19982.442626953125, validation_loss = 3006.6719970703125
 Trial: 72 Epoch: 35, train_loss: 19218.598510742188, validation_loss = 3009.5634765625
 Trial: 72 Epoch: 36, train_loss: 20898.99481201172, validation_loss = 2890.4112548828125
 Trial: 72 Epoch: 37, train_loss: 20490.706787109375, validation_loss = 3061.1343994140625
 Trial: 72 Epoch: 38, train_loss: 19515.17608642578, validation_loss = 2820.3590087890625
 Trial: 72 Epoch: 39, train_loss: 17915.532165527344, validation_loss = 2762.0179443359375
 Trial: 72 Epoch: 40, train_loss: 20026.95703125, validation_loss = 2627.736572265625
 Trial: 72 Epoch: 41, train_loss: 17751.591186523438, validation_loss = 2931.9783935546875
 Trial: 72 Epoch: 42, train_loss: 17933.292053222656, validation_loss = 2552.2369384765625
 Trial: 72 Epoch: 43, train_loss: 18749.79315185547, validation_loss = 2907.7109375
 Trial: 72 Epoch: 44, train_loss: 18539.18719482422, validation_loss = 3019.801513671875
 Trial: 72 Epoch: 45, train_loss: 19594.185546875, validation_loss = 3738.5643310546875
 Trial: 72 Epoch: 46, train_loss: 19397.32891845703, validation_loss = 3041.4554443359375
 Trial: 72 Epoch: 47, train_loss: 18514.138610839844, validation_loss = 2635.5399169921875
 Trial: 72 Epoch: 48, train_loss: 18520.771118164062, validation_loss = 2527.4342041015625
 Trial: 72 Epoch: 49, train_loss: 17366.43194580078, validation_loss = 3252.2088623046875
 Trial: 72 Epoch: 50, train_loss: 18611.67987060547, validation_loss = 2826.08935546875
 Trial: 72 Epoch: 51, train_loss: 18191.355346679688, validation_loss = 3263.98095703125
 Trial: 72 Epoch: 52, train_loss: 18387.631225585938, validation_loss = 3006.945068359375
 Trial: 72 Epoch: 53, train_loss: 20588.131103515625, validation_loss = 2502.6912841796875
 Trial: 72 Epoch: 54, train_loss: 20558.143493652344, validation_loss = 2922.93310546875
 Trial: 72 Epoch: 55, train_loss: 17497.359619140625, validation_loss = 2502.0047607421875
 Trial: 72 Epoch: 56, train_loss: 18294.312561035156, validation_loss = 2623.3695068359375
 Trial: 72 Epoch: 57, train_loss: 18685.6669921875, validation_loss = 2664.6007080078125
 Trial: 72 Epoch: 58, train_loss: 18830.301391601562, validation_loss = 2624.3643798828125
 Trial: 72 Epoch: 59, train_loss: 17647.645568847656, validation_loss = 2691.768310546875
 Trial: 72 Epoch: 60, train_loss: 18211.42755126953, validation_loss = 2568.436279296875
 Trial: 72 Epoch: 61, train_loss: 19422.570556640625, validation_loss = 2691.7420654296875
 Trial: 72 Epoch: 62, train_loss: 18830.28759765625, validation_loss = 2937.66552734375
 Trial: 72 Epoch: 63, train_loss: 17927.754760742188, validation_loss = 2864.8817138671875
 Trial: 72 Epoch: 64, train_loss: 19093.567260742188, validation_loss = 3724.222900390625
 Trial: 72 Epoch: 65, train_loss: 18659.45330810547, validation_loss = 2716.375244140625
 Trial: 72 Epoch: 66, train_loss: 17773.526611328125, validation_loss = 2932.7987060546875
 Trial: 72 Epoch: 67, train_loss: 17710.434020996094, validation_loss = 2442.220458984375
 Trial: 72 Epoch: 68, train_loss: 17241.375732421875, validation_loss = 2561.1142578125
 Trial: 72 Epoch: 69, train_loss: 16490.367736816406, validation_loss = 2755.7017822265625
 Trial: 72 Epoch: 70, train_loss: 17627.055603027344, validation_loss = 3158.7286376953125
 Trial: 72 Epoch: 71, train_loss: 16462.31280517578, validation_loss = 2507.647216796875
 Trial: 72 Epoch: 72, train_loss: 16378.483337402344, validation_loss = 2534.04443359375
 Trial: 72 Epoch: 73, train_loss: 16782.301635742188, validation_loss = 2709.673828125
 Trial: 72 Epoch: 74, train_loss: 16048.872192382812, validation_loss = 2699.25927734375
 Trial: 72 Epoch: 75, train_loss: 16645.87188720703, validation_loss = 3337.1708984375
 Trial: 72 Epoch: 76, train_loss: 18657.438598632812, validation_loss = 2864.278564453125
 Trial: 72 Epoch: 77, train_loss: 17292.645629882812, validation_loss = 2633.4178466796875
 Trial: 72 Epoch: 78, train_loss: 16032.598205566406, validation_loss = 2471.204345703125
 Trial: 72 Epoch: 79, train_loss: 19179.51123046875, validation_loss = 3374.8017578125
 Trial: 72 Epoch: 80, train_loss: 16429.365173339844, validation_loss = 2633.41064453125
 Trial: 72 Epoch: 81, train_loss: 16601.79364013672, validation_loss = 2951.5789794921875
 Trial: 72 Epoch: 82, train_loss: 16605.045043945312, validation_loss = 3354.7135009765625
 Trial: 72 Epoch: 83, train_loss: 16666.220153808594, validation_loss = 2772.610595703125
 Trial: 72 Epoch: 84, train_loss: 16884.787719726562, validation_loss = 2581.249267578125
 Trial: 72 Epoch: 85, train_loss: 16843.033447265625, validation_loss = 2851.818359375
 Trial: 72 Epoch: 86, train_loss: 17595.826721191406, validation_loss = 2587.986083984375
 Trial: 72 Epoch: 87, train_loss: 19099.171447753906, validation_loss = 3482.568115234375
 Trial: 72 Epoch: 88, train_loss: 17108.246032714844, validation_loss = 2508.555908203125
 Trial: 72 Epoch: 89, train_loss: 17813.66387939453, validation_loss = 2825.464111328125
 Trial: 72 Epoch: 90, train_loss: 16229.143188476562, validation_loss = 2983.989990234375
 Trial: 72 Epoch: 91, train_loss: 17337.954528808594, validation_loss = 2592.17724609375
 Trial: 72 Epoch: 92, train_loss: 17142.927307128906, validation_loss = 2545.95947265625
 Trial: 72 Epoch: 93, train_loss: 16872.686889648438, validation_loss = 2562.6448974609375
 Trial: 72 Epoch: 94, train_loss: 17644.78253173828, validation_loss = 2816.86962890625
 Trial: 72 Epoch: 95, train_loss: 18259.906127929688, validation_loss = 2612.3955078125
 Trial: 72 Epoch: 96, train_loss: 17120.815063476562, validation_loss = 2650.83447265625
 Trial: 72 Epoch: 97, train_loss: 16282.215637207031, validation_loss = 2500.740234375
 Trial: 72 Epoch: 98, train_loss: 15615.576721191406, validation_loss = 2423.1978759765625
 Trial: 72 Epoch: 99, train_loss: 15818.901489257812, validation_loss = 2664.3330078125
========Trial 73 params: {'n_layers': 1, 'K': 2, 'Dropout': 0.1, 'Hidden0': 256, 'optimizer': 'Adam', 'lr': 0.009244026902942777, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |   12288    |
|    sptH.0.gcn.bias     |    256     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |    3072    |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    6144    |
|    sptD.0.gcn.bias     |    256     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |    3072    |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |   12288    |
|    sptW.0.gcn.bias     |    256     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |    3072    |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 45285
 Trial: 73 Epoch: 0, train_loss: 109699.34399414062, validation_loss = 5490.7371826171875
 Trial: 73 Epoch: 1, train_loss: 36013.11853027344, validation_loss = 4758.519775390625
 Trial: 73 Epoch: 2, train_loss: 35756.181396484375, validation_loss = 4238.0452880859375
 Trial: 73 Epoch: 3, train_loss: 40927.216064453125, validation_loss = 5292.701171875
 Trial: 73 Epoch: 4, train_loss: 51870.50866699219, validation_loss = 3611.8077392578125
 Trial: 73 Epoch: 5, train_loss: 34592.08142089844, validation_loss = 5502.6942138671875
 Trial: 73 Epoch: 6, train_loss: 26272.344604492188, validation_loss = 4254.576171875
 Trial: 73 Epoch: 7, train_loss: 26146.813842773438, validation_loss = 4122.4647216796875
 Trial: 73 Epoch: 8, train_loss: 24868.309204101562, validation_loss = 3722.6070556640625
 Trial: 73 Epoch: 9, train_loss: 23833.117065429688, validation_loss = 3209.75830078125
 Trial: 73 Epoch: 10, train_loss: 23508.12841796875, validation_loss = 4264.7913818359375
 Trial: 73 Epoch: 11, train_loss: 24726.05938720703, validation_loss = 4218.785888671875
 Trial: 73 Epoch: 12, train_loss: 26648.404541015625, validation_loss = 3182.0714111328125
 Trial: 73 Epoch: 13, train_loss: 24196.66650390625, validation_loss = 4948.7550048828125
 Trial: 73 Epoch: 14, train_loss: 26372.069580078125, validation_loss = 4254.115478515625
 Trial: 73 Epoch: 15, train_loss: 23108.5908203125, validation_loss = 4038.6304931640625
 Trial: 73 Epoch: 16, train_loss: 22336.661254882812, validation_loss = 3214.5318603515625
 Trial: 73 Epoch: 17, train_loss: 20195.228759765625, validation_loss = 3635.0863037109375
 Trial: 73 Epoch: 18, train_loss: 23726.830200195312, validation_loss = 2898.5257568359375
 Trial: 73 Epoch: 19, train_loss: 21036.95037841797, validation_loss = 2747.7176513671875
 Trial: 73 Epoch: 20, train_loss: 19980.247802734375, validation_loss = 2938.996337890625
 Trial: 73 Epoch: 21, train_loss: 20191.23272705078, validation_loss = 3979.7442626953125
 Trial: 73 Epoch: 22, train_loss: 22020.78271484375, validation_loss = 3234.8939208984375
 Trial: 73 Epoch: 23, train_loss: 21171.789611816406, validation_loss = 3787.5340576171875
 Trial: 73 Epoch: 24, train_loss: 22460.625122070312, validation_loss = 3239.9398193359375
 Trial: 73 Epoch: 25, train_loss: 20788.74169921875, validation_loss = 3329.9876708984375
 Trial: 73 Epoch: 26, train_loss: 21199.640686035156, validation_loss = 2994.8489990234375
 Trial: 73 Epoch: 27, train_loss: 20334.36456298828, validation_loss = 2805.406982421875
 Trial: 73 Epoch: 28, train_loss: 21306.547485351562, validation_loss = 2827.4775390625
 Trial: 73 Epoch: 29, train_loss: 22901.91082763672, validation_loss = 2926.7635498046875
 Trial: 73 Epoch: 30, train_loss: 21648.638122558594, validation_loss = 4108.4266357421875
 Trial: 73 Epoch: 31, train_loss: 23576.707397460938, validation_loss = 5227.14697265625
 Trial: 73 Epoch: 32, train_loss: 25588.438598632812, validation_loss = 3010.421142578125
 Trial: 73 Epoch: 33, train_loss: 21121.363342285156, validation_loss = 3023.5267333984375
 Trial: 73 Epoch: 34, train_loss: 20042.959838867188, validation_loss = 3070.8009033203125
 Trial: 73 Epoch: 35, train_loss: 18983.45977783203, validation_loss = 3038.6171875
 Trial: 73 Epoch: 36, train_loss: 20930.756103515625, validation_loss = 2973.422119140625
 Trial: 73 Epoch: 37, train_loss: 20376.775390625, validation_loss = 3101.537353515625
 Trial: 73 Epoch: 38, train_loss: 19527.680053710938, validation_loss = 2934.701904296875
 Trial: 73 Epoch: 39, train_loss: 18007.125854492188, validation_loss = 2853.7332763671875
 Trial: 73 Epoch: 40, train_loss: 20059.386596679688, validation_loss = 2624.632080078125
 Trial: 73 Epoch: 41, train_loss: 17725.358947753906, validation_loss = 3001.55517578125
 Trial: 73 Epoch: 42, train_loss: 17811.303161621094, validation_loss = 2570.1043701171875
 Trial: 73 Epoch: 43, train_loss: 18735.54052734375, validation_loss = 3000.0689697265625
 Trial: 73 Epoch: 44, train_loss: 18707.727416992188, validation_loss = 3005.646728515625
 Trial: 73 Epoch: 45, train_loss: 19273.03924560547, validation_loss = 3546.0572509765625
 Trial: 73 Epoch: 46, train_loss: 18824.82843017578, validation_loss = 3156.30224609375
 Trial: 73 Epoch: 47, train_loss: 18423.970642089844, validation_loss = 2669.52880859375
 Trial: 73 Epoch: 48, train_loss: 18428.192504882812, validation_loss = 2586.342041015625
 Trial: 73 Epoch: 49, train_loss: 17212.109619140625, validation_loss = 3289.2938232421875
 Trial: 73 Epoch: 50, train_loss: 18491.78369140625, validation_loss = 2909.390625
 Trial: 73 Epoch: 51, train_loss: 18171.254333496094, validation_loss = 3410.8065185546875
 Trial: 73 Epoch: 52, train_loss: 18311.116821289062, validation_loss = 2943.2222900390625
 Trial: 73 Epoch: 53, train_loss: 20498.870178222656, validation_loss = 2517.3125
 Trial: 73 Epoch: 54, train_loss: 20408.140869140625, validation_loss = 2866.0887451171875
 Trial: 73 Epoch: 55, train_loss: 17544.16973876953, validation_loss = 2510.6331787109375
 Trial: 73 Epoch: 56, train_loss: 18030.858154296875, validation_loss = 2693.9915771484375
 Trial: 73 Epoch: 57, train_loss: 18712.153381347656, validation_loss = 2687.5538330078125
 Trial: 73 Epoch: 58, train_loss: 18428.64111328125, validation_loss = 2629.732177734375
 Trial: 73 Epoch: 59, train_loss: 17361.95977783203, validation_loss = 2576.6312255859375
 Trial: 73 Epoch: 60, train_loss: 18200.246643066406, validation_loss = 2540.1805419921875
 Trial: 73 Epoch: 61, train_loss: 19206.298828125, validation_loss = 2650.63525390625
 Trial: 73 Epoch: 62, train_loss: 18655.43359375, validation_loss = 2918.1558837890625
 Trial: 73 Epoch: 63, train_loss: 17837.10595703125, validation_loss = 2917.846923828125
 Trial: 73 Epoch: 64, train_loss: 19895.485778808594, validation_loss = 3576.1014404296875
 Trial: 73 Epoch: 65, train_loss: 18965.673889160156, validation_loss = 2677.4715576171875
 Trial: 73 Epoch: 66, train_loss: 17960.553588867188, validation_loss = 2807.3079833984375
 Trial: 73 Epoch: 67, train_loss: 16854.992248535156, validation_loss = 2522.065673828125
 Trial: 73 Epoch: 68, train_loss: 16467.581787109375, validation_loss = 2563.0465087890625
 Trial: 73 Epoch: 69, train_loss: 16243.486694335938, validation_loss = 2630.709228515625
 Trial: 73 Epoch: 70, train_loss: 17557.157287597656, validation_loss = 3168.6512451171875
 Trial: 73 Epoch: 71, train_loss: 16665.898681640625, validation_loss = 2503.0595703125
 Trial: 73 Epoch: 72, train_loss: 16248.6875, validation_loss = 2587.8824462890625
 Trial: 73 Epoch: 73, train_loss: 16759.34619140625, validation_loss = 2761.3602294921875
 Trial: 73 Epoch: 74, train_loss: 16166.912658691406, validation_loss = 2706.2864990234375
 Trial: 73 Epoch: 75, train_loss: 17210.885681152344, validation_loss = 3556.6922607421875
 Trial: 73 Epoch: 76, train_loss: 18689.216674804688, validation_loss = 2834.5125732421875
 Trial: 73 Epoch: 77, train_loss: 17211.594665527344, validation_loss = 2660.8568115234375
 Trial: 73 Epoch: 78, train_loss: 15925.941711425781, validation_loss = 2471.989501953125
 Trial: 73 Epoch: 79, train_loss: 19108.007446289062, validation_loss = 3724.2708740234375
 Trial: 73 Epoch: 80, train_loss: 16590.973754882812, validation_loss = 2813.6060791015625
 Trial: 73 Epoch: 81, train_loss: 17444.850158691406, validation_loss = 3029.0399169921875
 Trial: 73 Epoch: 82, train_loss: 16468.318420410156, validation_loss = 3365.8712158203125
 Trial: 73 Epoch: 83, train_loss: 16609.585693359375, validation_loss = 2902.53466796875
 Trial: 73 Epoch: 84, train_loss: 16960.84521484375, validation_loss = 2573.1414794921875
 Trial: 73 Epoch: 85, train_loss: 16984.09454345703, validation_loss = 2836.04736328125
 Trial: 73 Epoch: 86, train_loss: 17769.24334716797, validation_loss = 2643.1580810546875
 Trial: 73 Epoch: 87, train_loss: 19396.67022705078, validation_loss = 3604.897216796875
 Trial: 73 Epoch: 88, train_loss: 17312.12274169922, validation_loss = 2551.0711669921875
 Trial: 73 Epoch: 89, train_loss: 17726.565368652344, validation_loss = 2611.0577392578125
 Trial: 73 Epoch: 90, train_loss: 16125.643432617188, validation_loss = 3088.713134765625
 Trial: 73 Epoch: 91, train_loss: 17238.194213867188, validation_loss = 2660.3641357421875
 Trial: 73 Epoch: 92, train_loss: 16809.08526611328, validation_loss = 2478.7923583984375
 Trial: 73 Epoch: 93, train_loss: 16839.838317871094, validation_loss = 2654.925537109375
 Trial: 73 Epoch: 94, train_loss: 17462.364318847656, validation_loss = 2670.8170166015625
 Trial: 73 Epoch: 95, train_loss: 18118.55975341797, validation_loss = 2601.39501953125
 Trial: 73 Epoch: 96, train_loss: 16523.432495117188, validation_loss = 2571.3748779296875
 Trial: 73 Epoch: 97, train_loss: 15721.825622558594, validation_loss = 2479.918701171875
 Trial: 73 Epoch: 98, train_loss: 15448.615051269531, validation_loss = 2417.164306640625
 Trial: 73 Epoch: 99, train_loss: 15805.61083984375, validation_loss = 2748.7750244140625
========Trial 74 params: {'n_layers': 1, 'K': 2, 'Dropout': 0.1, 'Hidden0': 8, 'optimizer': 'Adam', 'lr': 0.009562189864351246, 'batch': 32}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |    384     |
|    sptH.0.gcn.bias     |     8      |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |     96     |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    192     |
|    sptD.0.gcn.bias     |     8      |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |     96     |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |    384     |
|    sptW.0.gcn.bias     |     8      |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |     96     |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 5853
 Trial: 74 Epoch: 0, train_loss: 113851.078125, validation_loss = 32306.87890625
========Trial 75 params: {'n_layers': 1, 'K': 2, 'Dropout': 0.1, 'Hidden0': 256, 'optimizer': 'Adam', 'lr': 0.006529012255381993, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |   12288    |
|    sptH.0.gcn.bias     |    256     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |    3072    |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    6144    |
|    sptD.0.gcn.bias     |    256     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |    3072    |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |   12288    |
|    sptW.0.gcn.bias     |    256     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |    3072    |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 45285
 Trial: 75 Epoch: 0, train_loss: 116105.81665039062, validation_loss = 5009.5828857421875
 Trial: 75 Epoch: 1, train_loss: 38397.8876953125, validation_loss = 5485.5137939453125
 Trial: 75 Epoch: 2, train_loss: 39238.34521484375, validation_loss = 5488.6092529296875
 Trial: 75 Epoch: 3, train_loss: 37858.899169921875, validation_loss = 4144.6551513671875
 Trial: 75 Epoch: 4, train_loss: 35797.266845703125, validation_loss = 3623.1376953125
 Trial: 75 Epoch: 5, train_loss: 28125.208129882812, validation_loss = 5913.0418701171875
 Trial: 75 Epoch: 6, train_loss: 25166.663330078125, validation_loss = 4007.43408203125
 Trial: 75 Epoch: 7, train_loss: 25490.993408203125, validation_loss = 4074.1126708984375
 Trial: 75 Epoch: 8, train_loss: 24416.492919921875, validation_loss = 3518.495361328125
 Trial: 75 Epoch: 9, train_loss: 23311.01495361328, validation_loss = 3256.4427490234375
 Trial: 75 Epoch: 10, train_loss: 23766.794067382812, validation_loss = 4784.5164794921875
 Trial: 75 Epoch: 11, train_loss: 25103.191467285156, validation_loss = 4395.707763671875
 Trial: 75 Epoch: 12, train_loss: 27285.719360351562, validation_loss = 3289.5020751953125
 Trial: 75 Epoch: 13, train_loss: 24766.26416015625, validation_loss = 4405.16796875
 Trial: 75 Epoch: 14, train_loss: 26844.569946289062, validation_loss = 5274.6148681640625
 Trial: 75 Epoch: 15, train_loss: 26156.169189453125, validation_loss = 5275.8553466796875
 Trial: 75 Epoch: 16, train_loss: 24632.3251953125, validation_loss = 3913.9259033203125
 Trial: 75 Epoch: 17, train_loss: 20291.41827392578, validation_loss = 3466.1461181640625
 Trial: 75 Epoch: 18, train_loss: 22531.606018066406, validation_loss = 3062.8323974609375
 Trial: 75 Epoch: 19, train_loss: 20377.38848876953, validation_loss = 2810.927001953125
 Trial: 75 Epoch: 20, train_loss: 20341.330688476562, validation_loss = 2911.5426025390625
 Trial: 75 Epoch: 21, train_loss: 20392.586059570312, validation_loss = 3729.6773681640625
 Trial: 75 Epoch: 22, train_loss: 21571.578186035156, validation_loss = 2958.73974609375
 Trial: 75 Epoch: 23, train_loss: 20630.92529296875, validation_loss = 3268.30908203125
 Trial: 75 Epoch: 24, train_loss: 21284.648864746094, validation_loss = 2925.9002685546875
 Trial: 75 Epoch: 25, train_loss: 19781.63720703125, validation_loss = 2884.536865234375
 Trial: 75 Epoch: 26, train_loss: 20659.83349609375, validation_loss = 2781.0203857421875
 Trial: 75 Epoch: 27, train_loss: 19669.48956298828, validation_loss = 2822.487548828125
 Trial: 75 Epoch: 28, train_loss: 20513.918090820312, validation_loss = 2928.8065185546875
 Trial: 75 Epoch: 29, train_loss: 21805.829162597656, validation_loss = 2951.3533935546875
 Trial: 75 Epoch: 30, train_loss: 21867.73681640625, validation_loss = 3512.381103515625
 Trial: 75 Epoch: 31, train_loss: 24130.695068359375, validation_loss = 6118.084228515625
 Trial: 75 Epoch: 32, train_loss: 26281.097045898438, validation_loss = 3082.0477294921875
 Trial: 75 Epoch: 33, train_loss: 21358.161987304688, validation_loss = 2900.1656494140625
 Trial: 75 Epoch: 34, train_loss: 21035.274291992188, validation_loss = 2821.4083251953125
 Trial: 75 Epoch: 35, train_loss: 19489.55712890625, validation_loss = 3050.855224609375
 Trial: 75 Epoch: 36, train_loss: 21369.248901367188, validation_loss = 2888.28759765625
 Trial: 75 Epoch: 37, train_loss: 20413.087524414062, validation_loss = 3056.5482177734375
 Trial: 75 Epoch: 38, train_loss: 18933.826232910156, validation_loss = 2728.5343017578125
 Trial: 75 Epoch: 39, train_loss: 18876.043334960938, validation_loss = 3253.6854248046875
 Trial: 75 Epoch: 40, train_loss: 21364.896057128906, validation_loss = 2913.2545166015625
 Trial: 75 Epoch: 41, train_loss: 18855.82940673828, validation_loss = 2678.287841796875
 Trial: 75 Epoch: 42, train_loss: 19196.513549804688, validation_loss = 2786.5233154296875
 Trial: 75 Epoch: 43, train_loss: 18195.79608154297, validation_loss = 2860.7169189453125
 Trial: 75 Epoch: 44, train_loss: 17858.6416015625, validation_loss = 3350.7386474609375
 Trial: 75 Epoch: 45, train_loss: 18669.934814453125, validation_loss = 4288.602294921875
 Trial: 75 Epoch: 46, train_loss: 19499.294067382812, validation_loss = 3188.70068359375
 Trial: 75 Epoch: 47, train_loss: 17832.455139160156, validation_loss = 2654.7779541015625
 Trial: 75 Epoch: 48, train_loss: 17803.707763671875, validation_loss = 2672.88818359375
 Trial: 75 Epoch: 49, train_loss: 16806.611877441406, validation_loss = 3469.14306640625
 Trial: 75 Epoch: 50, train_loss: 18600.138122558594, validation_loss = 2785.64404296875
 Trial: 75 Epoch: 51, train_loss: 17601.057556152344, validation_loss = 3063.8040771484375
 Trial: 75 Epoch: 52, train_loss: 18072.54656982422, validation_loss = 3030.064453125
 Trial: 75 Epoch: 53, train_loss: 19802.440795898438, validation_loss = 2557.0570068359375
 Trial: 75 Epoch: 54, train_loss: 18045.180603027344, validation_loss = 3032.32763671875
 Trial: 75 Epoch: 55, train_loss: 16845.792053222656, validation_loss = 2578.85986328125
 Trial: 75 Epoch: 56, train_loss: 17699.330505371094, validation_loss = 2846.1099853515625
 Trial: 75 Epoch: 57, train_loss: 17715.869567871094, validation_loss = 2728.84814453125
 Trial: 75 Epoch: 58, train_loss: 17637.57061767578, validation_loss = 2681.042724609375
 Trial: 75 Epoch: 59, train_loss: 16758.357177734375, validation_loss = 2630.3138427734375
 Trial: 75 Epoch: 60, train_loss: 17982.780639648438, validation_loss = 2635.0631103515625
 Trial: 75 Epoch: 61, train_loss: 18026.82440185547, validation_loss = 2622.09130859375
 Trial: 75 Epoch: 62, train_loss: 17957.67333984375, validation_loss = 3472.5245361328125
 Trial: 75 Epoch: 63, train_loss: 18187.484497070312, validation_loss = 2924.4412841796875
 Trial: 75 Epoch: 64, train_loss: 22352.51629638672, validation_loss = 3101.5604248046875
 Trial: 75 Epoch: 65, train_loss: 20133.021240234375, validation_loss = 2937.7532958984375
 Trial: 75 Epoch: 66, train_loss: 17328.569580078125, validation_loss = 2890.087158203125
 Trial: 75 Epoch: 67, train_loss: 17724.27569580078, validation_loss = 2500.3446044921875
 Trial: 75 Epoch: 68, train_loss: 16352.384460449219, validation_loss = 3126.2138671875
 Trial: 75 Epoch: 69, train_loss: 16044.334716796875, validation_loss = 2681.6475830078125
 Trial: 75 Epoch: 70, train_loss: 17890.83026123047, validation_loss = 3290.6739501953125
 Trial: 75 Epoch: 71, train_loss: 16729.794189453125, validation_loss = 2676.7938232421875
 Trial: 75 Epoch: 72, train_loss: 16192.724975585938, validation_loss = 2665.9736328125
 Trial: 75 Epoch: 73, train_loss: 17336.86944580078, validation_loss = 2762.6175537109375
 Trial: 75 Epoch: 74, train_loss: 16400.52276611328, validation_loss = 2919.267333984375
 Trial: 75 Epoch: 75, train_loss: 17055.343994140625, validation_loss = 2848.1141357421875
 Trial: 75 Epoch: 76, train_loss: 16336.620239257812, validation_loss = 3014.9151611328125
 Trial: 75 Epoch: 77, train_loss: 16672.710876464844, validation_loss = 2828.470458984375
 Trial: 75 Epoch: 78, train_loss: 15816.225891113281, validation_loss = 2650.1175537109375
 Trial: 75 Epoch: 79, train_loss: 17633.80584716797, validation_loss = 3231.973876953125
 Trial: 75 Epoch: 80, train_loss: 16261.976806640625, validation_loss = 2730.7117919921875
 Trial: 75 Epoch: 81, train_loss: 18616.710571289062, validation_loss = 2836.0733642578125
 Trial: 75 Epoch: 82, train_loss: 17134.659545898438, validation_loss = 4448.8658447265625
 Trial: 75 Epoch: 83, train_loss: 16385.036010742188, validation_loss = 2624.05419921875
 Trial: 75 Epoch: 84, train_loss: 15732.093078613281, validation_loss = 2730.115966796875
 Trial: 75 Epoch: 85, train_loss: 15627.521789550781, validation_loss = 2535.7882080078125
 Trial: 75 Epoch: 86, train_loss: 15534.414733886719, validation_loss = 2610.35302734375
 Trial: 75 Epoch: 87, train_loss: 15697.74057006836, validation_loss = 2688.164794921875
 Trial: 75 Epoch: 88, train_loss: 15455.712890625, validation_loss = 2674.419189453125
 Trial: 75 Epoch: 89, train_loss: 15703.674438476562, validation_loss = 2666.385009765625
 Trial: 75 Epoch: 90, train_loss: 14981.485046386719, validation_loss = 3051.41845703125
 Trial: 75 Epoch: 91, train_loss: 16428.559204101562, validation_loss = 2731.436767578125
 Trial: 75 Epoch: 92, train_loss: 16218.606506347656, validation_loss = 2546.4434814453125
 Trial: 75 Epoch: 93, train_loss: 16965.110778808594, validation_loss = 2785.579345703125
 Trial: 75 Epoch: 94, train_loss: 16209.715759277344, validation_loss = 2704.248046875
 Trial: 75 Epoch: 95, train_loss: 16303.559814453125, validation_loss = 2632.8245849609375
 Trial: 75 Epoch: 96, train_loss: 14911.942932128906, validation_loss = 2549.1090087890625
 Trial: 75 Epoch: 97, train_loss: 14918.649963378906, validation_loss = 2499.249267578125
 Trial: 75 Epoch: 98, train_loss: 14684.262145996094, validation_loss = 2470.8857421875
 Trial: 75 Epoch: 99, train_loss: 15374.076354980469, validation_loss = 2765.9970703125
========Trial 76 params: {'n_layers': 1, 'K': 2, 'Dropout': 0.1, 'Hidden0': 128, 'optimizer': 'Adam', 'lr': 0.0065689790486672195, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |    6144    |
|    sptH.0.gcn.bias     |    128     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |    1536    |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    3072    |
|    sptD.0.gcn.bias     |    128     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |    1536    |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |    6144    |
|    sptW.0.gcn.bias     |    128     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |    1536    |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 24933
 Trial: 76 Epoch: 0, train_loss: 148345.06201171875, validation_loss = 6523.534423828125
========Trial 77 params: {'n_layers': 1, 'K': 2, 'Dropout': 0.1, 'Hidden0': 16, 'optimizer': 'Adam', 'lr': 0.005508308259525884, 'batch': 16}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |    768     |
|    sptH.0.gcn.bias     |     16     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |    192     |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    384     |
|    sptD.0.gcn.bias     |     16     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |    192     |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |    768     |
|    sptW.0.gcn.bias     |     16     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |    192     |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 7125
 Trial: 77 Epoch: 0, train_loss: 150358.37109375, validation_loss = 26093.0859375
========Trial 78 params: {'n_layers': 1, 'K': 3, 'Dropout': 0.1, 'Hidden0': 256, 'optimizer': 'Adam', 'lr': 0.004335647175931196, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |   18432    |
|    sptH.0.gcn.bias     |    256     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |    3072    |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    9216    |
|    sptD.0.gcn.bias     |    256     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |    3072    |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |   18432    |
|    sptW.0.gcn.bias     |    256     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |    3072    |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 60645
 Trial: 78 Epoch: 0, train_loss: 224477.89794921875, validation_loss = 14650.49755859375
========Trial 79 params: {'n_layers': 1, 'K': 2, 'Dropout': 0.1, 'Hidden0': 256, 'optimizer': 'Adam', 'lr': 0.007841163160420675, 'batch': 8}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |   12288    |
|    sptH.0.gcn.bias     |    256     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |    3072    |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    6144    |
|    sptD.0.gcn.bias     |    256     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |    3072    |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |   12288    |
|    sptW.0.gcn.bias     |    256     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |    3072    |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 45285
 Trial: 79 Epoch: 0, train_loss: 175128.9189453125, validation_loss = 8813.7158203125
========Trial 80 params: {'n_layers': 1, 'K': 2, 'Dropout': 0.1, 'Hidden0': 256, 'optimizer': 'Adam', 'lr': 0.009622548839648472, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |   12288    |
|    sptH.0.gcn.bias     |    256     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |    3072    |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    6144    |
|    sptD.0.gcn.bias     |    256     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |    3072    |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |   12288    |
|    sptW.0.gcn.bias     |    256     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |    3072    |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 45285
 Trial: 80 Epoch: 0, train_loss: 109431.2783203125, validation_loss = 5273.432373046875
 Trial: 80 Epoch: 1, train_loss: 36407.45690917969, validation_loss = 4809.862548828125
 Trial: 80 Epoch: 2, train_loss: 36274.26416015625, validation_loss = 4252.883544921875
 Trial: 80 Epoch: 3, train_loss: 41802.66149902344, validation_loss = 5380.28857421875
 Trial: 80 Epoch: 4, train_loss: 51259.91418457031, validation_loss = 3687.173095703125
 Trial: 80 Epoch: 5, train_loss: 33411.09680175781, validation_loss = 5267.4266357421875
 Trial: 80 Epoch: 6, train_loss: 26089.14111328125, validation_loss = 4149.8177490234375
 Trial: 80 Epoch: 7, train_loss: 26285.215698242188, validation_loss = 3778.6790771484375
 Trial: 80 Epoch: 8, train_loss: 24609.53515625, validation_loss = 3703.7308349609375
 Trial: 80 Epoch: 9, train_loss: 23616.994873046875, validation_loss = 3185.4334716796875
 Trial: 80 Epoch: 10, train_loss: 23688.688842773438, validation_loss = 4448.0606689453125
 Trial: 80 Epoch: 11, train_loss: 25129.79656982422, validation_loss = 4300.047119140625
 Trial: 80 Epoch: 12, train_loss: 27529.691040039062, validation_loss = 3087.301513671875
 Trial: 80 Epoch: 13, train_loss: 24829.500732421875, validation_loss = 5017.4510498046875
 Trial: 80 Epoch: 14, train_loss: 27057.428344726562, validation_loss = 4472.4136962890625
 Trial: 80 Epoch: 15, train_loss: 24046.720642089844, validation_loss = 4142.218994140625
 Trial: 80 Epoch: 16, train_loss: 23279.083618164062, validation_loss = 3302.1907958984375
 Trial: 80 Epoch: 17, train_loss: 20228.706604003906, validation_loss = 3583.9248046875
 Trial: 80 Epoch: 18, train_loss: 23838.508728027344, validation_loss = 2951.1766357421875
 Trial: 80 Epoch: 19, train_loss: 21393.781005859375, validation_loss = 2742.425048828125
 Trial: 80 Epoch: 20, train_loss: 20248.0048828125, validation_loss = 2893.251708984375
 Trial: 80 Epoch: 21, train_loss: 20248.349365234375, validation_loss = 3837.1328125
 Trial: 80 Epoch: 22, train_loss: 22030.620544433594, validation_loss = 3091.24609375
 Trial: 80 Epoch: 23, train_loss: 21233.180541992188, validation_loss = 3629.9149169921875
 Trial: 80 Epoch: 24, train_loss: 22595.446655273438, validation_loss = 3183.186767578125
 Trial: 80 Epoch: 25, train_loss: 20872.876953125, validation_loss = 3538.58056640625
 Trial: 80 Epoch: 26, train_loss: 21309.91827392578, validation_loss = 3236.9686279296875
 Trial: 80 Epoch: 27, train_loss: 20728.67120361328, validation_loss = 2825.09814453125
 Trial: 80 Epoch: 28, train_loss: 21533.298461914062, validation_loss = 2801.9200439453125
 Trial: 80 Epoch: 29, train_loss: 23240.61474609375, validation_loss = 2902.3892822265625
 Trial: 80 Epoch: 30, train_loss: 22002.311645507812, validation_loss = 4052.3118896484375
 Trial: 80 Epoch: 31, train_loss: 23896.563842773438, validation_loss = 5310.9451904296875
 Trial: 80 Epoch: 32, train_loss: 25839.03350830078, validation_loss = 3037.0125732421875
 Trial: 80 Epoch: 33, train_loss: 21054.304748535156, validation_loss = 2997.2698974609375
 Trial: 80 Epoch: 34, train_loss: 20193.91162109375, validation_loss = 2956.982177734375
 Trial: 80 Epoch: 35, train_loss: 19424.767822265625, validation_loss = 3056.04833984375
 Trial: 80 Epoch: 36, train_loss: 21563.219665527344, validation_loss = 2935.42529296875
 Trial: 80 Epoch: 37, train_loss: 21200.696411132812, validation_loss = 3107.728759765625
 Trial: 80 Epoch: 38, train_loss: 19962.49542236328, validation_loss = 2944.3504638671875
 Trial: 80 Epoch: 39, train_loss: 18319.788513183594, validation_loss = 2862.2669677734375
 Trial: 80 Epoch: 40, train_loss: 20392.26904296875, validation_loss = 2610.627197265625
 Trial: 80 Epoch: 41, train_loss: 17847.3203125, validation_loss = 3151.795166015625
 Trial: 80 Epoch: 42, train_loss: 18035.041870117188, validation_loss = 2567.0279541015625
 Trial: 80 Epoch: 43, train_loss: 18966.77459716797, validation_loss = 2990.8209228515625
 Trial: 80 Epoch: 44, train_loss: 18945.125915527344, validation_loss = 2956.8671875
 Trial: 80 Epoch: 45, train_loss: 19232.411743164062, validation_loss = 3800.874755859375
 Trial: 80 Epoch: 46, train_loss: 18969.706665039062, validation_loss = 3143.0374755859375
 Trial: 80 Epoch: 47, train_loss: 18578.78839111328, validation_loss = 2647.1907958984375
 Trial: 80 Epoch: 48, train_loss: 18555.488403320312, validation_loss = 2562.7293701171875
 Trial: 80 Epoch: 49, train_loss: 17314.295471191406, validation_loss = 3325.6810302734375
 Trial: 80 Epoch: 50, train_loss: 18495.503173828125, validation_loss = 2799.190673828125
 Trial: 80 Epoch: 51, train_loss: 18042.419799804688, validation_loss = 3164.194580078125
 Trial: 80 Epoch: 52, train_loss: 18451.7431640625, validation_loss = 3013.259521484375
 Trial: 80 Epoch: 53, train_loss: 20558.204223632812, validation_loss = 2520.62158203125
 Trial: 80 Epoch: 54, train_loss: 20512.591186523438, validation_loss = 2804.87939453125
 Trial: 80 Epoch: 55, train_loss: 17664.04620361328, validation_loss = 2513.8922119140625
 Trial: 80 Epoch: 56, train_loss: 18224.347229003906, validation_loss = 2585.4722900390625
 Trial: 80 Epoch: 57, train_loss: 19242.822631835938, validation_loss = 2652.2010498046875
 Trial: 80 Epoch: 58, train_loss: 19131.794189453125, validation_loss = 2637.635009765625
 Trial: 80 Epoch: 59, train_loss: 17617.23486328125, validation_loss = 2602.8739013671875
 Trial: 80 Epoch: 60, train_loss: 18559.419555664062, validation_loss = 2570.750732421875
 Trial: 80 Epoch: 61, train_loss: 19234.066528320312, validation_loss = 2651.190185546875
 Trial: 80 Epoch: 62, train_loss: 18493.60516357422, validation_loss = 2927.525390625
 Trial: 80 Epoch: 63, train_loss: 17822.607666015625, validation_loss = 2908.2716064453125
 Trial: 80 Epoch: 64, train_loss: 19881.612548828125, validation_loss = 3389.2646484375
 Trial: 80 Epoch: 65, train_loss: 19056.56658935547, validation_loss = 2720.3494873046875
 Trial: 80 Epoch: 66, train_loss: 17842.451538085938, validation_loss = 2781.42724609375
 Trial: 80 Epoch: 67, train_loss: 17171.98944091797, validation_loss = 2545.6907958984375
 Trial: 80 Epoch: 68, train_loss: 16684.423706054688, validation_loss = 2593.94921875
 Trial: 80 Epoch: 69, train_loss: 16328.986938476562, validation_loss = 2600.948974609375
 Trial: 80 Epoch: 70, train_loss: 17762.65350341797, validation_loss = 3225.3055419921875
 Trial: 80 Epoch: 71, train_loss: 16774.549255371094, validation_loss = 2489.8349609375
 Trial: 80 Epoch: 72, train_loss: 16636.726135253906, validation_loss = 2575.5208740234375
 Trial: 80 Epoch: 73, train_loss: 16912.69500732422, validation_loss = 2789.4783935546875
 Trial: 80 Epoch: 74, train_loss: 16191.979187011719, validation_loss = 2734.8828125
 Trial: 80 Epoch: 75, train_loss: 17085.36602783203, validation_loss = 3537.2467041015625
 Trial: 80 Epoch: 76, train_loss: 18918.667358398438, validation_loss = 2891.332763671875
 Trial: 80 Epoch: 77, train_loss: 17199.48388671875, validation_loss = 2599.7398681640625
 Trial: 80 Epoch: 78, train_loss: 15971.270629882812, validation_loss = 2532.2855224609375
 Trial: 80 Epoch: 79, train_loss: 19313.157592773438, validation_loss = 3502.887451171875
 Trial: 80 Epoch: 80, train_loss: 16501.681701660156, validation_loss = 2780.7625732421875
 Trial: 80 Epoch: 81, train_loss: 17204.58477783203, validation_loss = 2926.475341796875
 Trial: 80 Epoch: 82, train_loss: 16621.799865722656, validation_loss = 3330.6802978515625
 Trial: 80 Epoch: 83, train_loss: 16701.683959960938, validation_loss = 2829.4036865234375
 Trial: 80 Epoch: 84, train_loss: 16880.367797851562, validation_loss = 2562.949462890625
 Trial: 80 Epoch: 85, train_loss: 17089.96221923828, validation_loss = 2846.0313720703125
 Trial: 80 Epoch: 86, train_loss: 18083.374389648438, validation_loss = 2631.84619140625
 Trial: 80 Epoch: 87, train_loss: 18919.613830566406, validation_loss = 3439.8037109375
 Trial: 80 Epoch: 88, train_loss: 16855.516845703125, validation_loss = 2510.1873779296875
 Trial: 80 Epoch: 89, train_loss: 17609.336364746094, validation_loss = 2640.3677978515625
 Trial: 80 Epoch: 90, train_loss: 16221.978820800781, validation_loss = 3005.356201171875
 Trial: 80 Epoch: 91, train_loss: 16916.04913330078, validation_loss = 2578.2213134765625
 Trial: 80 Epoch: 92, train_loss: 16854.91961669922, validation_loss = 2446.9619140625
 Trial: 80 Epoch: 93, train_loss: 16777.839233398438, validation_loss = 2537.8604736328125
 Trial: 80 Epoch: 94, train_loss: 17847.472290039062, validation_loss = 2800.5240478515625
 Trial: 80 Epoch: 95, train_loss: 18617.389709472656, validation_loss = 2635.4005126953125
 Trial: 80 Epoch: 96, train_loss: 17089.036376953125, validation_loss = 2650.366455078125
 Trial: 80 Epoch: 97, train_loss: 15916.016906738281, validation_loss = 2476.8050537109375
 Trial: 80 Epoch: 98, train_loss: 15495.947937011719, validation_loss = 2431.0301513671875
 Trial: 80 Epoch: 99, train_loss: 15894.9169921875, validation_loss = 2684.7784423828125
========Trial 81 params: {'n_layers': 1, 'K': 2, 'Dropout': 0.1, 'Hidden0': 256, 'optimizer': 'Adam', 'lr': 0.009558632058109124, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |   12288    |
|    sptH.0.gcn.bias     |    256     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |    3072    |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    6144    |
|    sptD.0.gcn.bias     |    256     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |    3072    |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |   12288    |
|    sptW.0.gcn.bias     |    256     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |    3072    |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 45285
 Trial: 81 Epoch: 0, train_loss: 109506.44506835938, validation_loss = 5228.156494140625
 Trial: 81 Epoch: 1, train_loss: 36241.408203125, validation_loss = 4829.051025390625
 Trial: 81 Epoch: 2, train_loss: 36315.37939453125, validation_loss = 4256.3001708984375
 Trial: 81 Epoch: 3, train_loss: 41507.256103515625, validation_loss = 5333.763916015625
 Trial: 81 Epoch: 4, train_loss: 50548.17883300781, validation_loss = 3623.49560546875
 Trial: 81 Epoch: 5, train_loss: 33265.54296875, validation_loss = 5297.92529296875
 Trial: 81 Epoch: 6, train_loss: 25886.20166015625, validation_loss = 4225.5457763671875
 Trial: 81 Epoch: 7, train_loss: 25720.4482421875, validation_loss = 3924.33544921875
 Trial: 81 Epoch: 8, train_loss: 24571.97021484375, validation_loss = 3657.5958251953125
 Trial: 81 Epoch: 9, train_loss: 23461.022583007812, validation_loss = 3204.673583984375
 Trial: 81 Epoch: 10, train_loss: 23629.450805664062, validation_loss = 4401.410400390625
 Trial: 81 Epoch: 11, train_loss: 25055.75848388672, validation_loss = 4379.202880859375
 Trial: 81 Epoch: 12, train_loss: 27362.807739257812, validation_loss = 3120.0047607421875
 Trial: 81 Epoch: 13, train_loss: 24660.053344726562, validation_loss = 4984.7969970703125
 Trial: 81 Epoch: 14, train_loss: 27019.703369140625, validation_loss = 4344.0238037109375
 Trial: 81 Epoch: 15, train_loss: 23519.5537109375, validation_loss = 4028.3038330078125
 Trial: 81 Epoch: 16, train_loss: 22876.29052734375, validation_loss = 3311.66357421875
 Trial: 81 Epoch: 17, train_loss: 20261.782104492188, validation_loss = 3754.9840087890625
 Trial: 81 Epoch: 18, train_loss: 24163.233154296875, validation_loss = 2935.81005859375
 Trial: 81 Epoch: 19, train_loss: 21557.99786376953, validation_loss = 2746.1019287109375
 Trial: 81 Epoch: 20, train_loss: 20202.22430419922, validation_loss = 2886.66796875
 Trial: 81 Epoch: 21, train_loss: 20229.773071289062, validation_loss = 3803.0177001953125
 Trial: 81 Epoch: 22, train_loss: 22027.837768554688, validation_loss = 3135.05712890625
 Trial: 81 Epoch: 23, train_loss: 21094.933837890625, validation_loss = 3739.5020751953125
 Trial: 81 Epoch: 24, train_loss: 23143.78271484375, validation_loss = 3224.4434814453125
 Trial: 81 Epoch: 25, train_loss: 21511.210510253906, validation_loss = 3608.145263671875
 Trial: 81 Epoch: 26, train_loss: 21249.486755371094, validation_loss = 3270.7283935546875
 Trial: 81 Epoch: 27, train_loss: 20598.932556152344, validation_loss = 2742.688720703125
 Trial: 81 Epoch: 28, train_loss: 21326.11669921875, validation_loss = 2733.17529296875
 Trial: 81 Epoch: 29, train_loss: 23171.69891357422, validation_loss = 2908.5714111328125
 Trial: 81 Epoch: 30, train_loss: 22014.578674316406, validation_loss = 4040.5433349609375
 Trial: 81 Epoch: 31, train_loss: 23878.092895507812, validation_loss = 5254.839111328125
 Trial: 81 Epoch: 32, train_loss: 25912.576171875, validation_loss = 3079.6500244140625
 Trial: 81 Epoch: 33, train_loss: 21065.452880859375, validation_loss = 3068.611328125
 Trial: 81 Epoch: 34, train_loss: 20131.03173828125, validation_loss = 2978.356689453125
 Trial: 81 Epoch: 35, train_loss: 19317.40948486328, validation_loss = 2899.260009765625
 Trial: 81 Epoch: 36, train_loss: 21200.50653076172, validation_loss = 2939.7933349609375
 Trial: 81 Epoch: 37, train_loss: 20997.451293945312, validation_loss = 3120.011962890625
 Trial: 81 Epoch: 38, train_loss: 19908.48309326172, validation_loss = 2859.529052734375
 Trial: 81 Epoch: 39, train_loss: 18206.737182617188, validation_loss = 2812.3900146484375
 Trial: 81 Epoch: 40, train_loss: 20002.52001953125, validation_loss = 2542.1483154296875
 Trial: 81 Epoch: 41, train_loss: 17933.700134277344, validation_loss = 2938.6982421875
 Trial: 81 Epoch: 42, train_loss: 17978.5390625, validation_loss = 2507.8458251953125
 Trial: 81 Epoch: 43, train_loss: 18949.76544189453, validation_loss = 2988.052978515625
 Trial: 81 Epoch: 44, train_loss: 18817.66485595703, validation_loss = 2958.859375
 Trial: 81 Epoch: 45, train_loss: 19768.220703125, validation_loss = 3880.5477294921875
 Trial: 81 Epoch: 46, train_loss: 19678.258544921875, validation_loss = 3061.8565673828125
 Trial: 81 Epoch: 47, train_loss: 18559.16680908203, validation_loss = 2580.7899169921875
 Trial: 81 Epoch: 48, train_loss: 18513.743713378906, validation_loss = 2628.2509765625
 Trial: 81 Epoch: 49, train_loss: 17777.963745117188, validation_loss = 3229.112548828125
 Trial: 81 Epoch: 50, train_loss: 18753.212158203125, validation_loss = 2784.842529296875
 Trial: 81 Epoch: 51, train_loss: 18159.217895507812, validation_loss = 3293.1055908203125
 Trial: 81 Epoch: 52, train_loss: 18483.588256835938, validation_loss = 2932.0723876953125
 Trial: 81 Epoch: 53, train_loss: 20788.678771972656, validation_loss = 2528.2203369140625
 Trial: 81 Epoch: 54, train_loss: 20868.796875, validation_loss = 2806.7977294921875
 Trial: 81 Epoch: 55, train_loss: 18092.775329589844, validation_loss = 2514.0028076171875
 Trial: 81 Epoch: 56, train_loss: 18380.41290283203, validation_loss = 2580.0849609375
 Trial: 81 Epoch: 57, train_loss: 19314.373168945312, validation_loss = 2629.54931640625
 Trial: 81 Epoch: 58, train_loss: 19203.686401367188, validation_loss = 2560.7769775390625
 Trial: 81 Epoch: 59, train_loss: 17833.87188720703, validation_loss = 2579.2742919921875
 Trial: 81 Epoch: 60, train_loss: 18676.45263671875, validation_loss = 2546.2017822265625
 Trial: 81 Epoch: 61, train_loss: 19507.75, validation_loss = 2638.0634765625
 Trial: 81 Epoch: 62, train_loss: 18792.889770507812, validation_loss = 2868.576416015625
 Trial: 81 Epoch: 63, train_loss: 17922.308349609375, validation_loss = 2811.72216796875
 Trial: 81 Epoch: 64, train_loss: 19342.765563964844, validation_loss = 3399.7525634765625
 Trial: 81 Epoch: 65, train_loss: 18770.374267578125, validation_loss = 2651.2435302734375
 Trial: 81 Epoch: 66, train_loss: 17732.295837402344, validation_loss = 2784.0152587890625
 Trial: 81 Epoch: 67, train_loss: 17304.476989746094, validation_loss = 2529.6929931640625
 Trial: 81 Epoch: 68, train_loss: 17044.15850830078, validation_loss = 2542.060546875
 Trial: 81 Epoch: 69, train_loss: 16408.12286376953, validation_loss = 2601.7723388671875
 Trial: 81 Epoch: 70, train_loss: 17728.5908203125, validation_loss = 3179.7060546875
 Trial: 81 Epoch: 71, train_loss: 16619.647216796875, validation_loss = 2465.8282470703125
 Trial: 81 Epoch: 72, train_loss: 16465.779052734375, validation_loss = 2554.623779296875
 Trial: 81 Epoch: 73, train_loss: 16850.410095214844, validation_loss = 2776.899658203125
 Trial: 81 Epoch: 74, train_loss: 16073.838256835938, validation_loss = 2636.632568359375
 Trial: 81 Epoch: 75, train_loss: 16934.744140625, validation_loss = 3391.5203857421875
 Trial: 81 Epoch: 76, train_loss: 19432.57196044922, validation_loss = 2945.1114501953125
 Trial: 81 Epoch: 77, train_loss: 17633.677001953125, validation_loss = 2540.2154541015625
 Trial: 81 Epoch: 78, train_loss: 15947.043518066406, validation_loss = 2510.8492431640625
 Trial: 81 Epoch: 79, train_loss: 19228.14910888672, validation_loss = 3443.2020263671875
 Trial: 81 Epoch: 80, train_loss: 16598.280212402344, validation_loss = 2658.5474853515625
 Trial: 81 Epoch: 81, train_loss: 16766.751892089844, validation_loss = 2889.8580322265625
 Trial: 81 Epoch: 82, train_loss: 16519.459106445312, validation_loss = 3280.7972412109375
 Trial: 81 Epoch: 83, train_loss: 16810.849060058594, validation_loss = 2752.6815185546875
 Trial: 81 Epoch: 84, train_loss: 16608.904846191406, validation_loss = 2508.1220703125
 Trial: 81 Epoch: 85, train_loss: 16611.58447265625, validation_loss = 2965.3575439453125
 Trial: 81 Epoch: 86, train_loss: 17735.461181640625, validation_loss = 2517.388671875
 Trial: 81 Epoch: 87, train_loss: 18807.025634765625, validation_loss = 3634.38427734375
 Trial: 81 Epoch: 88, train_loss: 16800.69482421875, validation_loss = 2489.7237548828125
 Trial: 81 Epoch: 89, train_loss: 17840.244750976562, validation_loss = 2753.434326171875
 Trial: 81 Epoch: 90, train_loss: 16785.94317626953, validation_loss = 3085.9376220703125
 Trial: 81 Epoch: 91, train_loss: 17655.89239501953, validation_loss = 2528.6624755859375
 Trial: 81 Epoch: 92, train_loss: 17289.162841796875, validation_loss = 2463.0418701171875
 Trial: 81 Epoch: 93, train_loss: 17037.573364257812, validation_loss = 2522.7154541015625
 Trial: 81 Epoch: 94, train_loss: 18071.629272460938, validation_loss = 2867.321533203125
 Trial: 81 Epoch: 95, train_loss: 18682.20196533203, validation_loss = 2664.6688232421875
 Trial: 81 Epoch: 96, train_loss: 17567.788513183594, validation_loss = 2677.153076171875
 Trial: 81 Epoch: 97, train_loss: 16034.227478027344, validation_loss = 2452.3414306640625
 Trial: 81 Epoch: 98, train_loss: 15416.240173339844, validation_loss = 2408.1009521484375
 Trial: 81 Epoch: 99, train_loss: 15726.562805175781, validation_loss = 2609.6026611328125
========Trial 82 params: {'n_layers': 1, 'K': 2, 'Dropout': 0.1, 'Hidden0': 256, 'optimizer': 'Adam', 'lr': 0.010914338384725051, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |   12288    |
|    sptH.0.gcn.bias     |    256     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |    3072    |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    6144    |
|    sptD.0.gcn.bias     |    256     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |    3072    |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |   12288    |
|    sptW.0.gcn.bias     |    256     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |    3072    |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 45285
 Trial: 82 Epoch: 0, train_loss: 113072.81689453125, validation_loss = 5257.9630126953125
 Trial: 82 Epoch: 1, train_loss: 38893.694091796875, validation_loss = 5014.4757080078125
 Trial: 82 Epoch: 2, train_loss: 35918.58935546875, validation_loss = 4550.96044921875
 Trial: 82 Epoch: 3, train_loss: 40966.34619140625, validation_loss = 5348.15478515625
 Trial: 82 Epoch: 4, train_loss: 57213.166748046875, validation_loss = 3883.0050048828125
 Trial: 82 Epoch: 5, train_loss: 38809.77673339844, validation_loss = 5591.0010986328125
 Trial: 82 Epoch: 6, train_loss: 27487.160278320312, validation_loss = 4312.763671875
 Trial: 82 Epoch: 7, train_loss: 27025.982055664062, validation_loss = 3818.390869140625
 Trial: 82 Epoch: 8, train_loss: 25186.627563476562, validation_loss = 3792.3804931640625
 Trial: 82 Epoch: 9, train_loss: 23966.042358398438, validation_loss = 3299.7154541015625
 Trial: 82 Epoch: 10, train_loss: 23874.995971679688, validation_loss = 4581.5064697265625
 Trial: 82 Epoch: 11, train_loss: 25283.46319580078, validation_loss = 4380.90185546875
 Trial: 82 Epoch: 12, train_loss: 27378.671997070312, validation_loss = 3188.81005859375
 Trial: 82 Epoch: 13, train_loss: 25070.313354492188, validation_loss = 5099.3199462890625
 Trial: 82 Epoch: 14, train_loss: 26433.437622070312, validation_loss = 4242.4451904296875
 Trial: 82 Epoch: 15, train_loss: 23383.77178955078, validation_loss = 4120.6961669921875
 Trial: 82 Epoch: 16, train_loss: 22712.782836914062, validation_loss = 3290.8433837890625
 Trial: 82 Epoch: 17, train_loss: 20649.00750732422, validation_loss = 3860.90380859375
 Trial: 82 Epoch: 18, train_loss: 23983.217529296875, validation_loss = 2956.6875
 Trial: 82 Epoch: 19, train_loss: 22096.429138183594, validation_loss = 2805.7255859375
 Trial: 82 Epoch: 20, train_loss: 20711.527587890625, validation_loss = 2969.404052734375
 Trial: 82 Epoch: 21, train_loss: 20672.13299560547, validation_loss = 4000.1759033203125
 Trial: 82 Epoch: 22, train_loss: 22740.22052001953, validation_loss = 3300.4302978515625
 Trial: 82 Epoch: 23, train_loss: 21752.489135742188, validation_loss = 3873.4512939453125
 Trial: 82 Epoch: 24, train_loss: 23723.431701660156, validation_loss = 3394.497314453125
 Trial: 82 Epoch: 25, train_loss: 21730.26123046875, validation_loss = 3467.0413818359375
 Trial: 82 Epoch: 26, train_loss: 22176.880859375, validation_loss = 3147.3367919921875
 Trial: 82 Epoch: 27, train_loss: 20945.968017578125, validation_loss = 2763.3521728515625
 Trial: 82 Epoch: 28, train_loss: 22200.41015625, validation_loss = 2779.8187255859375
 Trial: 82 Epoch: 29, train_loss: 24336.093383789062, validation_loss = 2882.4554443359375
 Trial: 82 Epoch: 30, train_loss: 22231.355895996094, validation_loss = 4009.289794921875
 Trial: 82 Epoch: 31, train_loss: 23504.03759765625, validation_loss = 4766.165283203125
 Trial: 82 Epoch: 32, train_loss: 25639.118408203125, validation_loss = 3016.694091796875
 Trial: 82 Epoch: 33, train_loss: 21179.300415039062, validation_loss = 2976.6260986328125
 Trial: 82 Epoch: 34, train_loss: 20576.7783203125, validation_loss = 2938.3193359375
 Trial: 82 Epoch: 35, train_loss: 19934.87432861328, validation_loss = 3194.9185791015625
 Trial: 82 Epoch: 36, train_loss: 22146.306701660156, validation_loss = 3074.8072509765625
 Trial: 82 Epoch: 37, train_loss: 21460.691284179688, validation_loss = 3183.0308837890625
 Trial: 82 Epoch: 38, train_loss: 20964.59979248047, validation_loss = 2869.8870849609375
 Trial: 82 Epoch: 39, train_loss: 18572.781677246094, validation_loss = 2746.803955078125
 Trial: 82 Epoch: 40, train_loss: 19646.773803710938, validation_loss = 2512.259521484375
 Trial: 82 Epoch: 41, train_loss: 18183.237854003906, validation_loss = 2732.28369140625
 Trial: 82 Epoch: 42, train_loss: 18174.42510986328, validation_loss = 2469.86962890625
 Trial: 82 Epoch: 43, train_loss: 19728.645935058594, validation_loss = 3072.0040283203125
 Trial: 82 Epoch: 44, train_loss: 19132.907775878906, validation_loss = 2821.7176513671875
 Trial: 82 Epoch: 45, train_loss: 20672.692321777344, validation_loss = 3595.284423828125
 Trial: 82 Epoch: 46, train_loss: 19820.783935546875, validation_loss = 3132.687744140625
 Trial: 82 Epoch: 47, train_loss: 19007.022583007812, validation_loss = 2511.6954345703125
 Trial: 82 Epoch: 48, train_loss: 18893.769653320312, validation_loss = 2521.45263671875
 Trial: 82 Epoch: 49, train_loss: 17750.9716796875, validation_loss = 3255.967529296875
 Trial: 82 Epoch: 50, train_loss: 19005.385864257812, validation_loss = 2695.5030517578125
 Trial: 82 Epoch: 51, train_loss: 18030.51629638672, validation_loss = 2978.8184814453125
 Trial: 82 Epoch: 52, train_loss: 18610.89825439453, validation_loss = 2677.1925048828125
 Trial: 82 Epoch: 53, train_loss: 20414.858764648438, validation_loss = 2451.2764892578125
 Trial: 82 Epoch: 54, train_loss: 21756.13330078125, validation_loss = 2638.681884765625
 Trial: 82 Epoch: 55, train_loss: 18444.772521972656, validation_loss = 2514.664794921875
 Trial: 82 Epoch: 56, train_loss: 17895.753051757812, validation_loss = 2438.7794189453125
 Trial: 82 Epoch: 57, train_loss: 19361.8720703125, validation_loss = 2736.053466796875
 Trial: 82 Epoch: 58, train_loss: 19922.271545410156, validation_loss = 2716.5289306640625
 Trial: 82 Epoch: 59, train_loss: 18742.040771484375, validation_loss = 2782.4451904296875
 Trial: 82 Epoch: 60, train_loss: 19036.924560546875, validation_loss = 2581.007568359375
 Trial: 82 Epoch: 61, train_loss: 20631.063842773438, validation_loss = 2707.5574951171875
 Trial: 82 Epoch: 62, train_loss: 19651.774291992188, validation_loss = 2829.1143798828125
 Trial: 82 Epoch: 63, train_loss: 18345.758239746094, validation_loss = 2758.6300048828125
 Trial: 82 Epoch: 64, train_loss: 19931.68096923828, validation_loss = 3310.0631103515625
 Trial: 82 Epoch: 65, train_loss: 19425.998474121094, validation_loss = 2635.1776123046875
 Trial: 82 Epoch: 66, train_loss: 18333.66375732422, validation_loss = 2678.0958251953125
 Trial: 82 Epoch: 67, train_loss: 17217.736206054688, validation_loss = 2419.399658203125
 Trial: 82 Epoch: 68, train_loss: 17227.86688232422, validation_loss = 2498.71337890625
 Trial: 82 Epoch: 69, train_loss: 16623.12664794922, validation_loss = 2488.142333984375
 Trial: 82 Epoch: 70, train_loss: 17881.95343017578, validation_loss = 2991.668701171875
 Trial: 82 Epoch: 71, train_loss: 16824.1181640625, validation_loss = 2399.6658935546875
 Trial: 82 Epoch: 72, train_loss: 16791.017456054688, validation_loss = 2458.90869140625
 Trial: 82 Epoch: 73, train_loss: 17957.068908691406, validation_loss = 2709.15869140625
 Trial: 82 Epoch: 74, train_loss: 16765.734375, validation_loss = 2506.2008056640625
 Trial: 82 Epoch: 75, train_loss: 17276.659118652344, validation_loss = 3381.5389404296875
 Trial: 82 Epoch: 76, train_loss: 19526.571838378906, validation_loss = 2830.333740234375
 Trial: 82 Epoch: 77, train_loss: 17941.692138671875, validation_loss = 2471.2176513671875
 Trial: 82 Epoch: 78, train_loss: 16031.731628417969, validation_loss = 2493.1553955078125
 Trial: 82 Epoch: 79, train_loss: 18881.64910888672, validation_loss = 3431.33740234375
 Trial: 82 Epoch: 80, train_loss: 16778.341735839844, validation_loss = 2781.584716796875
 Trial: 82 Epoch: 81, train_loss: 18011.21502685547, validation_loss = 2980.55419921875
 Trial: 82 Epoch: 82, train_loss: 17344.29034423828, validation_loss = 3084.9185791015625
 Trial: 82 Epoch: 83, train_loss: 17176.06072998047, validation_loss = 2704.412109375
 Trial: 82 Epoch: 84, train_loss: 16995.82635498047, validation_loss = 2467.024658203125
 Trial: 82 Epoch: 85, train_loss: 16734.41485595703, validation_loss = 2592.0301513671875
 Trial: 82 Epoch: 86, train_loss: 16593.801025390625, validation_loss = 2393.1885986328125
 Trial: 82 Epoch: 87, train_loss: 18518.92010498047, validation_loss = 3305.3238525390625
 Trial: 82 Epoch: 88, train_loss: 17029.251525878906, validation_loss = 2420.147216796875
 Trial: 82 Epoch: 89, train_loss: 18487.688354492188, validation_loss = 2655.0498046875
 Trial: 82 Epoch: 90, train_loss: 17793.212951660156, validation_loss = 3265.158447265625
 Trial: 82 Epoch: 91, train_loss: 19208.054565429688, validation_loss = 2464.3800048828125
 Trial: 82 Epoch: 92, train_loss: 17717.350646972656, validation_loss = 2634.0679931640625
 Trial: 82 Epoch: 93, train_loss: 16803.519775390625, validation_loss = 2409.292724609375
 Trial: 82 Epoch: 94, train_loss: 17731.695922851562, validation_loss = 2760.5950927734375
 Trial: 82 Epoch: 95, train_loss: 17381.339233398438, validation_loss = 2525.3656005859375
 Trial: 82 Epoch: 96, train_loss: 18068.63934326172, validation_loss = 2490.8231201171875
 Trial: 82 Epoch: 97, train_loss: 16064.971496582031, validation_loss = 2265.296630859375
 Trial: 82 Epoch: 98, train_loss: 16081.061096191406, validation_loss = 2272.614990234375
 Trial: 82 Epoch: 99, train_loss: 16399.022888183594, validation_loss = 2619.4093017578125
========Trial 83 params: {'n_layers': 1, 'K': 2, 'Dropout': 0.1, 'Hidden0': 256, 'optimizer': 'Adam', 'lr': 0.01088226016772659, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |   12288    |
|    sptH.0.gcn.bias     |    256     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |    3072    |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    6144    |
|    sptD.0.gcn.bias     |    256     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |    3072    |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |   12288    |
|    sptW.0.gcn.bias     |    256     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |    3072    |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 45285
 Trial: 83 Epoch: 0, train_loss: 112774.13427734375, validation_loss = 5248.430908203125
 Trial: 83 Epoch: 1, train_loss: 38861.71154785156, validation_loss = 4994.898681640625
 Trial: 83 Epoch: 2, train_loss: 36107.25439453125, validation_loss = 4493.008544921875
 Trial: 83 Epoch: 3, train_loss: 40883.81201171875, validation_loss = 5387.954833984375
 Trial: 83 Epoch: 4, train_loss: 57571.238037109375, validation_loss = 3925.7115478515625
 Trial: 83 Epoch: 5, train_loss: 39313.573486328125, validation_loss = 5442.856201171875
 Trial: 83 Epoch: 6, train_loss: 27778.239990234375, validation_loss = 4164.9263916015625
 Trial: 83 Epoch: 7, train_loss: 27607.316528320312, validation_loss = 3708.8076171875
 Trial: 83 Epoch: 8, train_loss: 25301.072265625, validation_loss = 3806.4730224609375
 Trial: 83 Epoch: 9, train_loss: 23978.237182617188, validation_loss = 3322.59912109375
 Trial: 83 Epoch: 10, train_loss: 24013.525756835938, validation_loss = 4408.0628662109375
 Trial: 83 Epoch: 11, train_loss: 25042.766479492188, validation_loss = 4361.739501953125
 Trial: 83 Epoch: 12, train_loss: 26962.314086914062, validation_loss = 3150.5908203125
 Trial: 83 Epoch: 13, train_loss: 24815.284057617188, validation_loss = 5189.0189208984375
 Trial: 83 Epoch: 14, train_loss: 26828.789672851562, validation_loss = 4090.8878173828125
 Trial: 83 Epoch: 15, train_loss: 23209.419799804688, validation_loss = 3877.717041015625
 Trial: 83 Epoch: 16, train_loss: 22512.5244140625, validation_loss = 3231.0699462890625
 Trial: 83 Epoch: 17, train_loss: 20546.433959960938, validation_loss = 3853.849365234375
 Trial: 83 Epoch: 18, train_loss: 23680.701171875, validation_loss = 2982.419677734375
 Trial: 83 Epoch: 19, train_loss: 21827.10662841797, validation_loss = 2853.3822021484375
 Trial: 83 Epoch: 20, train_loss: 20842.186157226562, validation_loss = 2962.3712158203125
 Trial: 83 Epoch: 21, train_loss: 20717.83984375, validation_loss = 4112.66455078125
 Trial: 83 Epoch: 22, train_loss: 22967.993530273438, validation_loss = 3350.25390625
 Trial: 83 Epoch: 23, train_loss: 21808.757446289062, validation_loss = 3870.9940185546875
 Trial: 83 Epoch: 24, train_loss: 23547.667907714844, validation_loss = 3435.5660400390625
 Trial: 83 Epoch: 25, train_loss: 21698.973754882812, validation_loss = 3644.178466796875
 Trial: 83 Epoch: 26, train_loss: 21610.284118652344, validation_loss = 3226.9873046875
 Trial: 83 Epoch: 27, train_loss: 20805.3681640625, validation_loss = 2840.573486328125
 Trial: 83 Epoch: 28, train_loss: 21730.322631835938, validation_loss = 2762.3348388671875
 Trial: 83 Epoch: 29, train_loss: 23564.571655273438, validation_loss = 2800.395263671875
 Trial: 83 Epoch: 30, train_loss: 21605.85089111328, validation_loss = 4230.3834228515625
 Trial: 83 Epoch: 31, train_loss: 23216.749145507812, validation_loss = 4617.18896484375
 Trial: 83 Epoch: 32, train_loss: 25382.85626220703, validation_loss = 3052.0537109375
 Trial: 83 Epoch: 33, train_loss: 20801.26092529297, validation_loss = 3145.8338623046875
 Trial: 83 Epoch: 34, train_loss: 20401.723876953125, validation_loss = 3015.807373046875
 Trial: 83 Epoch: 35, train_loss: 19438.474670410156, validation_loss = 3099.085693359375
 Trial: 83 Epoch: 36, train_loss: 21594.767822265625, validation_loss = 3067.6934814453125
 Trial: 83 Epoch: 37, train_loss: 20656.332153320312, validation_loss = 3159.6641845703125
 Trial: 83 Epoch: 38, train_loss: 20150.284912109375, validation_loss = 2890.06787109375
 Trial: 83 Epoch: 39, train_loss: 18244.969177246094, validation_loss = 2840.196044921875
 Trial: 83 Epoch: 40, train_loss: 19277.522338867188, validation_loss = 2606.12939453125
 Trial: 83 Epoch: 41, train_loss: 17963.721923828125, validation_loss = 2798.880126953125
 Trial: 83 Epoch: 42, train_loss: 18353.356994628906, validation_loss = 2503.515380859375
 Trial: 83 Epoch: 43, train_loss: 19305.10076904297, validation_loss = 3068.9151611328125
 Trial: 83 Epoch: 44, train_loss: 19129.011840820312, validation_loss = 2856.6326904296875
 Trial: 83 Epoch: 45, train_loss: 21005.9765625, validation_loss = 3711.0760498046875
 Trial: 83 Epoch: 46, train_loss: 20056.574462890625, validation_loss = 3332.98828125
 Trial: 83 Epoch: 47, train_loss: 19043.474975585938, validation_loss = 2628.001708984375
 Trial: 83 Epoch: 48, train_loss: 18737.05059814453, validation_loss = 2669.7252197265625
 Trial: 83 Epoch: 49, train_loss: 17783.66046142578, validation_loss = 3423.3499755859375
 Trial: 83 Epoch: 50, train_loss: 18925.130859375, validation_loss = 2811.16015625
 Trial: 83 Epoch: 51, train_loss: 17974.890014648438, validation_loss = 3288.400146484375
 Trial: 83 Epoch: 52, train_loss: 19161.228942871094, validation_loss = 3155.7786865234375
 Trial: 83 Epoch: 53, train_loss: 21491.076904296875, validation_loss = 2548.4296875
 Trial: 83 Epoch: 54, train_loss: 21001.75506591797, validation_loss = 2787.927001953125
 Trial: 83 Epoch: 55, train_loss: 18059.831176757812, validation_loss = 2591.8642578125
 Trial: 83 Epoch: 56, train_loss: 18344.306213378906, validation_loss = 2543.154541015625
 Trial: 83 Epoch: 57, train_loss: 19630.745056152344, validation_loss = 2638.54541015625
 Trial: 83 Epoch: 58, train_loss: 19087.785766601562, validation_loss = 2604.5057373046875
 Trial: 83 Epoch: 59, train_loss: 17692.55322265625, validation_loss = 2627.9306640625
 Trial: 83 Epoch: 60, train_loss: 18292.62469482422, validation_loss = 2615.7132568359375
 Trial: 83 Epoch: 61, train_loss: 19706.06103515625, validation_loss = 2758.1829833984375
 Trial: 83 Epoch: 62, train_loss: 18950.8525390625, validation_loss = 2866.4368896484375
 Trial: 83 Epoch: 63, train_loss: 17347.844604492188, validation_loss = 2783.4739990234375
 Trial: 83 Epoch: 64, train_loss: 19304.19696044922, validation_loss = 3454.326171875
 Trial: 83 Epoch: 65, train_loss: 18804.987365722656, validation_loss = 2743.4600830078125
 Trial: 83 Epoch: 66, train_loss: 17668.510009765625, validation_loss = 2847.810302734375
 Trial: 83 Epoch: 67, train_loss: 17288.69305419922, validation_loss = 2484.6885986328125
 Trial: 83 Epoch: 68, train_loss: 16735.279357910156, validation_loss = 2603.0679931640625
 Trial: 83 Epoch: 69, train_loss: 16210.211120605469, validation_loss = 2551.056640625
 Trial: 83 Epoch: 70, train_loss: 18036.484252929688, validation_loss = 3291.529052734375
 Trial: 83 Epoch: 71, train_loss: 16552.729431152344, validation_loss = 2512.4315185546875
 Trial: 83 Epoch: 72, train_loss: 16343.392761230469, validation_loss = 2518.43505859375
 Trial: 83 Epoch: 73, train_loss: 16612.7724609375, validation_loss = 2739.951416015625
 Trial: 83 Epoch: 74, train_loss: 16311.402404785156, validation_loss = 2579.7579345703125
 Trial: 83 Epoch: 75, train_loss: 16932.89337158203, validation_loss = 3563.359130859375
 Trial: 83 Epoch: 76, train_loss: 19429.624755859375, validation_loss = 2876.451904296875
 Trial: 83 Epoch: 77, train_loss: 17341.562438964844, validation_loss = 2705.8555908203125
 Trial: 83 Epoch: 78, train_loss: 16019.36962890625, validation_loss = 2575.739990234375
 Trial: 83 Epoch: 79, train_loss: 19332.704711914062, validation_loss = 3554.271484375
 Trial: 83 Epoch: 80, train_loss: 16696.44012451172, validation_loss = 2855.3424072265625
 Trial: 83 Epoch: 81, train_loss: 17008.783203125, validation_loss = 3067.8868408203125
 Trial: 83 Epoch: 82, train_loss: 16510.35205078125, validation_loss = 3153.983642578125
 Trial: 83 Epoch: 83, train_loss: 17395.675231933594, validation_loss = 2865.3411865234375
 Trial: 83 Epoch: 84, train_loss: 16413.088623046875, validation_loss = 2606.363525390625
 Trial: 83 Epoch: 85, train_loss: 16184.0869140625, validation_loss = 2858.401123046875
 Trial: 83 Epoch: 86, train_loss: 16536.090393066406, validation_loss = 2541.581298828125
 Trial: 83 Epoch: 87, train_loss: 17393.853759765625, validation_loss = 3218.533447265625
 Trial: 83 Epoch: 88, train_loss: 16302.279296875, validation_loss = 2474.3790283203125
 Trial: 83 Epoch: 89, train_loss: 17682.779541015625, validation_loss = 2718.33544921875
 Trial: 83 Epoch: 90, train_loss: 17000.20928955078, validation_loss = 3473.025634765625
 Trial: 83 Epoch: 91, train_loss: 18041.55126953125, validation_loss = 2624.78515625
 Trial: 83 Epoch: 92, train_loss: 17270.868286132812, validation_loss = 2715.6060791015625
 Trial: 83 Epoch: 93, train_loss: 16290.676147460938, validation_loss = 2519.107177734375
 Trial: 83 Epoch: 94, train_loss: 17446.300170898438, validation_loss = 2870.522705078125
 Trial: 83 Epoch: 95, train_loss: 17091.90753173828, validation_loss = 2739.2012939453125
 Trial: 83 Epoch: 96, train_loss: 17663.2705078125, validation_loss = 2635.3209228515625
 Trial: 83 Epoch: 97, train_loss: 15518.7646484375, validation_loss = 2415.7069091796875
 Trial: 83 Epoch: 98, train_loss: 15480.009155273438, validation_loss = 2382.701171875
 Trial: 83 Epoch: 99, train_loss: 15662.114807128906, validation_loss = 2717.6785888671875
========Trial 84 params: {'n_layers': 1, 'K': 2, 'Dropout': 0.1, 'Hidden0': 256, 'optimizer': 'Adam', 'lr': 0.010868509429095076, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |   12288    |
|    sptH.0.gcn.bias     |    256     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |    3072    |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    6144    |
|    sptD.0.gcn.bias     |    256     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |    3072    |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |   12288    |
|    sptW.0.gcn.bias     |    256     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |    3072    |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 45285
 Trial: 84 Epoch: 0, train_loss: 112649.904296875, validation_loss = 5268.3763427734375
 Trial: 84 Epoch: 1, train_loss: 38873.46032714844, validation_loss = 4983.8934326171875
 Trial: 84 Epoch: 2, train_loss: 35969.0703125, validation_loss = 4552.3475341796875
 Trial: 84 Epoch: 3, train_loss: 40952.082763671875, validation_loss = 5413.908935546875
 Trial: 84 Epoch: 4, train_loss: 58215.56164550781, validation_loss = 3959.135009765625
 Trial: 84 Epoch: 5, train_loss: 39404.22790527344, validation_loss = 5294.0965576171875
 Trial: 84 Epoch: 6, train_loss: 27895.10595703125, validation_loss = 4130.6982421875
 Trial: 84 Epoch: 7, train_loss: 27941.53271484375, validation_loss = 3643.1181640625
 Trial: 84 Epoch: 8, train_loss: 25247.079345703125, validation_loss = 3814.5616455078125
 Trial: 84 Epoch: 9, train_loss: 23940.988525390625, validation_loss = 3331.87939453125
 Trial: 84 Epoch: 10, train_loss: 23850.198364257812, validation_loss = 4392.3056640625
 Trial: 84 Epoch: 11, train_loss: 24986.4970703125, validation_loss = 4297.564453125
 Trial: 84 Epoch: 12, train_loss: 27190.872802734375, validation_loss = 3190.640869140625
 Trial: 84 Epoch: 13, train_loss: 25236.22607421875, validation_loss = 4854.905029296875
 Trial: 84 Epoch: 14, train_loss: 26945.706909179688, validation_loss = 4465.2327880859375
 Trial: 84 Epoch: 15, train_loss: 23708.829711914062, validation_loss = 4106.8406982421875
 Trial: 84 Epoch: 16, train_loss: 22864.1650390625, validation_loss = 3309.303466796875
 Trial: 84 Epoch: 17, train_loss: 20783.84552001953, validation_loss = 3847.40673828125
 Trial: 84 Epoch: 18, train_loss: 24237.857788085938, validation_loss = 2974.7860107421875
 Trial: 84 Epoch: 19, train_loss: 21631.375427246094, validation_loss = 2813.721923828125
 Trial: 84 Epoch: 20, train_loss: 20530.11199951172, validation_loss = 3015.2437744140625
 Trial: 84 Epoch: 21, train_loss: 20581.818481445312, validation_loss = 3971.7420654296875
 Trial: 84 Epoch: 22, train_loss: 22522.340454101562, validation_loss = 3309.921142578125
 Trial: 84 Epoch: 23, train_loss: 21545.794860839844, validation_loss = 3911.527587890625
 Trial: 84 Epoch: 24, train_loss: 23388.73828125, validation_loss = 3409.92333984375
 Trial: 84 Epoch: 25, train_loss: 21537.472534179688, validation_loss = 3484.09716796875
 Trial: 84 Epoch: 26, train_loss: 21969.28466796875, validation_loss = 3125.1800537109375
 Trial: 84 Epoch: 27, train_loss: 20756.7138671875, validation_loss = 2806.35205078125
 Trial: 84 Epoch: 28, train_loss: 22144.723876953125, validation_loss = 2823.4757080078125
 Trial: 84 Epoch: 29, train_loss: 24040.198120117188, validation_loss = 2929.8804931640625
 Trial: 84 Epoch: 30, train_loss: 21709.024658203125, validation_loss = 4097.2119140625
 Trial: 84 Epoch: 31, train_loss: 23734.384643554688, validation_loss = 5002.2159423828125
 Trial: 84 Epoch: 32, train_loss: 25586.422302246094, validation_loss = 3044.1832275390625
 Trial: 84 Epoch: 33, train_loss: 21191.579528808594, validation_loss = 3005.7882080078125
 Trial: 84 Epoch: 34, train_loss: 20217.415405273438, validation_loss = 3092.7818603515625
 Trial: 84 Epoch: 35, train_loss: 20011.292541503906, validation_loss = 3185.462646484375
 Trial: 84 Epoch: 36, train_loss: 21938.869201660156, validation_loss = 3056.0648193359375
 Trial: 84 Epoch: 37, train_loss: 21319.267944335938, validation_loss = 3264.2198486328125
 Trial: 84 Epoch: 38, train_loss: 20638.18536376953, validation_loss = 2940.6959228515625
 Trial: 84 Epoch: 39, train_loss: 18452.719787597656, validation_loss = 2779.7474365234375
 Trial: 84 Epoch: 40, train_loss: 19542.18719482422, validation_loss = 2555.2532958984375
 Trial: 84 Epoch: 41, train_loss: 18272.542724609375, validation_loss = 2698.2252197265625
 Trial: 84 Epoch: 42, train_loss: 18016.005798339844, validation_loss = 2523.67333984375
 Trial: 84 Epoch: 43, train_loss: 19353.005798339844, validation_loss = 3062.435791015625
 Trial: 84 Epoch: 44, train_loss: 19020.631225585938, validation_loss = 2885.7784423828125
 Trial: 84 Epoch: 45, train_loss: 21042.017639160156, validation_loss = 3700.876220703125
 Trial: 84 Epoch: 46, train_loss: 19899.509704589844, validation_loss = 3283.3538818359375
 Trial: 84 Epoch: 47, train_loss: 18954.33282470703, validation_loss = 2559.0565185546875
 Trial: 84 Epoch: 48, train_loss: 18833.258728027344, validation_loss = 2539.3975830078125
 Trial: 84 Epoch: 49, train_loss: 17592.890991210938, validation_loss = 3323.5205078125
 Trial: 84 Epoch: 50, train_loss: 19305.423828125, validation_loss = 2811.3599853515625
 Trial: 84 Epoch: 51, train_loss: 18126.783752441406, validation_loss = 3201.4669189453125
 Trial: 84 Epoch: 52, train_loss: 18247.64813232422, validation_loss = 2966.89453125
 Trial: 84 Epoch: 53, train_loss: 20866.29541015625, validation_loss = 2507.0784912109375
 Trial: 84 Epoch: 54, train_loss: 20954.112243652344, validation_loss = 2716.29248046875
 Trial: 84 Epoch: 55, train_loss: 17929.253051757812, validation_loss = 2544.4967041015625
 Trial: 84 Epoch: 56, train_loss: 17724.846130371094, validation_loss = 2519.097900390625
 Trial: 84 Epoch: 57, train_loss: 19078.319458007812, validation_loss = 2774.9771728515625
 Trial: 84 Epoch: 58, train_loss: 19737.903930664062, validation_loss = 2775.3687744140625
 Trial: 84 Epoch: 59, train_loss: 18331.580200195312, validation_loss = 2923.1126708984375
 Trial: 84 Epoch: 60, train_loss: 18513.368896484375, validation_loss = 2640.5811767578125
 Trial: 84 Epoch: 61, train_loss: 19988.979736328125, validation_loss = 2817.693603515625
 Trial: 84 Epoch: 62, train_loss: 19298.591186523438, validation_loss = 2889.4559326171875
 Trial: 84 Epoch: 63, train_loss: 17874.075927734375, validation_loss = 2754.1104736328125
 Trial: 84 Epoch: 64, train_loss: 18251.975524902344, validation_loss = 3156.872802734375
 Trial: 84 Epoch: 65, train_loss: 18187.77032470703, validation_loss = 2628.24658203125
 Trial: 84 Epoch: 66, train_loss: 17292.151794433594, validation_loss = 2748.09228515625
 Trial: 84 Epoch: 67, train_loss: 17176.442993164062, validation_loss = 2476.7857666015625
 Trial: 84 Epoch: 68, train_loss: 17062.811279296875, validation_loss = 2570.25341796875
 Trial: 84 Epoch: 69, train_loss: 16623.619689941406, validation_loss = 2674.0260009765625
 Trial: 84 Epoch: 70, train_loss: 17549.57452392578, validation_loss = 3027.191162109375
 Trial: 84 Epoch: 71, train_loss: 16659.85528564453, validation_loss = 2562.2928466796875
 Trial: 84 Epoch: 72, train_loss: 16437.617736816406, validation_loss = 2475.4398193359375
 Trial: 84 Epoch: 73, train_loss: 17624.1591796875, validation_loss = 2741.4417724609375
 Trial: 84 Epoch: 74, train_loss: 16335.329162597656, validation_loss = 2654.8668212890625
 Trial: 84 Epoch: 75, train_loss: 16644.6455078125, validation_loss = 3316.8436279296875
 Trial: 84 Epoch: 76, train_loss: 18956.22930908203, validation_loss = 3240.85107421875
 Trial: 84 Epoch: 77, train_loss: 18301.476196289062, validation_loss = 2525.7003173828125
 Trial: 84 Epoch: 78, train_loss: 16052.324645996094, validation_loss = 2502.2774658203125
 Trial: 84 Epoch: 79, train_loss: 19047.870239257812, validation_loss = 3560.6947021484375
 Trial: 84 Epoch: 80, train_loss: 16382.987731933594, validation_loss = 2757.2640380859375
 Trial: 84 Epoch: 81, train_loss: 16879.30389404297, validation_loss = 3014.1158447265625
 Trial: 84 Epoch: 82, train_loss: 16698.418579101562, validation_loss = 3134.609619140625
 Trial: 84 Epoch: 83, train_loss: 16491.17901611328, validation_loss = 2808.5391845703125
 Trial: 84 Epoch: 84, train_loss: 16380.595825195312, validation_loss = 2693.9844970703125
 Trial: 84 Epoch: 85, train_loss: 16135.785339355469, validation_loss = 2661.0823974609375
 Trial: 84 Epoch: 86, train_loss: 16240.775390625, validation_loss = 2560.5150146484375
 Trial: 84 Epoch: 87, train_loss: 17509.65008544922, validation_loss = 3305.9735107421875
 Trial: 84 Epoch: 88, train_loss: 16447.546264648438, validation_loss = 2469.8271484375
 Trial: 84 Epoch: 89, train_loss: 17469.15692138672, validation_loss = 2513.1683349609375
 Trial: 84 Epoch: 90, train_loss: 17267.42987060547, validation_loss = 3877.1741943359375
 Trial: 84 Epoch: 91, train_loss: 19131.23065185547, validation_loss = 2559.0146484375
 Trial: 84 Epoch: 92, train_loss: 17940.52264404297, validation_loss = 2966.1734619140625
 Trial: 84 Epoch: 93, train_loss: 16670.55194091797, validation_loss = 2436.89208984375
 Trial: 84 Epoch: 94, train_loss: 17233.956787109375, validation_loss = 2992.883056640625
 Trial: 84 Epoch: 95, train_loss: 17080.761779785156, validation_loss = 2701.2933349609375
 Trial: 84 Epoch: 96, train_loss: 18393.369873046875, validation_loss = 2683.795166015625
 Trial: 84 Epoch: 97, train_loss: 15513.6630859375, validation_loss = 2383.54931640625
 Trial: 84 Epoch: 98, train_loss: 15330.539916992188, validation_loss = 2408.2646484375
 Trial: 84 Epoch: 99, train_loss: 15848.36767578125, validation_loss = 2797.9776611328125
========Trial 85 params: {'n_layers': 1, 'K': 2, 'Dropout': 0.1, 'Hidden0': 256, 'optimizer': 'Adam', 'lr': 0.006605564261245883, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |   12288    |
|    sptH.0.gcn.bias     |    256     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |    3072    |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    6144    |
|    sptD.0.gcn.bias     |    256     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |    3072    |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |   12288    |
|    sptW.0.gcn.bias     |    256     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |    3072    |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 45285
 Trial: 85 Epoch: 0, train_loss: 115915.76318359375, validation_loss = 5068.9344482421875
 Trial: 85 Epoch: 1, train_loss: 38276.88195800781, validation_loss = 5514.3419189453125
 Trial: 85 Epoch: 2, train_loss: 39332.8515625, validation_loss = 5495.0262451171875
 Trial: 85 Epoch: 3, train_loss: 38223.46142578125, validation_loss = 4142.89453125
 Trial: 85 Epoch: 4, train_loss: 35574.96740722656, validation_loss = 3637.312255859375
 Trial: 85 Epoch: 5, train_loss: 28181.568115234375, validation_loss = 6020.610595703125
 Trial: 85 Epoch: 6, train_loss: 25210.130859375, validation_loss = 4015.23876953125
 Trial: 85 Epoch: 7, train_loss: 25546.927490234375, validation_loss = 4057.7271728515625
 Trial: 85 Epoch: 8, train_loss: 24444.281616210938, validation_loss = 3551.150390625
 Trial: 85 Epoch: 9, train_loss: 23445.300170898438, validation_loss = 3249.1119384765625
 Trial: 85 Epoch: 10, train_loss: 23741.098999023438, validation_loss = 4706.262939453125
 Trial: 85 Epoch: 11, train_loss: 24862.57635498047, validation_loss = 4306.84326171875
 Trial: 85 Epoch: 12, train_loss: 27178.216186523438, validation_loss = 3238.89111328125
 Trial: 85 Epoch: 13, train_loss: 24917.650024414062, validation_loss = 4443.8038330078125
 Trial: 85 Epoch: 14, train_loss: 27094.369506835938, validation_loss = 4764.6114501953125
 Trial: 85 Epoch: 15, train_loss: 24304.10919189453, validation_loss = 4447.7928466796875
 Trial: 85 Epoch: 16, train_loss: 23180.471313476562, validation_loss = 3554.71728515625
 Trial: 85 Epoch: 17, train_loss: 20042.89892578125, validation_loss = 3415.8062744140625
 Trial: 85 Epoch: 18, train_loss: 22616.961669921875, validation_loss = 2972.322998046875
 Trial: 85 Epoch: 19, train_loss: 20533.457275390625, validation_loss = 2782.80615234375
 Trial: 85 Epoch: 20, train_loss: 19962.78369140625, validation_loss = 2890.2060546875
 Trial: 85 Epoch: 21, train_loss: 20134.143981933594, validation_loss = 3801.056640625
 Trial: 85 Epoch: 22, train_loss: 21888.44512939453, validation_loss = 3114.03759765625
 Trial: 85 Epoch: 23, train_loss: 20925.1181640625, validation_loss = 3417.78076171875
 Trial: 85 Epoch: 24, train_loss: 21645.970092773438, validation_loss = 3016.0474853515625
 Trial: 85 Epoch: 25, train_loss: 19772.8583984375, validation_loss = 2964.3494873046875
 Trial: 85 Epoch: 26, train_loss: 20667.434936523438, validation_loss = 2875.3372802734375
 Trial: 85 Epoch: 27, train_loss: 19643.18341064453, validation_loss = 2785.0728759765625
 Trial: 85 Epoch: 28, train_loss: 20560.87109375, validation_loss = 2854.00146484375
 Trial: 85 Epoch: 29, train_loss: 22264.07257080078, validation_loss = 2894.19140625
 Trial: 85 Epoch: 30, train_loss: 21883.98602294922, validation_loss = 3668.544189453125
 Trial: 85 Epoch: 31, train_loss: 24044.036499023438, validation_loss = 5820.064697265625
 Trial: 85 Epoch: 32, train_loss: 26209.77276611328, validation_loss = 2964.2431640625
 Trial: 85 Epoch: 33, train_loss: 21246.212463378906, validation_loss = 2852.2666015625
 Trial: 85 Epoch: 34, train_loss: 20783.9775390625, validation_loss = 2846.6435546875
 Trial: 85 Epoch: 35, train_loss: 19364.283752441406, validation_loss = 3050.8818359375
 Trial: 85 Epoch: 36, train_loss: 21001.139526367188, validation_loss = 2893.9556884765625
 Trial: 85 Epoch: 37, train_loss: 20321.418579101562, validation_loss = 3090.1131591796875
 Trial: 85 Epoch: 38, train_loss: 18806.871459960938, validation_loss = 2771.6707763671875
 Trial: 85 Epoch: 39, train_loss: 18601.56121826172, validation_loss = 3088.22802734375
 Trial: 85 Epoch: 40, train_loss: 21342.206787109375, validation_loss = 2937.2235107421875
 Trial: 85 Epoch: 41, train_loss: 18790.909118652344, validation_loss = 2740.8248291015625
 Trial: 85 Epoch: 42, train_loss: 19081.329650878906, validation_loss = 2749.6163330078125
 Trial: 85 Epoch: 43, train_loss: 18153.05059814453, validation_loss = 2776.31787109375
 Trial: 85 Epoch: 44, train_loss: 17984.540283203125, validation_loss = 3219.41796875
 Trial: 85 Epoch: 45, train_loss: 18685.27130126953, validation_loss = 4129.452392578125
 Trial: 85 Epoch: 46, train_loss: 19502.654724121094, validation_loss = 3232.6849365234375
 Trial: 85 Epoch: 47, train_loss: 17789.13946533203, validation_loss = 2598.6885986328125
 Trial: 85 Epoch: 48, train_loss: 17937.09698486328, validation_loss = 2683.1090087890625
 Trial: 85 Epoch: 49, train_loss: 16936.660522460938, validation_loss = 3497.613037109375
 Trial: 85 Epoch: 50, train_loss: 18704.856994628906, validation_loss = 2804.252197265625
 Trial: 85 Epoch: 51, train_loss: 17725.177490234375, validation_loss = 3111.738037109375
 Trial: 85 Epoch: 52, train_loss: 17897.035766601562, validation_loss = 3027.9873046875
 Trial: 85 Epoch: 53, train_loss: 19529.572387695312, validation_loss = 2525.405517578125
 Trial: 85 Epoch: 54, train_loss: 18571.95361328125, validation_loss = 2988.294677734375
 Trial: 85 Epoch: 55, train_loss: 17015.97772216797, validation_loss = 2568.7230224609375
 Trial: 85 Epoch: 56, train_loss: 17743.94219970703, validation_loss = 2809.6075439453125
 Trial: 85 Epoch: 57, train_loss: 17595.01348876953, validation_loss = 2740.0234375
 Trial: 85 Epoch: 58, train_loss: 17728.371520996094, validation_loss = 2652.242919921875
 Trial: 85 Epoch: 59, train_loss: 16606.216064453125, validation_loss = 2594.7008056640625
 Trial: 85 Epoch: 60, train_loss: 17653.6240234375, validation_loss = 2586.7396240234375
 Trial: 85 Epoch: 61, train_loss: 17689.687561035156, validation_loss = 2606.6998291015625
 Trial: 85 Epoch: 62, train_loss: 17803.105895996094, validation_loss = 3342.3270263671875
 Trial: 85 Epoch: 63, train_loss: 17827.225830078125, validation_loss = 2936.3612060546875
 Trial: 85 Epoch: 64, train_loss: 21588.763732910156, validation_loss = 3234.8653564453125
 Trial: 85 Epoch: 65, train_loss: 19930.239013671875, validation_loss = 2853.022216796875
 Trial: 85 Epoch: 66, train_loss: 17461.968017578125, validation_loss = 2860.8935546875
 Trial: 85 Epoch: 67, train_loss: 17216.43328857422, validation_loss = 2512.8306884765625
 Trial: 85 Epoch: 68, train_loss: 16184.823425292969, validation_loss = 2956.2005615234375
 Trial: 85 Epoch: 69, train_loss: 15841.270202636719, validation_loss = 2683.1795654296875
 Trial: 85 Epoch: 70, train_loss: 17459.985778808594, validation_loss = 3313.40625
 Trial: 85 Epoch: 71, train_loss: 16201.484619140625, validation_loss = 2611.295654296875
 Trial: 85 Epoch: 72, train_loss: 16070.963256835938, validation_loss = 2662.1234130859375
 Trial: 85 Epoch: 73, train_loss: 17086.141723632812, validation_loss = 2831.123046875
 Trial: 85 Epoch: 74, train_loss: 16207.034790039062, validation_loss = 2781.6402587890625
 Trial: 85 Epoch: 75, train_loss: 16788.532165527344, validation_loss = 2894.8095703125
 Trial: 85 Epoch: 76, train_loss: 16286.744445800781, validation_loss = 2909.1070556640625
 Trial: 85 Epoch: 77, train_loss: 16628.413024902344, validation_loss = 2784.296630859375
 Trial: 85 Epoch: 78, train_loss: 15330.300109863281, validation_loss = 2607.3321533203125
 Trial: 85 Epoch: 79, train_loss: 17699.638793945312, validation_loss = 3435.1414794921875
 Trial: 85 Epoch: 80, train_loss: 16315.566345214844, validation_loss = 2823.2120361328125
 Trial: 85 Epoch: 81, train_loss: 19053.574645996094, validation_loss = 2824.7117919921875
 Trial: 85 Epoch: 82, train_loss: 17123.17755126953, validation_loss = 4376.868408203125
 Trial: 85 Epoch: 83, train_loss: 16327.576965332031, validation_loss = 2563.0501708984375
 Trial: 85 Epoch: 84, train_loss: 15825.270874023438, validation_loss = 2757.160888671875
 Trial: 85 Epoch: 85, train_loss: 15671.985107421875, validation_loss = 2541.294189453125
 Trial: 85 Epoch: 86, train_loss: 15708.240600585938, validation_loss = 2573.8048095703125
 Trial: 85 Epoch: 87, train_loss: 16199.129211425781, validation_loss = 2797.7725830078125
 Trial: 85 Epoch: 88, train_loss: 15520.182312011719, validation_loss = 2713.52734375
 Trial: 85 Epoch: 89, train_loss: 15899.360412597656, validation_loss = 2704.89111328125
 Trial: 85 Epoch: 90, train_loss: 14952.705322265625, validation_loss = 2995.472412109375
 Trial: 85 Epoch: 91, train_loss: 15798.902770996094, validation_loss = 2690.0850830078125
 Trial: 85 Epoch: 92, train_loss: 15868.893005371094, validation_loss = 2532.25244140625
 Trial: 85 Epoch: 93, train_loss: 16422.245544433594, validation_loss = 2684.9635009765625
 Trial: 85 Epoch: 94, train_loss: 15904.224609375, validation_loss = 2689.3892822265625
 Trial: 85 Epoch: 95, train_loss: 16765.092407226562, validation_loss = 2635.9947509765625
 Trial: 85 Epoch: 96, train_loss: 15271.785095214844, validation_loss = 2571.6558837890625
 Trial: 85 Epoch: 97, train_loss: 15194.321472167969, validation_loss = 2483.29638671875
 Trial: 85 Epoch: 98, train_loss: 14611.899108886719, validation_loss = 2523.5604248046875
 Trial: 85 Epoch: 99, train_loss: 15290.373107910156, validation_loss = 2697.0523681640625
========Trial 86 params: {'n_layers': 1, 'K': 2, 'Dropout': 0.1, 'Hidden0': 8, 'optimizer': 'Adam', 'lr': 0.010671603392666889, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |    384     |
|    sptH.0.gcn.bias     |     8      |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |     96     |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    192     |
|    sptD.0.gcn.bias     |     8      |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |     96     |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |    384     |
|    sptW.0.gcn.bias     |     8      |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |     96     |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 5853
 Trial: 86 Epoch: 0, train_loss: 213143.85400390625, validation_loss = 17945.83251953125
========Trial 87 params: {'n_layers': 1, 'K': 2, 'Dropout': 0.1, 'Hidden0': 64, 'optimizer': 'Adam', 'lr': 0.00767572609959147, 'batch': 32}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |    3072    |
|    sptH.0.gcn.bias     |     64     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |    768     |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    1536    |
|    sptD.0.gcn.bias     |     64     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |    768     |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |    3072    |
|    sptW.0.gcn.bias     |     64     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |    768     |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 14757
 Trial: 87 Epoch: 0, train_loss: 110403.64453125, validation_loss = 25425.33984375
========Trial 88 params: {'n_layers': 1, 'K': 3, 'Dropout': 0.1, 'Hidden0': 4, 'optimizer': 'Adam', 'lr': 0.006994576273881687, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |    288     |
|    sptH.0.gcn.bias     |     4      |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |     48     |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    144     |
|    sptD.0.gcn.bias     |     4      |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |     48     |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |    288     |
|    sptW.0.gcn.bias     |     4      |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |     48     |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 5457
 Trial: 88 Epoch: 0, train_loss: 252504.888671875, validation_loss = 21499.376953125
========Trial 89 params: {'n_layers': 1, 'K': 2, 'Dropout': 0.1, 'Hidden0': 256, 'optimizer': 'Adam', 'lr': 0.00575208141281214, 'batch': 64}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |   12288    |
|    sptH.0.gcn.bias     |    256     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |    3072    |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    6144    |
|    sptD.0.gcn.bias     |    256     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |    3072    |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |   12288    |
|    sptW.0.gcn.bias     |    256     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |    3072    |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 45285
 Trial: 89 Epoch: 0, train_loss: 68417.609375, validation_loss = 30344.318359375
========Trial 90 params: {'n_layers': 1, 'K': 2, 'Dropout': 0.1, 'Hidden0': 256, 'optimizer': 'Adam', 'lr': 0.01183642652173906, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |   12288    |
|    sptH.0.gcn.bias     |    256     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |    3072    |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    6144    |
|    sptD.0.gcn.bias     |    256     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |    3072    |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |   12288    |
|    sptW.0.gcn.bias     |    256     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |    3072    |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 45285
 Trial: 90 Epoch: 0, train_loss: 121494.85693359375, validation_loss = 5349.1824951171875
 Trial: 90 Epoch: 1, train_loss: 39104.44775390625, validation_loss = 5206.549560546875
 Trial: 90 Epoch: 2, train_loss: 34650.053466796875, validation_loss = 4390.363037109375
 Trial: 90 Epoch: 3, train_loss: 39974.51306152344, validation_loss = 5288.0556640625
 Trial: 90 Epoch: 4, train_loss: 60143.791015625, validation_loss = 3880.9915771484375
 Trial: 90 Epoch: 5, train_loss: 40944.62219238281, validation_loss = 5233.8427734375
 Trial: 90 Epoch: 6, train_loss: 28683.0439453125, validation_loss = 4098.9617919921875
 Trial: 90 Epoch: 7, train_loss: 28487.37744140625, validation_loss = 3630.203369140625
 Trial: 90 Epoch: 8, train_loss: 25578.635986328125, validation_loss = 3750.8643798828125
 Trial: 90 Epoch: 9, train_loss: 24093.364501953125, validation_loss = 3363.021484375
 Trial: 90 Epoch: 10, train_loss: 24112.175170898438, validation_loss = 4329.079833984375
 Trial: 90 Epoch: 11, train_loss: 25064.73602294922, validation_loss = 4229.451171875
 Trial: 90 Epoch: 12, train_loss: 27180.826171875, validation_loss = 3159.542236328125
 Trial: 90 Epoch: 13, train_loss: 25490.636962890625, validation_loss = 4783.1556396484375
 Trial: 90 Epoch: 14, train_loss: 28036.068725585938, validation_loss = 5106.6546630859375
 Trial: 90 Epoch: 15, train_loss: 25899.77081298828, validation_loss = 4808.7174072265625
 Trial: 90 Epoch: 16, train_loss: 24741.973876953125, validation_loss = 3399.2015380859375
 Trial: 90 Epoch: 17, train_loss: 20727.09002685547, validation_loss = 3510.5650634765625
 Trial: 90 Epoch: 18, train_loss: 22894.60595703125, validation_loss = 3001.0650634765625
 Trial: 90 Epoch: 19, train_loss: 20966.526611328125, validation_loss = 2846.20654296875
 Trial: 90 Epoch: 20, train_loss: 20808.0576171875, validation_loss = 3011.3834228515625
 Trial: 90 Epoch: 21, train_loss: 21144.550659179688, validation_loss = 4143.282470703125
 Trial: 90 Epoch: 22, train_loss: 23175.675659179688, validation_loss = 3314.6588134765625
 Trial: 90 Epoch: 23, train_loss: 22532.297119140625, validation_loss = 3719.964599609375
 Trial: 90 Epoch: 24, train_loss: 23148.221069335938, validation_loss = 3390.6414794921875
 Trial: 90 Epoch: 25, train_loss: 21412.204711914062, validation_loss = 3102.8194580078125
 Trial: 90 Epoch: 26, train_loss: 22249.495239257812, validation_loss = 2948.430419921875
 Trial: 90 Epoch: 27, train_loss: 20635.293334960938, validation_loss = 2722.1214599609375
 Trial: 90 Epoch: 28, train_loss: 21599.205078125, validation_loss = 2875.3016357421875
 Trial: 90 Epoch: 29, train_loss: 23560.75830078125, validation_loss = 2809.21875
 Trial: 90 Epoch: 30, train_loss: 21739.277954101562, validation_loss = 3970.9488525390625
 Trial: 90 Epoch: 31, train_loss: 22679.184936523438, validation_loss = 4391.0548095703125
 Trial: 90 Epoch: 32, train_loss: 24796.991638183594, validation_loss = 3103.4481201171875
 Trial: 90 Epoch: 33, train_loss: 20550.329467773438, validation_loss = 3132.885009765625
 Trial: 90 Epoch: 34, train_loss: 20845.834838867188, validation_loss = 3008.380126953125
 Trial: 90 Epoch: 35, train_loss: 20014.08380126953, validation_loss = 3240.8560791015625
 Trial: 90 Epoch: 36, train_loss: 22604.781494140625, validation_loss = 3203.36328125
 Trial: 90 Epoch: 37, train_loss: 21422.964111328125, validation_loss = 3283.5047607421875
 Trial: 90 Epoch: 38, train_loss: 21125.4072265625, validation_loss = 2969.176513671875
 Trial: 90 Epoch: 39, train_loss: 18679.63690185547, validation_loss = 2771.5596923828125
 Trial: 90 Epoch: 40, train_loss: 19498.56427001953, validation_loss = 2537.3431396484375
 Trial: 90 Epoch: 41, train_loss: 18405.198669433594, validation_loss = 2711.4736328125
 Trial: 90 Epoch: 42, train_loss: 18052.928771972656, validation_loss = 2553.5963134765625
 Trial: 90 Epoch: 43, train_loss: 19550.7470703125, validation_loss = 3067.79931640625
 Trial: 90 Epoch: 44, train_loss: 19016.80584716797, validation_loss = 2899.941162109375
 Trial: 90 Epoch: 45, train_loss: 21435.548095703125, validation_loss = 3367.602294921875
 Trial: 90 Epoch: 46, train_loss: 19504.122680664062, validation_loss = 3350.91845703125
 Trial: 90 Epoch: 47, train_loss: 19275.981811523438, validation_loss = 2490.275390625
 Trial: 90 Epoch: 48, train_loss: 19139.39697265625, validation_loss = 2627.5618896484375
 Trial: 90 Epoch: 49, train_loss: 17791.450561523438, validation_loss = 3231.445556640625
 Trial: 90 Epoch: 50, train_loss: 19118.108642578125, validation_loss = 2775.3468017578125
 Trial: 90 Epoch: 51, train_loss: 18423.406372070312, validation_loss = 3219.207763671875
 Trial: 90 Epoch: 52, train_loss: 18684.803161621094, validation_loss = 3000.4085693359375
 Trial: 90 Epoch: 53, train_loss: 20993.779296875, validation_loss = 2583.4517822265625
 Trial: 90 Epoch: 54, train_loss: 20820.711669921875, validation_loss = 2675.1640625
 Trial: 90 Epoch: 55, train_loss: 17755.575561523438, validation_loss = 2497.03515625
 Trial: 90 Epoch: 56, train_loss: 18074.330322265625, validation_loss = 2552.387939453125
 Trial: 90 Epoch: 57, train_loss: 19860.881591796875, validation_loss = 2739.5455322265625
 Trial: 90 Epoch: 58, train_loss: 19781.472412109375, validation_loss = 2689.78173828125
 Trial: 90 Epoch: 59, train_loss: 18280.816162109375, validation_loss = 2823.69580078125
 Trial: 90 Epoch: 60, train_loss: 18203.224365234375, validation_loss = 2638.513916015625
 Trial: 90 Epoch: 61, train_loss: 20274.537475585938, validation_loss = 2940.025146484375
 Trial: 90 Epoch: 62, train_loss: 18764.550659179688, validation_loss = 2866.2933349609375
 Trial: 90 Epoch: 63, train_loss: 17794.267333984375, validation_loss = 2778.45361328125
 Trial: 90 Epoch: 64, train_loss: 18490.97119140625, validation_loss = 3289.472412109375
 Trial: 90 Epoch: 65, train_loss: 18182.655395507812, validation_loss = 2606.1612548828125
 Trial: 90 Epoch: 66, train_loss: 17240.55712890625, validation_loss = 2722.0489501953125
 Trial: 90 Epoch: 67, train_loss: 17821.237854003906, validation_loss = 2422.18603515625
 Trial: 90 Epoch: 68, train_loss: 17764.276916503906, validation_loss = 2558.07275390625
 Trial: 90 Epoch: 69, train_loss: 16484.0966796875, validation_loss = 2546.14794921875
 Trial: 90 Epoch: 70, train_loss: 18013.43603515625, validation_loss = 3115.855224609375
 Trial: 90 Epoch: 71, train_loss: 16678.590270996094, validation_loss = 2505.4132080078125
 Trial: 90 Epoch: 72, train_loss: 16872.974243164062, validation_loss = 2516.651611328125
 Trial: 90 Epoch: 73, train_loss: 18113.10614013672, validation_loss = 2790.4058837890625
 Trial: 90 Epoch: 74, train_loss: 16391.528259277344, validation_loss = 2618.881103515625
 Trial: 90 Epoch: 75, train_loss: 16895.296142578125, validation_loss = 3347.4893798828125
 Trial: 90 Epoch: 76, train_loss: 19392.706909179688, validation_loss = 3147.5045166015625
 Trial: 90 Epoch: 77, train_loss: 18689.273315429688, validation_loss = 2514.140625
 Trial: 90 Epoch: 78, train_loss: 15981.336242675781, validation_loss = 2508.5489501953125
 Trial: 90 Epoch: 79, train_loss: 19311.92889404297, validation_loss = 3616.99658203125
 Trial: 90 Epoch: 80, train_loss: 16922.247009277344, validation_loss = 2716.7845458984375
 Trial: 90 Epoch: 81, train_loss: 16941.663940429688, validation_loss = 3048.2318115234375
 Trial: 90 Epoch: 82, train_loss: 17169.78125, validation_loss = 3239.2181396484375
 Trial: 90 Epoch: 83, train_loss: 16709.8203125, validation_loss = 2720.822265625
 Trial: 90 Epoch: 84, train_loss: 16858.723754882812, validation_loss = 2589.9212646484375
 Trial: 90 Epoch: 85, train_loss: 16780.477661132812, validation_loss = 3006.2142333984375
 Trial: 90 Epoch: 86, train_loss: 17270.121643066406, validation_loss = 2540.1124267578125
 Trial: 90 Epoch: 87, train_loss: 18394.736389160156, validation_loss = 3315.22802734375
 Trial: 90 Epoch: 88, train_loss: 16614.80810546875, validation_loss = 2408.0601806640625
 Trial: 90 Epoch: 89, train_loss: 19010.781616210938, validation_loss = 2656.324951171875
 Trial: 90 Epoch: 90, train_loss: 17192.368408203125, validation_loss = 3226.648681640625
 Trial: 90 Epoch: 91, train_loss: 18717.868774414062, validation_loss = 2522.3485107421875
 Trial: 90 Epoch: 92, train_loss: 18079.477416992188, validation_loss = 2917.075927734375
 Trial: 90 Epoch: 93, train_loss: 16984.18133544922, validation_loss = 2429.7476806640625
 Trial: 90 Epoch: 94, train_loss: 17921.143127441406, validation_loss = 2857.03564453125
 Trial: 90 Epoch: 95, train_loss: 17908.23651123047, validation_loss = 2565.5777587890625
 Trial: 90 Epoch: 96, train_loss: 18414.383178710938, validation_loss = 2587.92236328125
 Trial: 90 Epoch: 97, train_loss: 16079.333374023438, validation_loss = 2484.9761962890625
 Trial: 90 Epoch: 98, train_loss: 15911.487670898438, validation_loss = 2397.788818359375
 Trial: 90 Epoch: 99, train_loss: 16577.776611328125, validation_loss = 2829.168212890625
========Trial 91 params: {'n_layers': 1, 'K': 2, 'Dropout': 0.1, 'Hidden0': 256, 'optimizer': 'Adam', 'lr': 0.009891566529842933, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |   12288    |
|    sptH.0.gcn.bias     |    256     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |    3072    |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    6144    |
|    sptD.0.gcn.bias     |    256     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |    3072    |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |   12288    |
|    sptW.0.gcn.bias     |    256     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |    3072    |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 45285
 Trial: 91 Epoch: 0, train_loss: 109587.5947265625, validation_loss = 5213.837158203125
 Trial: 91 Epoch: 1, train_loss: 36787.27673339844, validation_loss = 4808.3743896484375
 Trial: 91 Epoch: 2, train_loss: 35882.98205566406, validation_loss = 4298.247802734375
 Trial: 91 Epoch: 3, train_loss: 41203.35302734375, validation_loss = 5445.45947265625
 Trial: 91 Epoch: 4, train_loss: 55953.556640625, validation_loss = 3983.781005859375
 Trial: 91 Epoch: 5, train_loss: 35727.79943847656, validation_loss = 4887.0152587890625
 Trial: 91 Epoch: 6, train_loss: 26866.132446289062, validation_loss = 3970.771728515625
 Trial: 91 Epoch: 7, train_loss: 27871.72705078125, validation_loss = 3582.7388916015625
 Trial: 91 Epoch: 8, train_loss: 24827.869995117188, validation_loss = 3760.091064453125
 Trial: 91 Epoch: 9, train_loss: 23538.773803710938, validation_loss = 3253.1160888671875
 Trial: 91 Epoch: 10, train_loss: 23518.284545898438, validation_loss = 4304.3277587890625
 Trial: 91 Epoch: 11, train_loss: 24876.609741210938, validation_loss = 4212.73974609375
 Trial: 91 Epoch: 12, train_loss: 27000.804443359375, validation_loss = 3189.1044921875
 Trial: 91 Epoch: 13, train_loss: 25392.694702148438, validation_loss = 4476.6116943359375
 Trial: 91 Epoch: 14, train_loss: 27364.332397460938, validation_loss = 5456.7401123046875
 Trial: 91 Epoch: 15, train_loss: 26890.49609375, validation_loss = 5140.0867919921875
 Trial: 91 Epoch: 16, train_loss: 24967.441162109375, validation_loss = 3554.491455078125
 Trial: 91 Epoch: 17, train_loss: 20423.3515625, validation_loss = 3420.936767578125
 Trial: 91 Epoch: 18, train_loss: 22983.844482421875, validation_loss = 3014.352294921875
 Trial: 91 Epoch: 19, train_loss: 20844.100341796875, validation_loss = 2769.302490234375
 Trial: 91 Epoch: 20, train_loss: 20403.166259765625, validation_loss = 2960.3992919921875
 Trial: 91 Epoch: 21, train_loss: 20687.061279296875, validation_loss = 3925.5003662109375
 Trial: 91 Epoch: 22, train_loss: 22353.777893066406, validation_loss = 3091.5712890625
 Trial: 91 Epoch: 23, train_loss: 21458.47039794922, validation_loss = 3508.943603515625
 Trial: 91 Epoch: 24, train_loss: 22278.290771484375, validation_loss = 3146.760009765625
 Trial: 91 Epoch: 25, train_loss: 20607.57470703125, validation_loss = 3036.0782470703125
 Trial: 91 Epoch: 26, train_loss: 21586.58526611328, validation_loss = 2878.8551025390625
 Trial: 91 Epoch: 27, train_loss: 20265.261779785156, validation_loss = 2691.760009765625
 Trial: 91 Epoch: 28, train_loss: 21319.7880859375, validation_loss = 2816.004150390625
 Trial: 91 Epoch: 29, train_loss: 23316.619995117188, validation_loss = 2903.25146484375
 Trial: 91 Epoch: 30, train_loss: 21857.710998535156, validation_loss = 3894.622802734375
 Trial: 91 Epoch: 31, train_loss: 22687.40380859375, validation_loss = 4701.486328125
 Trial: 91 Epoch: 32, train_loss: 25479.349975585938, validation_loss = 3031.207275390625
 Trial: 91 Epoch: 33, train_loss: 20772.09228515625, validation_loss = 2991.7459716796875
 Trial: 91 Epoch: 34, train_loss: 20554.401733398438, validation_loss = 3016.4534912109375
 Trial: 91 Epoch: 35, train_loss: 19509.871948242188, validation_loss = 3000.102783203125
 Trial: 91 Epoch: 36, train_loss: 21843.78564453125, validation_loss = 2994.802734375
 Trial: 91 Epoch: 37, train_loss: 21303.881591796875, validation_loss = 3120.59326171875
 Trial: 91 Epoch: 38, train_loss: 20176.198791503906, validation_loss = 2826.4896240234375
 Trial: 91 Epoch: 39, train_loss: 18538.27618408203, validation_loss = 2835.3416748046875
 Trial: 91 Epoch: 40, train_loss: 20471.605834960938, validation_loss = 2605.4107666015625
 Trial: 91 Epoch: 41, train_loss: 18023.388061523438, validation_loss = 2965.376220703125
 Trial: 91 Epoch: 42, train_loss: 17918.895141601562, validation_loss = 2500.1383056640625
 Trial: 91 Epoch: 43, train_loss: 19271.73321533203, validation_loss = 3049.3148193359375
 Trial: 91 Epoch: 44, train_loss: 18968.036743164062, validation_loss = 2944.967529296875
 Trial: 91 Epoch: 45, train_loss: 19880.40753173828, validation_loss = 3617.9730224609375
 Trial: 91 Epoch: 46, train_loss: 19065.04345703125, validation_loss = 3195.6002197265625
 Trial: 91 Epoch: 47, train_loss: 18958.902221679688, validation_loss = 2647.9599609375
 Trial: 91 Epoch: 48, train_loss: 18665.284240722656, validation_loss = 2562.719970703125
 Trial: 91 Epoch: 49, train_loss: 17608.53582763672, validation_loss = 3323.195068359375
 Trial: 91 Epoch: 50, train_loss: 19127.595458984375, validation_loss = 2899.4757080078125
 Trial: 91 Epoch: 51, train_loss: 18362.456481933594, validation_loss = 3192.809814453125
 Trial: 91 Epoch: 52, train_loss: 18189.305725097656, validation_loss = 2908.020263671875
 Trial: 91 Epoch: 53, train_loss: 20594.058959960938, validation_loss = 2430.0128173828125
 Trial: 91 Epoch: 54, train_loss: 20429.87188720703, validation_loss = 2824.9554443359375
 Trial: 91 Epoch: 55, train_loss: 17461.70733642578, validation_loss = 2498.9515380859375
 Trial: 91 Epoch: 56, train_loss: 18311.697998046875, validation_loss = 2603.31884765625
 Trial: 91 Epoch: 57, train_loss: 19554.160034179688, validation_loss = 2644.2275390625
 Trial: 91 Epoch: 58, train_loss: 19211.194702148438, validation_loss = 2581.6895751953125
 Trial: 91 Epoch: 59, train_loss: 17419.549682617188, validation_loss = 2557.83251953125
 Trial: 91 Epoch: 60, train_loss: 18633.538024902344, validation_loss = 2518.50927734375
 Trial: 91 Epoch: 61, train_loss: 20294.534912109375, validation_loss = 2648.4833984375
 Trial: 91 Epoch: 62, train_loss: 18814.847045898438, validation_loss = 2913.8642578125
 Trial: 91 Epoch: 63, train_loss: 18061.133666992188, validation_loss = 2813.6558837890625
 Trial: 91 Epoch: 64, train_loss: 20091.48809814453, validation_loss = 3389.4111328125
 Trial: 91 Epoch: 65, train_loss: 19605.593017578125, validation_loss = 2684.0655517578125
 Trial: 91 Epoch: 66, train_loss: 18159.04180908203, validation_loss = 2729.4537353515625
 Trial: 91 Epoch: 67, train_loss: 16717.651306152344, validation_loss = 2510.5968017578125
 Trial: 91 Epoch: 68, train_loss: 16831.390625, validation_loss = 2568.7584228515625
 Trial: 91 Epoch: 69, train_loss: 16267.749877929688, validation_loss = 2551.5213623046875
 Trial: 91 Epoch: 70, train_loss: 17686.361389160156, validation_loss = 3172.7867431640625
 Trial: 91 Epoch: 71, train_loss: 16649.04571533203, validation_loss = 2491.853759765625
 Trial: 91 Epoch: 72, train_loss: 16372.623840332031, validation_loss = 2540.8642578125
 Trial: 91 Epoch: 73, train_loss: 17224.41619873047, validation_loss = 2722.076904296875
 Trial: 91 Epoch: 74, train_loss: 16468.15899658203, validation_loss = 2644.08447265625
 Trial: 91 Epoch: 75, train_loss: 17784.581970214844, validation_loss = 3528.0633544921875
 Trial: 91 Epoch: 76, train_loss: 18806.794372558594, validation_loss = 2711.9930419921875
 Trial: 91 Epoch: 77, train_loss: 16987.236450195312, validation_loss = 2648.7003173828125
 Trial: 91 Epoch: 78, train_loss: 16128.922180175781, validation_loss = 2484.099609375
 Trial: 91 Epoch: 79, train_loss: 19609.264404296875, validation_loss = 3820.5321044921875
 Trial: 91 Epoch: 80, train_loss: 16914.45379638672, validation_loss = 2863.8277587890625
 Trial: 91 Epoch: 81, train_loss: 18316.035034179688, validation_loss = 2877.19580078125
 Trial: 91 Epoch: 82, train_loss: 17417.443969726562, validation_loss = 3426.77734375
 Trial: 91 Epoch: 83, train_loss: 16744.276123046875, validation_loss = 2683.58544921875
 Trial: 91 Epoch: 84, train_loss: 17472.806091308594, validation_loss = 2560.4493408203125
 Trial: 91 Epoch: 85, train_loss: 16997.707458496094, validation_loss = 2674.7379150390625
 Trial: 91 Epoch: 86, train_loss: 18090.71453857422, validation_loss = 2566.064208984375
 Trial: 91 Epoch: 87, train_loss: 19507.748596191406, validation_loss = 3226.75927734375
 Trial: 91 Epoch: 88, train_loss: 17473.81903076172, validation_loss = 2464.7421875
 Trial: 91 Epoch: 89, train_loss: 17984.41766357422, validation_loss = 2581.8131103515625
 Trial: 91 Epoch: 90, train_loss: 15891.778503417969, validation_loss = 2796.152587890625
 Trial: 91 Epoch: 91, train_loss: 16731.964782714844, validation_loss = 2590.8250732421875
 Trial: 91 Epoch: 92, train_loss: 16433.81622314453, validation_loss = 2358.92236328125
 Trial: 91 Epoch: 93, train_loss: 16813.406311035156, validation_loss = 2485.6290283203125
 Trial: 91 Epoch: 94, train_loss: 17802.699462890625, validation_loss = 2740.7144775390625
 Trial: 91 Epoch: 95, train_loss: 18399.4033203125, validation_loss = 2494.906005859375
 Trial: 91 Epoch: 96, train_loss: 17003.206420898438, validation_loss = 2468.1728515625
 Trial: 91 Epoch: 97, train_loss: 15834.170776367188, validation_loss = 2372.1268310546875
 Trial: 91 Epoch: 98, train_loss: 15409.697082519531, validation_loss = 2419.4310302734375
 Trial: 91 Epoch: 99, train_loss: 16172.86376953125, validation_loss = 2717.353515625
========Trial 92 params: {'n_layers': 1, 'K': 2, 'Dropout': 0.1, 'Hidden0': 256, 'optimizer': 'Adam', 'lr': 0.00977665073919552, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |   12288    |
|    sptH.0.gcn.bias     |    256     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |    3072    |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    6144    |
|    sptD.0.gcn.bias     |    256     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |    3072    |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |   12288    |
|    sptW.0.gcn.bias     |    256     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |    3072    |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 45285
 Trial: 92 Epoch: 0, train_loss: 109785.99536132812, validation_loss = 5172.1031494140625
 Trial: 92 Epoch: 1, train_loss: 36687.794677734375, validation_loss = 4779.15234375
 Trial: 92 Epoch: 2, train_loss: 36251.091552734375, validation_loss = 4312.707275390625
 Trial: 92 Epoch: 3, train_loss: 41341.33190917969, validation_loss = 5465.553466796875
 Trial: 92 Epoch: 4, train_loss: 51817.30859375, validation_loss = 3645.9910888671875
 Trial: 92 Epoch: 5, train_loss: 33806.044921875, validation_loss = 5196.8521728515625
 Trial: 92 Epoch: 6, train_loss: 26087.520874023438, validation_loss = 4188.7896728515625
 Trial: 92 Epoch: 7, train_loss: 26193.4541015625, validation_loss = 3822.47021484375
 Trial: 92 Epoch: 8, train_loss: 24577.64697265625, validation_loss = 3721.101318359375
 Trial: 92 Epoch: 9, train_loss: 23486.31201171875, validation_loss = 3212.6986083984375
 Trial: 92 Epoch: 10, train_loss: 23552.038452148438, validation_loss = 4412.046875
 Trial: 92 Epoch: 11, train_loss: 25219.272338867188, validation_loss = 4435.443115234375
 Trial: 92 Epoch: 12, train_loss: 27390.7919921875, validation_loss = 3110.8914794921875
 Trial: 92 Epoch: 13, train_loss: 24832.997436523438, validation_loss = 4893.2681884765625
 Trial: 92 Epoch: 14, train_loss: 27220.56396484375, validation_loss = 4533.81640625
 Trial: 92 Epoch: 15, train_loss: 24120.153564453125, validation_loss = 4185.3280029296875
 Trial: 92 Epoch: 16, train_loss: 23227.779296875, validation_loss = 3232.95947265625
 Trial: 92 Epoch: 17, train_loss: 20200.761108398438, validation_loss = 3605.6983642578125
 Trial: 92 Epoch: 18, train_loss: 23894.175537109375, validation_loss = 2948.551025390625
 Trial: 92 Epoch: 19, train_loss: 21241.646362304688, validation_loss = 2742.8248291015625
 Trial: 92 Epoch: 20, train_loss: 20200.709106445312, validation_loss = 2897.4158935546875
 Trial: 92 Epoch: 21, train_loss: 20422.03546142578, validation_loss = 3778.2969970703125
 Trial: 92 Epoch: 22, train_loss: 22010.87744140625, validation_loss = 3085.750244140625
 Trial: 92 Epoch: 23, train_loss: 21137.663635253906, validation_loss = 3583.7764892578125
 Trial: 92 Epoch: 24, train_loss: 22797.696411132812, validation_loss = 3298.6431884765625
 Trial: 92 Epoch: 25, train_loss: 21227.150634765625, validation_loss = 3306.0433349609375
 Trial: 92 Epoch: 26, train_loss: 22138.951416015625, validation_loss = 2928.89697265625
 Trial: 92 Epoch: 27, train_loss: 20423.576904296875, validation_loss = 2705.9510498046875
 Trial: 92 Epoch: 28, train_loss: 21652.9306640625, validation_loss = 2857.9034423828125
 Trial: 92 Epoch: 29, train_loss: 23739.839599609375, validation_loss = 3064.68505859375
 Trial: 92 Epoch: 30, train_loss: 22124.30926513672, validation_loss = 3829.962158203125
 Trial: 92 Epoch: 31, train_loss: 23662.476684570312, validation_loss = 5264.447998046875
 Trial: 92 Epoch: 32, train_loss: 25890.931579589844, validation_loss = 2952.84765625
 Trial: 92 Epoch: 33, train_loss: 21494.039794921875, validation_loss = 2894.3209228515625
 Trial: 92 Epoch: 34, train_loss: 20408.080810546875, validation_loss = 2990.296630859375
 Trial: 92 Epoch: 35, train_loss: 19589.444091796875, validation_loss = 3071.8604736328125
 Trial: 92 Epoch: 36, train_loss: 21517.898071289062, validation_loss = 2919.6396484375
 Trial: 92 Epoch: 37, train_loss: 21281.763916015625, validation_loss = 2979.4119873046875
 Trial: 92 Epoch: 38, train_loss: 20249.56591796875, validation_loss = 2773.3743896484375
 Trial: 92 Epoch: 39, train_loss: 18153.043518066406, validation_loss = 2736.2559814453125
 Trial: 92 Epoch: 40, train_loss: 20003.326904296875, validation_loss = 2557.9178466796875
 Trial: 92 Epoch: 41, train_loss: 17992.804809570312, validation_loss = 2845.8460693359375
 Trial: 92 Epoch: 42, train_loss: 17901.415283203125, validation_loss = 2507.384765625
 Trial: 92 Epoch: 43, train_loss: 19396.92266845703, validation_loss = 3034.000732421875
 Trial: 92 Epoch: 44, train_loss: 18986.28387451172, validation_loss = 2911.0228271484375
 Trial: 92 Epoch: 45, train_loss: 19710.714721679688, validation_loss = 3746.2122802734375
 Trial: 92 Epoch: 46, train_loss: 19567.344116210938, validation_loss = 2992.1639404296875
 Trial: 92 Epoch: 47, train_loss: 18494.06671142578, validation_loss = 2634.554931640625
 Trial: 92 Epoch: 48, train_loss: 18475.413818359375, validation_loss = 2469.228515625
 Trial: 92 Epoch: 49, train_loss: 17449.147216796875, validation_loss = 3351.11669921875
 Trial: 92 Epoch: 50, train_loss: 19048.7431640625, validation_loss = 2763.9970703125
 Trial: 92 Epoch: 51, train_loss: 17913.143920898438, validation_loss = 2924.0086669921875
 Trial: 92 Epoch: 52, train_loss: 17883.146606445312, validation_loss = 2822.279541015625
 Trial: 92 Epoch: 53, train_loss: 20290.287963867188, validation_loss = 2465.0294189453125
 Trial: 92 Epoch: 54, train_loss: 20639.992797851562, validation_loss = 2720.84423828125
 Trial: 92 Epoch: 55, train_loss: 17966.389892578125, validation_loss = 2449.8978271484375
 Trial: 92 Epoch: 56, train_loss: 17969.13330078125, validation_loss = 2479.2479248046875
 Trial: 92 Epoch: 57, train_loss: 18983.69842529297, validation_loss = 2666.2916259765625
 Trial: 92 Epoch: 58, train_loss: 19519.750610351562, validation_loss = 2655.9210205078125
 Trial: 92 Epoch: 59, train_loss: 18507.51025390625, validation_loss = 2844.2615966796875
 Trial: 92 Epoch: 60, train_loss: 18362.885375976562, validation_loss = 2594.99169921875
 Trial: 92 Epoch: 61, train_loss: 20103.1044921875, validation_loss = 2702.788818359375
 Trial: 92 Epoch: 62, train_loss: 18923.739013671875, validation_loss = 2871.80322265625
 Trial: 92 Epoch: 63, train_loss: 17833.158569335938, validation_loss = 2752.89208984375
 Trial: 92 Epoch: 64, train_loss: 18526.3603515625, validation_loss = 3119.4779052734375
 Trial: 92 Epoch: 65, train_loss: 18360.973571777344, validation_loss = 2633.3470458984375
 Trial: 92 Epoch: 66, train_loss: 17564.410522460938, validation_loss = 2694.166748046875
 Trial: 92 Epoch: 67, train_loss: 16723.85430908203, validation_loss = 2432.2847900390625
 Trial: 92 Epoch: 68, train_loss: 16692.410888671875, validation_loss = 2488.4510498046875
 Trial: 92 Epoch: 69, train_loss: 16247.517150878906, validation_loss = 2532.657958984375
 Trial: 92 Epoch: 70, train_loss: 17879.11688232422, validation_loss = 3142.8367919921875
 Trial: 92 Epoch: 71, train_loss: 16645.28192138672, validation_loss = 2484.5411376953125
 Trial: 92 Epoch: 72, train_loss: 17692.1376953125, validation_loss = 2511.2969970703125
 Trial: 92 Epoch: 73, train_loss: 18136.673461914062, validation_loss = 2660.751708984375
 Trial: 92 Epoch: 74, train_loss: 16967.848876953125, validation_loss = 2573.78173828125
 Trial: 92 Epoch: 75, train_loss: 17705.339416503906, validation_loss = 3533.6761474609375
 Trial: 92 Epoch: 76, train_loss: 18815.127014160156, validation_loss = 2691.3363037109375
 Trial: 92 Epoch: 77, train_loss: 17136.988830566406, validation_loss = 2643.899169921875
 Trial: 92 Epoch: 78, train_loss: 16396.01153564453, validation_loss = 2636.2353515625
 Trial: 92 Epoch: 79, train_loss: 19944.531677246094, validation_loss = 3850.678955078125
 Trial: 92 Epoch: 80, train_loss: 16994.830200195312, validation_loss = 2801.256591796875
 Trial: 92 Epoch: 81, train_loss: 17572.678955078125, validation_loss = 2999.9185791015625
 Trial: 92 Epoch: 82, train_loss: 16747.94024658203, validation_loss = 3329.1229248046875
 Trial: 92 Epoch: 83, train_loss: 16531.279663085938, validation_loss = 2624.310302734375
 Trial: 92 Epoch: 84, train_loss: 17029.103942871094, validation_loss = 2488.686279296875
 Trial: 92 Epoch: 85, train_loss: 16356.985717773438, validation_loss = 2571.267578125
 Trial: 92 Epoch: 86, train_loss: 16899.35235595703, validation_loss = 2430.864501953125
 Trial: 92 Epoch: 87, train_loss: 18828.680603027344, validation_loss = 3348.8187255859375
 Trial: 92 Epoch: 88, train_loss: 16955.74530029297, validation_loss = 2414.0338134765625
 Trial: 92 Epoch: 89, train_loss: 17994.264892578125, validation_loss = 2823.6180419921875
 Trial: 92 Epoch: 90, train_loss: 17063.4541015625, validation_loss = 2788.1014404296875
 Trial: 92 Epoch: 91, train_loss: 17939.055725097656, validation_loss = 2501.5921630859375
 Trial: 92 Epoch: 92, train_loss: 17094.874572753906, validation_loss = 2396.43896484375
 Trial: 92 Epoch: 93, train_loss: 16825.90509033203, validation_loss = 2467.4051513671875
 Trial: 92 Epoch: 94, train_loss: 17629.989685058594, validation_loss = 2642.958251953125
 Trial: 92 Epoch: 95, train_loss: 17535.18524169922, validation_loss = 2588.1978759765625
 Trial: 92 Epoch: 96, train_loss: 16975.74151611328, validation_loss = 2484.4403076171875
 Trial: 92 Epoch: 97, train_loss: 15539.581420898438, validation_loss = 2426.8402099609375
 Trial: 92 Epoch: 98, train_loss: 15426.717651367188, validation_loss = 2428.8223876953125
 Trial: 92 Epoch: 99, train_loss: 15825.657287597656, validation_loss = 2727.6988525390625
========Trial 93 params: {'n_layers': 1, 'K': 2, 'Dropout': 0.1, 'Hidden0': 256, 'optimizer': 'Adam', 'lr': 0.010232530968924404, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |   12288    |
|    sptH.0.gcn.bias     |    256     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |    3072    |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    6144    |
|    sptD.0.gcn.bias     |    256     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |    3072    |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |   12288    |
|    sptW.0.gcn.bias     |    256     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |    3072    |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 45285
 Trial: 93 Epoch: 0, train_loss: 110232.44116210938, validation_loss = 5251.0533447265625
 Trial: 93 Epoch: 1, train_loss: 37309.9853515625, validation_loss = 4867.169677734375
 Trial: 93 Epoch: 2, train_loss: 36478.598876953125, validation_loss = 4388.8651123046875
 Trial: 93 Epoch: 3, train_loss: 41616.781982421875, validation_loss = 5372.11376953125
 Trial: 93 Epoch: 4, train_loss: 53814.11474609375, validation_loss = 3710.2513427734375
 Trial: 93 Epoch: 5, train_loss: 35608.44384765625, validation_loss = 5449.1739501953125
 Trial: 93 Epoch: 6, train_loss: 26415.1689453125, validation_loss = 4229.335205078125
 Trial: 93 Epoch: 7, train_loss: 26244.353393554688, validation_loss = 3793.0169677734375
 Trial: 93 Epoch: 8, train_loss: 24661.57568359375, validation_loss = 3764.1319580078125
 Trial: 93 Epoch: 9, train_loss: 23537.479248046875, validation_loss = 3246.6187744140625
 Trial: 93 Epoch: 10, train_loss: 23688.2822265625, validation_loss = 4162.5137939453125
 Trial: 93 Epoch: 11, train_loss: 24264.632385253906, validation_loss = 3949.27587890625
 Trial: 93 Epoch: 12, train_loss: 26243.832275390625, validation_loss = 3110.3280029296875
 Trial: 93 Epoch: 13, train_loss: 24322.296142578125, validation_loss = 5144.93798828125
 Trial: 93 Epoch: 14, train_loss: 26608.070922851562, validation_loss = 3788.574462890625
 Trial: 93 Epoch: 15, train_loss: 22336.898864746094, validation_loss = 3686.7716064453125
 Trial: 93 Epoch: 16, train_loss: 21690.954467773438, validation_loss = 3211.067138671875
 Trial: 93 Epoch: 17, train_loss: 20390.390686035156, validation_loss = 4041.159423828125
 Trial: 93 Epoch: 18, train_loss: 24186.901977539062, validation_loss = 2985.8878173828125
 Trial: 93 Epoch: 19, train_loss: 22402.468688964844, validation_loss = 2808.3985595703125
 Trial: 93 Epoch: 20, train_loss: 20510.793212890625, validation_loss = 2930.33935546875
 Trial: 93 Epoch: 21, train_loss: 20275.280700683594, validation_loss = 3759.939697265625
 Trial: 93 Epoch: 22, train_loss: 21953.049255371094, validation_loss = 3193.57373046875
 Trial: 93 Epoch: 23, train_loss: 21096.294921875, validation_loss = 3836.5963134765625
 Trial: 93 Epoch: 24, train_loss: 23440.752197265625, validation_loss = 3379.0733642578125
 Trial: 93 Epoch: 25, train_loss: 21382.26123046875, validation_loss = 3600.0740966796875
 Trial: 93 Epoch: 26, train_loss: 21554.13641357422, validation_loss = 3292.968017578125
 Trial: 93 Epoch: 27, train_loss: 20477.578674316406, validation_loss = 2895.4404296875
 Trial: 93 Epoch: 28, train_loss: 21288.249633789062, validation_loss = 2682.9842529296875
 Trial: 93 Epoch: 29, train_loss: 23189.075805664062, validation_loss = 2770.17822265625
 Trial: 93 Epoch: 30, train_loss: 21733.745361328125, validation_loss = 4091.4080810546875
 Trial: 93 Epoch: 31, train_loss: 22429.130493164062, validation_loss = 4177.4732666015625
 Trial: 93 Epoch: 32, train_loss: 24659.19921875, validation_loss = 2953.4288330078125
 Trial: 93 Epoch: 33, train_loss: 19955.137145996094, validation_loss = 3328.1390380859375
 Trial: 93 Epoch: 34, train_loss: 21273.167358398438, validation_loss = 2777.1048583984375
 Trial: 93 Epoch: 35, train_loss: 19492.51678466797, validation_loss = 3239.71044921875
 Trial: 93 Epoch: 36, train_loss: 21974.561950683594, validation_loss = 3023.8067626953125
 Trial: 93 Epoch: 37, train_loss: 21022.4130859375, validation_loss = 3251.169189453125
 Trial: 93 Epoch: 38, train_loss: 20785.965942382812, validation_loss = 2835.28369140625
 Trial: 93 Epoch: 39, train_loss: 18259.37872314453, validation_loss = 2786.7879638671875
 Trial: 93 Epoch: 40, train_loss: 19237.225952148438, validation_loss = 2556.7891845703125
 Trial: 93 Epoch: 41, train_loss: 17997.893615722656, validation_loss = 2707.88720703125
 Trial: 93 Epoch: 42, train_loss: 18029.978271484375, validation_loss = 2487.5284423828125
 Trial: 93 Epoch: 43, train_loss: 19094.58770751953, validation_loss = 3016.3751220703125
 Trial: 93 Epoch: 44, train_loss: 18823.920471191406, validation_loss = 2911.40576171875
 Trial: 93 Epoch: 45, train_loss: 20305.68243408203, validation_loss = 3329.759521484375
 Trial: 93 Epoch: 46, train_loss: 19507.813049316406, validation_loss = 3341.153564453125
 Trial: 93 Epoch: 47, train_loss: 19016.136169433594, validation_loss = 2498.2064208984375
 Trial: 93 Epoch: 48, train_loss: 18984.307739257812, validation_loss = 2531.3448486328125
 Trial: 93 Epoch: 49, train_loss: 17692.083618164062, validation_loss = 3297.367919921875
 Trial: 93 Epoch: 50, train_loss: 19029.62335205078, validation_loss = 2803.802734375
 Trial: 93 Epoch: 51, train_loss: 18273.42889404297, validation_loss = 3185.66455078125
 Trial: 93 Epoch: 52, train_loss: 18345.886840820312, validation_loss = 2902.595703125
 Trial: 93 Epoch: 53, train_loss: 20805.390380859375, validation_loss = 2462.4290771484375
 Trial: 93 Epoch: 54, train_loss: 21224.792419433594, validation_loss = 2692.692626953125
 Trial: 93 Epoch: 55, train_loss: 17425.0126953125, validation_loss = 2488.6490478515625
 Trial: 93 Epoch: 56, train_loss: 17779.55517578125, validation_loss = 2490.7784423828125
 Trial: 93 Epoch: 57, train_loss: 19188.254272460938, validation_loss = 2778.0977783203125
 Trial: 93 Epoch: 58, train_loss: 19215.72882080078, validation_loss = 2615.70703125
 Trial: 93 Epoch: 59, train_loss: 18011.827392578125, validation_loss = 2695.3253173828125
 Trial: 93 Epoch: 60, train_loss: 17932.622985839844, validation_loss = 2607.975341796875
 Trial: 93 Epoch: 61, train_loss: 19578.196166992188, validation_loss = 3150.9892578125
 Trial: 93 Epoch: 62, train_loss: 17798.836486816406, validation_loss = 2746.357421875
 Trial: 93 Epoch: 63, train_loss: 17142.9912109375, validation_loss = 2711.8333740234375
 Trial: 93 Epoch: 64, train_loss: 17466.94415283203, validation_loss = 3009.209228515625
 Trial: 93 Epoch: 65, train_loss: 17502.259765625, validation_loss = 2580.1806640625
 Trial: 93 Epoch: 66, train_loss: 16607.211303710938, validation_loss = 2614.02734375
 Trial: 93 Epoch: 67, train_loss: 17529.268188476562, validation_loss = 2619.369140625
 Trial: 93 Epoch: 68, train_loss: 17542.27081298828, validation_loss = 2475.3060302734375
 Trial: 93 Epoch: 69, train_loss: 16479.590209960938, validation_loss = 2645.6473388671875
 Trial: 93 Epoch: 70, train_loss: 17786.474243164062, validation_loss = 2981.887939453125
 Trial: 93 Epoch: 71, train_loss: 16480.823608398438, validation_loss = 2471.558349609375
 Trial: 93 Epoch: 72, train_loss: 16366.503784179688, validation_loss = 2483.0738525390625
 Trial: 93 Epoch: 73, train_loss: 17475.600463867188, validation_loss = 2653.0955810546875
 Trial: 93 Epoch: 74, train_loss: 16073.763122558594, validation_loss = 2654.179443359375
 Trial: 93 Epoch: 75, train_loss: 16928.357666015625, validation_loss = 3278.634765625
 Trial: 93 Epoch: 76, train_loss: 19490.679260253906, validation_loss = 3142.6207275390625
 Trial: 93 Epoch: 77, train_loss: 18829.91845703125, validation_loss = 2397.6878662109375
 Trial: 93 Epoch: 78, train_loss: 15861.334533691406, validation_loss = 2417.32763671875
 Trial: 93 Epoch: 79, train_loss: 19105.177490234375, validation_loss = 3238.7977294921875
 Trial: 93 Epoch: 80, train_loss: 16072.744567871094, validation_loss = 2573.64013671875
 Trial: 93 Epoch: 81, train_loss: 16491.112182617188, validation_loss = 2706.81103515625
 Trial: 93 Epoch: 82, train_loss: 16565.06561279297, validation_loss = 3242.71484375
 Trial: 93 Epoch: 83, train_loss: 16269.401489257812, validation_loss = 2614.055908203125
 Trial: 93 Epoch: 84, train_loss: 16536.505126953125, validation_loss = 2480.5628662109375
 Trial: 93 Epoch: 85, train_loss: 16125.492492675781, validation_loss = 2672.2603759765625
 Trial: 93 Epoch: 86, train_loss: 16753.89825439453, validation_loss = 2774.3974609375
 Trial: 93 Epoch: 87, train_loss: 19465.3955078125, validation_loss = 3358.727783203125
 Trial: 93 Epoch: 88, train_loss: 16377.939758300781, validation_loss = 2428.892333984375
 Trial: 93 Epoch: 89, train_loss: 17804.3310546875, validation_loss = 2579.049072265625
 Trial: 93 Epoch: 90, train_loss: 17023.158142089844, validation_loss = 3480.267333984375
 Trial: 93 Epoch: 91, train_loss: 18659.950927734375, validation_loss = 2516.1365966796875
 Trial: 93 Epoch: 92, train_loss: 17751.580017089844, validation_loss = 2694.5750732421875
 Trial: 93 Epoch: 93, train_loss: 17005.398559570312, validation_loss = 2409.5504150390625
 Trial: 93 Epoch: 94, train_loss: 17802.178161621094, validation_loss = 2876.799560546875
 Trial: 93 Epoch: 95, train_loss: 17801.5380859375, validation_loss = 2709.912353515625
 Trial: 93 Epoch: 96, train_loss: 18283.68292236328, validation_loss = 2567.2469482421875
 Trial: 93 Epoch: 97, train_loss: 15834.828063964844, validation_loss = 2351.2164306640625
 Trial: 93 Epoch: 98, train_loss: 15370.002807617188, validation_loss = 2373.0355224609375
 Trial: 93 Epoch: 99, train_loss: 16081.041809082031, validation_loss = 2689.4771728515625
========Trial 94 params: {'n_layers': 1, 'K': 2, 'Dropout': 0.1, 'Hidden0': 256, 'optimizer': 'Adam', 'lr': 0.010123863614246978, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |   12288    |
|    sptH.0.gcn.bias     |    256     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |    3072    |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    6144    |
|    sptD.0.gcn.bias     |    256     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |    3072    |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |   12288    |
|    sptW.0.gcn.bias     |    256     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |    3072    |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 45285
 Trial: 94 Epoch: 0, train_loss: 109565.46264648438, validation_loss = 5244.7657470703125
 Trial: 94 Epoch: 1, train_loss: 37357.614990234375, validation_loss = 4868.5693359375
 Trial: 94 Epoch: 2, train_loss: 36416.592041015625, validation_loss = 4362.2784423828125
 Trial: 94 Epoch: 3, train_loss: 41387.92224121094, validation_loss = 5289.87841796875
 Trial: 94 Epoch: 4, train_loss: 54801.91564941406, validation_loss = 3748.4541015625
 Trial: 94 Epoch: 5, train_loss: 36599.646240234375, validation_loss = 5348.9886474609375
 Trial: 94 Epoch: 6, train_loss: 27006.989990234375, validation_loss = 4222.205810546875
 Trial: 94 Epoch: 7, train_loss: 27214.070434570312, validation_loss = 3676.821044921875
 Trial: 94 Epoch: 8, train_loss: 24822.801391601562, validation_loss = 3808.7381591796875
 Trial: 94 Epoch: 9, train_loss: 23571.2001953125, validation_loss = 3273.0516357421875
 Trial: 94 Epoch: 10, train_loss: 23547.150756835938, validation_loss = 4222.7034912109375
 Trial: 94 Epoch: 11, train_loss: 24609.844787597656, validation_loss = 4242.347900390625
 Trial: 94 Epoch: 12, train_loss: 26568.909912109375, validation_loss = 3187.93212890625
 Trial: 94 Epoch: 13, train_loss: 24446.17333984375, validation_loss = 5086.412353515625
 Trial: 94 Epoch: 14, train_loss: 26614.262451171875, validation_loss = 4080.49951171875
 Trial: 94 Epoch: 15, train_loss: 22836.426208496094, validation_loss = 3901.1142578125
 Trial: 94 Epoch: 16, train_loss: 22342.41064453125, validation_loss = 3245.246826171875
 Trial: 94 Epoch: 17, train_loss: 20334.425903320312, validation_loss = 3721.399169921875
 Trial: 94 Epoch: 18, train_loss: 23914.47119140625, validation_loss = 2931.2099609375
 Trial: 94 Epoch: 19, train_loss: 21879.445190429688, validation_loss = 2796.92626953125
 Trial: 94 Epoch: 20, train_loss: 20347.784240722656, validation_loss = 2924.0906982421875
 Trial: 94 Epoch: 21, train_loss: 20318.77783203125, validation_loss = 3865.2745361328125
 Trial: 94 Epoch: 22, train_loss: 22031.814697265625, validation_loss = 3309.027099609375
 Trial: 94 Epoch: 23, train_loss: 21359.63653564453, validation_loss = 3970.3636474609375
 Trial: 94 Epoch: 24, train_loss: 23882.0693359375, validation_loss = 3372.2449951171875
 Trial: 94 Epoch: 25, train_loss: 21728.526245117188, validation_loss = 3590.2919921875
 Trial: 94 Epoch: 26, train_loss: 22196.67156982422, validation_loss = 3102.9163818359375
 Trial: 94 Epoch: 27, train_loss: 20626.19677734375, validation_loss = 2764.7142333984375
 Trial: 94 Epoch: 28, train_loss: 21607.415893554688, validation_loss = 2753.8349609375
 Trial: 94 Epoch: 29, train_loss: 23595.947265625, validation_loss = 2898.0032958984375
 Trial: 94 Epoch: 30, train_loss: 22059.297424316406, validation_loss = 4084.5411376953125
 Trial: 94 Epoch: 31, train_loss: 23308.828857421875, validation_loss = 4835.655517578125
 Trial: 94 Epoch: 32, train_loss: 25808.84130859375, validation_loss = 3114.7869873046875
 Trial: 94 Epoch: 33, train_loss: 20886.99169921875, validation_loss = 3026.1529541015625
 Trial: 94 Epoch: 34, train_loss: 20349.6748046875, validation_loss = 2995.8017578125
 Trial: 94 Epoch: 35, train_loss: 19576.952392578125, validation_loss = 3033.1614990234375
 Trial: 94 Epoch: 36, train_loss: 21556.033630371094, validation_loss = 2991.9326171875
 Trial: 94 Epoch: 37, train_loss: 21319.397705078125, validation_loss = 3194.715087890625
 Trial: 94 Epoch: 38, train_loss: 20780.371704101562, validation_loss = 2909.7523193359375
 Trial: 94 Epoch: 39, train_loss: 18498.435791015625, validation_loss = 2713.2803955078125
 Trial: 94 Epoch: 40, train_loss: 19543.11407470703, validation_loss = 2562.4293212890625
 Trial: 94 Epoch: 41, train_loss: 18066.93780517578, validation_loss = 2736.5035400390625
 Trial: 94 Epoch: 42, train_loss: 18105.82293701172, validation_loss = 2495.7728271484375
 Trial: 94 Epoch: 43, train_loss: 21401.374633789062, validation_loss = 2628.9658203125
 Trial: 94 Epoch: 44, train_loss: 20208.14794921875, validation_loss = 3868.0206298828125
 Trial: 94 Epoch: 45, train_loss: 21874.874267578125, validation_loss = 3584.13134765625
 Trial: 94 Epoch: 46, train_loss: 20811.091247558594, validation_loss = 3208.1666259765625
 Trial: 94 Epoch: 47, train_loss: 19027.791625976562, validation_loss = 2631.3109130859375
 Trial: 94 Epoch: 48, train_loss: 18694.75311279297, validation_loss = 2564.9696044921875
 Trial: 94 Epoch: 49, train_loss: 17755.004333496094, validation_loss = 3476.755126953125
 Trial: 94 Epoch: 50, train_loss: 19900.047485351562, validation_loss = 2880.212646484375
 Trial: 94 Epoch: 51, train_loss: 18516.914611816406, validation_loss = 3126.786865234375
 Trial: 94 Epoch: 52, train_loss: 18369.309936523438, validation_loss = 2887.9937744140625
 Trial: 94 Epoch: 53, train_loss: 20875.110595703125, validation_loss = 2481.8660888671875
 Trial: 94 Epoch: 54, train_loss: 20772.963317871094, validation_loss = 2801.98876953125
 Trial: 94 Epoch: 55, train_loss: 17716.148803710938, validation_loss = 2458.1148681640625
 Trial: 94 Epoch: 56, train_loss: 18366.81561279297, validation_loss = 2576.7066650390625
 Trial: 94 Epoch: 57, train_loss: 19649.708129882812, validation_loss = 2670.626708984375
 Trial: 94 Epoch: 58, train_loss: 19577.04296875, validation_loss = 2650.7900390625
 Trial: 94 Epoch: 59, train_loss: 17844.408813476562, validation_loss = 2639.26513671875
 Trial: 94 Epoch: 60, train_loss: 18427.06866455078, validation_loss = 2554.1248779296875
 Trial: 94 Epoch: 61, train_loss: 19699.206176757812, validation_loss = 2660.902099609375
 Trial: 94 Epoch: 62, train_loss: 19146.860473632812, validation_loss = 2974.584228515625
 Trial: 94 Epoch: 63, train_loss: 18273.760375976562, validation_loss = 2763.4879150390625
 Trial: 94 Epoch: 64, train_loss: 19908.189025878906, validation_loss = 3376.6268310546875
 Trial: 94 Epoch: 65, train_loss: 19204.566955566406, validation_loss = 2659.3294677734375
 Trial: 94 Epoch: 66, train_loss: 17693.384826660156, validation_loss = 2699.7581787109375
 Trial: 94 Epoch: 67, train_loss: 17037.13055419922, validation_loss = 2506.952880859375
 Trial: 94 Epoch: 68, train_loss: 17199.13507080078, validation_loss = 2493.30517578125
 Trial: 94 Epoch: 69, train_loss: 16629.103759765625, validation_loss = 2640.400390625
 Trial: 94 Epoch: 70, train_loss: 17576.408325195312, validation_loss = 2979.315673828125
 Trial: 94 Epoch: 71, train_loss: 16851.499877929688, validation_loss = 2486.014404296875
 Trial: 94 Epoch: 72, train_loss: 16457.72833251953, validation_loss = 2440.268798828125
 Trial: 94 Epoch: 73, train_loss: 17341.66143798828, validation_loss = 2711.1815185546875
 Trial: 94 Epoch: 74, train_loss: 16516.538208007812, validation_loss = 2550.71044921875
 Trial: 94 Epoch: 75, train_loss: 17087.995239257812, validation_loss = 3279.304443359375
 Trial: 94 Epoch: 76, train_loss: 19564.76055908203, validation_loss = 2964.2010498046875
 Trial: 94 Epoch: 77, train_loss: 18235.7734375, validation_loss = 2473.90234375
 Trial: 94 Epoch: 78, train_loss: 16106.611145019531, validation_loss = 2482.0933837890625
 Trial: 94 Epoch: 79, train_loss: 18883.66571044922, validation_loss = 3504.9322509765625
 Trial: 94 Epoch: 80, train_loss: 16702.36151123047, validation_loss = 2757.5072021484375
 Trial: 94 Epoch: 81, train_loss: 17198.42431640625, validation_loss = 2887.751953125
 Trial: 94 Epoch: 82, train_loss: 16660.270080566406, validation_loss = 3268.0902099609375
 Trial: 94 Epoch: 83, train_loss: 16495.68048095703, validation_loss = 2652.354736328125
 Trial: 94 Epoch: 84, train_loss: 16953.434448242188, validation_loss = 2488.0252685546875
 Trial: 94 Epoch: 85, train_loss: 16515.208068847656, validation_loss = 2623.2318115234375
 Trial: 94 Epoch: 86, train_loss: 16899.236755371094, validation_loss = 2544.9874267578125
 Trial: 94 Epoch: 87, train_loss: 17496.805297851562, validation_loss = 3230.328125
 Trial: 94 Epoch: 88, train_loss: 16546.26690673828, validation_loss = 2418.2767333984375
 Trial: 94 Epoch: 89, train_loss: 18117.695739746094, validation_loss = 2597.2623291015625
 Trial: 94 Epoch: 90, train_loss: 17106.415100097656, validation_loss = 3240.2996826171875
 Trial: 94 Epoch: 91, train_loss: 18124.895874023438, validation_loss = 2582.2327880859375
 Trial: 94 Epoch: 92, train_loss: 17189.687133789062, validation_loss = 2602.6138916015625
 Trial: 94 Epoch: 93, train_loss: 16777.14727783203, validation_loss = 2477.9764404296875
 Trial: 94 Epoch: 94, train_loss: 17626.17529296875, validation_loss = 2949.923828125
 Trial: 94 Epoch: 95, train_loss: 17509.201049804688, validation_loss = 2655.3316650390625
 Trial: 94 Epoch: 96, train_loss: 18177.639709472656, validation_loss = 2600.820556640625
 Trial: 94 Epoch: 97, train_loss: 15648.385681152344, validation_loss = 2412.4224853515625
 Trial: 94 Epoch: 98, train_loss: 15555.513000488281, validation_loss = 2356.06201171875
 Trial: 94 Epoch: 99, train_loss: 16132.927856445312, validation_loss = 2880.48291015625
========Trial 95 params: {'n_layers': 1, 'K': 2, 'Dropout': 0.1, 'Hidden0': 256, 'optimizer': 'Adam', 'lr': 0.010756981447051851, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |   12288    |
|    sptH.0.gcn.bias     |    256     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |    3072    |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    6144    |
|    sptD.0.gcn.bias     |    256     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |    3072    |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |   12288    |
|    sptW.0.gcn.bias     |    256     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |    3072    |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 45285
 Trial: 95 Epoch: 0, train_loss: 112161.90991210938, validation_loss = 5235.388427734375
 Trial: 95 Epoch: 1, train_loss: 38546.54919433594, validation_loss = 5053.109375
 Trial: 95 Epoch: 2, train_loss: 36565.764892578125, validation_loss = 4395.747802734375
 Trial: 95 Epoch: 3, train_loss: 41148.849853515625, validation_loss = 5344.481201171875
 Trial: 95 Epoch: 4, train_loss: 55861.68640136719, validation_loss = 3820.3370361328125
 Trial: 95 Epoch: 5, train_loss: 38018.19445800781, validation_loss = 5641.0340576171875
 Trial: 95 Epoch: 6, train_loss: 27482.16357421875, validation_loss = 4280.8751220703125
 Trial: 95 Epoch: 7, train_loss: 27231.994262695312, validation_loss = 3810.2060546875
 Trial: 95 Epoch: 8, train_loss: 25226.009765625, validation_loss = 3858.8026123046875
 Trial: 95 Epoch: 9, train_loss: 23900.53826904297, validation_loss = 3276.818603515625
 Trial: 95 Epoch: 10, train_loss: 23959.070068359375, validation_loss = 4272.5008544921875
 Trial: 95 Epoch: 11, train_loss: 24652.432861328125, validation_loss = 4039.89990234375
 Trial: 95 Epoch: 12, train_loss: 26485.18505859375, validation_loss = 3088.9375
 Trial: 95 Epoch: 13, train_loss: 24844.744873046875, validation_loss = 5217.6221923828125
 Trial: 95 Epoch: 14, train_loss: 26843.77392578125, validation_loss = 4138.8900146484375
 Trial: 95 Epoch: 15, train_loss: 22960.951538085938, validation_loss = 3871.2340087890625
 Trial: 95 Epoch: 16, train_loss: 22493.278442382812, validation_loss = 3202.674072265625
 Trial: 95 Epoch: 17, train_loss: 20774.497924804688, validation_loss = 4033.4093017578125
 Trial: 95 Epoch: 18, train_loss: 24242.329833984375, validation_loss = 2954.4212646484375
 Trial: 95 Epoch: 19, train_loss: 21975.306701660156, validation_loss = 2841.7275390625
 Trial: 95 Epoch: 20, train_loss: 20753.490661621094, validation_loss = 2954.583984375
 Trial: 95 Epoch: 21, train_loss: 20577.693237304688, validation_loss = 4049.9742431640625
 Trial: 95 Epoch: 22, train_loss: 22620.227172851562, validation_loss = 3282.6182861328125
 Trial: 95 Epoch: 23, train_loss: 21640.848693847656, validation_loss = 3887.9244384765625
 Trial: 95 Epoch: 24, train_loss: 23567.021850585938, validation_loss = 3348.0587158203125
 Trial: 95 Epoch: 25, train_loss: 21719.844848632812, validation_loss = 3655.2501220703125
 Trial: 95 Epoch: 26, train_loss: 22212.3837890625, validation_loss = 3167.255615234375
 Trial: 95 Epoch: 27, train_loss: 20928.3818359375, validation_loss = 2786.7440185546875
 Trial: 95 Epoch: 28, train_loss: 21722.041381835938, validation_loss = 2777.2969970703125
 Trial: 95 Epoch: 29, train_loss: 23890.28973388672, validation_loss = 2878.10205078125
 Trial: 95 Epoch: 30, train_loss: 22039.858337402344, validation_loss = 4290.9149169921875
 Trial: 95 Epoch: 31, train_loss: 23446.63671875, validation_loss = 4704.573974609375
 Trial: 95 Epoch: 32, train_loss: 25436.855346679688, validation_loss = 3036.430908203125
 Trial: 95 Epoch: 33, train_loss: 20955.722290039062, validation_loss = 3092.185546875
 Trial: 95 Epoch: 34, train_loss: 20279.949951171875, validation_loss = 3065.1729736328125
 Trial: 95 Epoch: 35, train_loss: 19509.19073486328, validation_loss = 3130.44287109375
 Trial: 95 Epoch: 36, train_loss: 21398.020568847656, validation_loss = 3055.01123046875
 Trial: 95 Epoch: 37, train_loss: 20819.0966796875, validation_loss = 3246.1494140625
 Trial: 95 Epoch: 38, train_loss: 20260.368225097656, validation_loss = 2957.2325439453125
 Trial: 95 Epoch: 39, train_loss: 18390.301147460938, validation_loss = 2809.85498046875
 Trial: 95 Epoch: 40, train_loss: 19362.81427001953, validation_loss = 2633.7974853515625
 Trial: 95 Epoch: 41, train_loss: 18011.7421875, validation_loss = 2918.46435546875
 Trial: 95 Epoch: 42, train_loss: 18337.4697265625, validation_loss = 2545.169677734375
 Trial: 95 Epoch: 43, train_loss: 19364.199279785156, validation_loss = 3154.9735107421875
 Trial: 95 Epoch: 44, train_loss: 19219.323303222656, validation_loss = 2858.24658203125
 Trial: 95 Epoch: 45, train_loss: 20731.32781982422, validation_loss = 3763.6622314453125
 Trial: 95 Epoch: 46, train_loss: 19668.520446777344, validation_loss = 3318.626708984375
 Trial: 95 Epoch: 47, train_loss: 18864.234130859375, validation_loss = 2657.569580078125
 Trial: 95 Epoch: 48, train_loss: 18826.146911621094, validation_loss = 2608.605224609375
 Trial: 95 Epoch: 49, train_loss: 17893.282775878906, validation_loss = 3280.4951171875
 Trial: 95 Epoch: 50, train_loss: 18476.721740722656, validation_loss = 2737.8280029296875
 Trial: 95 Epoch: 51, train_loss: 17891.506958007812, validation_loss = 3433.407470703125
 Trial: 95 Epoch: 52, train_loss: 18976.918334960938, validation_loss = 3270.0394287109375
 Trial: 95 Epoch: 53, train_loss: 21542.90216064453, validation_loss = 2543.1217041015625
 Trial: 95 Epoch: 54, train_loss: 20845.691955566406, validation_loss = 2756.8602294921875
 Trial: 95 Epoch: 55, train_loss: 17627.257934570312, validation_loss = 2609.1607666015625
 Trial: 95 Epoch: 56, train_loss: 17965.652465820312, validation_loss = 2613.6943359375
 Trial: 95 Epoch: 57, train_loss: 19075.190673828125, validation_loss = 2867.927490234375
 Trial: 95 Epoch: 58, train_loss: 19065.667541503906, validation_loss = 2713.962890625
 Trial: 95 Epoch: 59, train_loss: 18005.507080078125, validation_loss = 2761.0269775390625
 Trial: 95 Epoch: 60, train_loss: 18335.1787109375, validation_loss = 2644.9478759765625
 Trial: 95 Epoch: 61, train_loss: 19868.281494140625, validation_loss = 2967.4700927734375
 Trial: 95 Epoch: 62, train_loss: 19091.126098632812, validation_loss = 3040.696044921875
 Trial: 95 Epoch: 63, train_loss: 17802.208984375, validation_loss = 2836.4725341796875
 Trial: 95 Epoch: 64, train_loss: 18616.15594482422, validation_loss = 3680.9423828125
 Trial: 95 Epoch: 65, train_loss: 18341.57275390625, validation_loss = 2797.70166015625
 Trial: 95 Epoch: 66, train_loss: 17472.43927001953, validation_loss = 2913.64892578125
 Trial: 95 Epoch: 67, train_loss: 17130.148193359375, validation_loss = 2621.740966796875
 Trial: 95 Epoch: 68, train_loss: 16858.99951171875, validation_loss = 2611.476318359375
 Trial: 95 Epoch: 69, train_loss: 16401.851196289062, validation_loss = 2677.8643798828125
 Trial: 95 Epoch: 70, train_loss: 18246.668518066406, validation_loss = 3334.5272216796875
 Trial: 95 Epoch: 71, train_loss: 16886.53662109375, validation_loss = 2598.919189453125
 Trial: 95 Epoch: 72, train_loss: 16548.446655273438, validation_loss = 2588.9739990234375
 Trial: 95 Epoch: 73, train_loss: 17460.33544921875, validation_loss = 3007.3701171875
 Trial: 95 Epoch: 74, train_loss: 16724.434997558594, validation_loss = 2705.86962890625
 Trial: 95 Epoch: 75, train_loss: 17317.26025390625, validation_loss = 3788.283447265625
 Trial: 95 Epoch: 76, train_loss: 19461.94793701172, validation_loss = 3192.0565185546875
 Trial: 95 Epoch: 77, train_loss: 17941.93048095703, validation_loss = 2711.177978515625
 Trial: 95 Epoch: 78, train_loss: 15929.955444335938, validation_loss = 2562.0316162109375
 Trial: 95 Epoch: 79, train_loss: 19389.357971191406, validation_loss = 3722.69775390625
 Trial: 95 Epoch: 80, train_loss: 16876.982543945312, validation_loss = 2746.068115234375
 Trial: 95 Epoch: 81, train_loss: 16870.035705566406, validation_loss = 2952.4678955078125
 Trial: 95 Epoch: 82, train_loss: 16403.240173339844, validation_loss = 3504.123779296875
 Trial: 95 Epoch: 83, train_loss: 16956.6943359375, validation_loss = 2775.2822265625
 Trial: 95 Epoch: 84, train_loss: 16482.13165283203, validation_loss = 2625.21142578125
 Trial: 95 Epoch: 85, train_loss: 16165.724060058594, validation_loss = 2891.121826171875
 Trial: 95 Epoch: 86, train_loss: 16831.636962890625, validation_loss = 2555.04541015625
 Trial: 95 Epoch: 87, train_loss: 18353.617797851562, validation_loss = 3660.932373046875
 Trial: 95 Epoch: 88, train_loss: 16610.96270751953, validation_loss = 2522.4857177734375
 Trial: 95 Epoch: 89, train_loss: 17758.095947265625, validation_loss = 2725.16943359375
 Trial: 95 Epoch: 90, train_loss: 17311.55450439453, validation_loss = 3801.770263671875
 Trial: 95 Epoch: 91, train_loss: 18473.217163085938, validation_loss = 2569.487548828125
 Trial: 95 Epoch: 92, train_loss: 17556.05145263672, validation_loss = 3117.57177734375
 Trial: 95 Epoch: 93, train_loss: 16823.04656982422, validation_loss = 2509.9403076171875
 Trial: 95 Epoch: 94, train_loss: 17447.404846191406, validation_loss = 3109.5657958984375
 Trial: 95 Epoch: 95, train_loss: 16712.7724609375, validation_loss = 2764.4527587890625
 Trial: 95 Epoch: 96, train_loss: 17700.982055664062, validation_loss = 2787.0184326171875
 Trial: 95 Epoch: 97, train_loss: 15537.222229003906, validation_loss = 2479.39697265625
 Trial: 95 Epoch: 98, train_loss: 15735.090515136719, validation_loss = 2426.8572998046875
 Trial: 95 Epoch: 99, train_loss: 15967.330688476562, validation_loss = 2636.1807861328125
========Trial 96 params: {'n_layers': 1, 'K': 2, 'Dropout': 0.1, 'Hidden0': 256, 'optimizer': 'Adam', 'lr': 0.011921828131369628, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |   12288    |
|    sptH.0.gcn.bias     |    256     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |    3072    |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    6144    |
|    sptD.0.gcn.bias     |    256     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |    3072    |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |   12288    |
|    sptW.0.gcn.bias     |    256     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |    3072    |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 45285
 Trial: 96 Epoch: 0, train_loss: 122489.76806640625, validation_loss = 5224.5032958984375
 Trial: 96 Epoch: 1, train_loss: 39130.440673828125, validation_loss = 5190.3123779296875
 Trial: 96 Epoch: 2, train_loss: 34573.11279296875, validation_loss = 4380.383056640625
 Trial: 96 Epoch: 3, train_loss: 39522.655517578125, validation_loss = 5221.232421875
 Trial: 96 Epoch: 4, train_loss: 59661.557861328125, validation_loss = 3874.559814453125
 Trial: 96 Epoch: 5, train_loss: 40721.009033203125, validation_loss = 5479.12890625
 Trial: 96 Epoch: 6, train_loss: 28062.817749023438, validation_loss = 4393.420166015625
 Trial: 96 Epoch: 7, train_loss: 27085.30859375, validation_loss = 3871.8192138671875
 Trial: 96 Epoch: 8, train_loss: 25333.182250976562, validation_loss = 3748.0445556640625
 Trial: 96 Epoch: 9, train_loss: 24006.57647705078, validation_loss = 3341.952392578125
 Trial: 96 Epoch: 10, train_loss: 24407.314208984375, validation_loss = 4259.312744140625
 Trial: 96 Epoch: 11, train_loss: 24553.55712890625, validation_loss = 3945.3125
 Trial: 96 Epoch: 12, train_loss: 26607.135864257812, validation_loss = 3082.1995849609375
 Trial: 96 Epoch: 13, train_loss: 25002.407592773438, validation_loss = 5230.8721923828125
 Trial: 96 Epoch: 14, train_loss: 27842.549194335938, validation_loss = 4363.1412353515625
 Trial: 96 Epoch: 15, train_loss: 23546.23565673828, validation_loss = 3972.283203125
 Trial: 96 Epoch: 16, train_loss: 23046.62353515625, validation_loss = 3252.709716796875
 Trial: 96 Epoch: 17, train_loss: 20641.26300048828, validation_loss = 3801.1890869140625
 Trial: 96 Epoch: 18, train_loss: 23645.480590820312, validation_loss = 2938.3001708984375
 Trial: 96 Epoch: 19, train_loss: 21933.44189453125, validation_loss = 2870.4580078125
 Trial: 96 Epoch: 20, train_loss: 20774.64501953125, validation_loss = 2955.7060546875
 Trial: 96 Epoch: 21, train_loss: 20914.79608154297, validation_loss = 4120.882568359375
 Trial: 96 Epoch: 22, train_loss: 23140.668701171875, validation_loss = 3338.927490234375
 Trial: 96 Epoch: 23, train_loss: 22274.111877441406, validation_loss = 3934.485595703125
 Trial: 96 Epoch: 24, train_loss: 24051.13232421875, validation_loss = 3491.1981201171875
 Trial: 96 Epoch: 25, train_loss: 22272.734741210938, validation_loss = 3584.2532958984375
 Trial: 96 Epoch: 26, train_loss: 22355.4345703125, validation_loss = 3127.0235595703125
 Trial: 96 Epoch: 27, train_loss: 21080.848266601562, validation_loss = 2727.8204345703125
 Trial: 96 Epoch: 28, train_loss: 22303.58203125, validation_loss = 2880.2344970703125
 Trial: 96 Epoch: 29, train_loss: 24361.3193359375, validation_loss = 2879.703857421875
 Trial: 96 Epoch: 30, train_loss: 22474.66339111328, validation_loss = 4233.19287109375
 Trial: 96 Epoch: 31, train_loss: 23980.551025390625, validation_loss = 4866.759521484375
 Trial: 96 Epoch: 32, train_loss: 25815.757690429688, validation_loss = 3086.4345703125
 Trial: 96 Epoch: 33, train_loss: 20736.986328125, validation_loss = 3115.49267578125
 Trial: 96 Epoch: 34, train_loss: 20332.723388671875, validation_loss = 2958.9976806640625
 Trial: 96 Epoch: 35, train_loss: 19872.361083984375, validation_loss = 3147.22216796875
 Trial: 96 Epoch: 36, train_loss: 21910.886108398438, validation_loss = 3030.5494384765625
 Trial: 96 Epoch: 37, train_loss: 20964.099853515625, validation_loss = 3185.586669921875
 Trial: 96 Epoch: 38, train_loss: 20413.088256835938, validation_loss = 2928.870361328125
 Trial: 96 Epoch: 39, train_loss: 18504.108337402344, validation_loss = 2809.3748779296875
 Trial: 96 Epoch: 40, train_loss: 19666.047119140625, validation_loss = 2548.47412109375
 Trial: 96 Epoch: 41, train_loss: 18117.036010742188, validation_loss = 2868.461669921875
 Trial: 96 Epoch: 42, train_loss: 18260.656677246094, validation_loss = 2516.4595947265625
 Trial: 96 Epoch: 43, train_loss: 19463.32257080078, validation_loss = 3007.9608154296875
 Trial: 96 Epoch: 44, train_loss: 19104.51043701172, validation_loss = 2849.62158203125
 Trial: 96 Epoch: 45, train_loss: 20869.749450683594, validation_loss = 3658.4716796875
 Trial: 96 Epoch: 46, train_loss: 19723.929443359375, validation_loss = 3236.59130859375
 Trial: 96 Epoch: 47, train_loss: 18917.495056152344, validation_loss = 2561.478759765625
 Trial: 96 Epoch: 48, train_loss: 18724.58447265625, validation_loss = 2560.4970703125
 Trial: 96 Epoch: 49, train_loss: 17720.59161376953, validation_loss = 3300.04638671875
 Trial: 96 Epoch: 50, train_loss: 18775.563598632812, validation_loss = 2791.6085205078125
 Trial: 96 Epoch: 51, train_loss: 18030.409606933594, validation_loss = 3195.0751953125
 Trial: 96 Epoch: 52, train_loss: 18805.62615966797, validation_loss = 3139.5765380859375
 Trial: 96 Epoch: 53, train_loss: 21413.010803222656, validation_loss = 2554.138427734375
 Trial: 96 Epoch: 54, train_loss: 21024.660888671875, validation_loss = 2677.7381591796875
 Trial: 96 Epoch: 55, train_loss: 18041.351684570312, validation_loss = 2523.0130615234375
 Trial: 96 Epoch: 56, train_loss: 18416.808959960938, validation_loss = 2523.2705078125
 Trial: 96 Epoch: 57, train_loss: 19551.80078125, validation_loss = 2679.5447998046875
 Trial: 96 Epoch: 58, train_loss: 19174.433959960938, validation_loss = 2667.075927734375
 Trial: 96 Epoch: 59, train_loss: 17824.24853515625, validation_loss = 2685.801513671875
 Trial: 96 Epoch: 60, train_loss: 18164.05047607422, validation_loss = 2622.49951171875
 Trial: 96 Epoch: 61, train_loss: 19595.295043945312, validation_loss = 2851.4013671875
 Trial: 96 Epoch: 62, train_loss: 18554.790405273438, validation_loss = 2768.4306640625
 Trial: 96 Epoch: 63, train_loss: 17311.83428955078, validation_loss = 2818.670654296875
 Trial: 96 Epoch: 64, train_loss: 18715.724365234375, validation_loss = 3337.2811279296875
 Trial: 96 Epoch: 65, train_loss: 18137.136840820312, validation_loss = 2618.1485595703125
 Trial: 96 Epoch: 66, train_loss: 17195.780212402344, validation_loss = 2745.676513671875
 Trial: 96 Epoch: 67, train_loss: 18246.711181640625, validation_loss = 2469.1492919921875
 Trial: 96 Epoch: 68, train_loss: 17827.41925048828, validation_loss = 2501.9371337890625
 Trial: 96 Epoch: 69, train_loss: 16614.38214111328, validation_loss = 2662.270751953125
 Trial: 96 Epoch: 70, train_loss: 17925.377319335938, validation_loss = 3079.809326171875
 Trial: 96 Epoch: 71, train_loss: 16597.890747070312, validation_loss = 2509.3477783203125
 Trial: 96 Epoch: 72, train_loss: 16483.46160888672, validation_loss = 2528.74658203125
 Trial: 96 Epoch: 73, train_loss: 16971.359497070312, validation_loss = 2693.5758056640625
 Trial: 96 Epoch: 74, train_loss: 16247.294006347656, validation_loss = 2610.9224853515625
 Trial: 96 Epoch: 75, train_loss: 16277.134887695312, validation_loss = 3302.75732421875
 Trial: 96 Epoch: 76, train_loss: 19156.17724609375, validation_loss = 3160.79248046875
 Trial: 96 Epoch: 77, train_loss: 18314.882873535156, validation_loss = 2508.3389892578125
 Trial: 96 Epoch: 78, train_loss: 15852.582885742188, validation_loss = 2480.380615234375
 Trial: 96 Epoch: 79, train_loss: 19584.104919433594, validation_loss = 3239.5238037109375
 Trial: 96 Epoch: 80, train_loss: 16860.279846191406, validation_loss = 2656.70654296875
 Trial: 96 Epoch: 81, train_loss: 16900.258056640625, validation_loss = 2833.3248291015625
 Trial: 96 Epoch: 82, train_loss: 16466.876525878906, validation_loss = 3152.3692626953125
 Trial: 96 Epoch: 83, train_loss: 16778.19952392578, validation_loss = 2827.55517578125
 Trial: 96 Epoch: 84, train_loss: 16253.354370117188, validation_loss = 2660.8931884765625
 Trial: 96 Epoch: 85, train_loss: 16470.485717773438, validation_loss = 2783.973388671875
 Trial: 96 Epoch: 86, train_loss: 16694.762084960938, validation_loss = 2474.146728515625
 Trial: 96 Epoch: 87, train_loss: 16758.023315429688, validation_loss = 2799.9776611328125
 Trial: 96 Epoch: 88, train_loss: 16624.082763671875, validation_loss = 2533.2991943359375
 Trial: 96 Epoch: 89, train_loss: 18059.496337890625, validation_loss = 2640.0252685546875
 Trial: 96 Epoch: 90, train_loss: 17280.890563964844, validation_loss = 3951.9573974609375
 Trial: 96 Epoch: 91, train_loss: 18430.902770996094, validation_loss = 2516.0635986328125
 Trial: 96 Epoch: 92, train_loss: 17539.51104736328, validation_loss = 2935.47119140625
 Trial: 96 Epoch: 93, train_loss: 16387.89239501953, validation_loss = 2505.4859619140625
 Trial: 96 Epoch: 94, train_loss: 17457.116333007812, validation_loss = 2839.9835205078125
 Trial: 96 Epoch: 95, train_loss: 16870.198974609375, validation_loss = 2668.813720703125
 Trial: 96 Epoch: 96, train_loss: 18109.920349121094, validation_loss = 2582.2452392578125
 Trial: 96 Epoch: 97, train_loss: 15718.381713867188, validation_loss = 2442.380615234375
 Trial: 96 Epoch: 98, train_loss: 15843.427795410156, validation_loss = 2345.0162353515625
 Trial: 96 Epoch: 99, train_loss: 15944.861022949219, validation_loss = 2702.436767578125
========Trial 97 params: {'n_layers': 1, 'K': 2, 'Dropout': 0.1, 'Hidden0': 256, 'optimizer': 'Adam', 'lr': 0.007876172781663684, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |   12288    |
|    sptH.0.gcn.bias     |    256     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |    3072    |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    6144    |
|    sptD.0.gcn.bias     |    256     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |    3072    |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |   12288    |
|    sptW.0.gcn.bias     |    256     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |    3072    |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 45285
 Trial: 97 Epoch: 0, train_loss: 114693.80102539062, validation_loss = 5899.1923828125
========Trial 98 params: {'n_layers': 1, 'K': 2, 'Dropout': 0.1, 'Hidden0': 128, 'optimizer': 'Adam', 'lr': 0.0046351021682569495, 'batch': 4}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |    6144    |
|    sptH.0.gcn.bias     |    128     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |    1536    |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    3072    |
|    sptD.0.gcn.bias     |    128     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |    1536    |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |    6144    |
|    sptW.0.gcn.bias     |    128     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |    1536    |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 24933
 Trial: 98 Epoch: 0, train_loss: 172672.35986328125, validation_loss = 7353.10986328125
========Trial 99 params: {'n_layers': 1, 'K': 2, 'Dropout': 0.1, 'Hidden0': 256, 'optimizer': 'Adam', 'lr': 0.011821389030330429, 'batch': 16}===========
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
|     sptH.0.sAtt.Vs     |    729     |
|     sptH.0.sAtt.bs     |    729     |
|     sptH.0.sAtt.W1     |     24     |
|     sptH.0.sAtt.W2     |     24     |
|     sptH.0.sAtt.W3     |     1      |
|   sptH.0.gcn.weight    |   12288    |
|    sptH.0.gcn.bias     |    256     |
| sptH.0.timeConv.weight |     3      |
|  sptH.0.timeConv.bias  |     1      |
|     sptH.2.weight      |    3072    |
|      sptH.2.bias       |     12     |
|     sptD.0.sAtt.Vs     |    729     |
|     sptD.0.sAtt.bs     |    729     |
|     sptD.0.sAtt.W1     |     12     |
|     sptD.0.sAtt.W2     |     12     |
|     sptD.0.sAtt.W3     |     1      |
|   sptD.0.gcn.weight    |    6144    |
|    sptD.0.gcn.bias     |    256     |
| sptD.0.timeConv.weight |     3      |
|  sptD.0.timeConv.bias  |     1      |
|     sptD.2.weight      |    3072    |
|      sptD.2.bias       |     12     |
|     sptW.0.sAtt.Vs     |    729     |
|     sptW.0.sAtt.bs     |    729     |
|     sptW.0.sAtt.W1     |     24     |
|     sptW.0.sAtt.W2     |     24     |
|     sptW.0.sAtt.W3     |     1      |
|   sptW.0.gcn.weight    |   12288    |
|    sptW.0.gcn.bias     |    256     |
| sptW.0.timeConv.weight |     3      |
|  sptW.0.timeConv.bias  |     1      |
|     sptW.2.weight      |    3072    |
|      sptW.2.bias       |     12     |
|       Fusion.Wh        |     12     |
|       Fusion.Wd        |     12     |
|       Fusion.Ww        |     12     |
+------------------------+------------+
Total Trainable Params: 45285
 Trial: 99 Epoch: 0, train_loss: 128420.662109375, validation_loss = 17374.826171875
Study statistics: 
  Number of finished trials:  100
  Number of pruned trials:  51
  Number of complete trials:  49
Best trial:
  Value:  2609.6026611328125
  Params: 
    n_layers: 1
    K: 2
    Dropout: 0.1
    Hidden0: 256
    optimizer: Adam
    lr: 0.009558632058109124
    batch: 4
